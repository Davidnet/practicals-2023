{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m2s4kN_QPQVe"
      },
      "source": [
        "# Jax, Optax, Haiku and Convolutional Neural Networks\n",
        "\n",
        "\n",
        "\n",
        "<a href=\"https://colab.research.google.com/github/khipu-ai/practicals-2023/blob/main/notebooks/jax_optax_haiku_cnn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
        "\n",
        "## Copyright\n",
        "\n",
        "* Deep Learning Indaba 2022. Apache License 2.0.\n",
        "* Khipu 2023. Apache License 2.0.\n",
        "\n",
        "## Authors\n",
        "\n",
        "* Kale-ab Tessera (Indaba 2022)\n",
        "* Ignacio Ram√≠rez (Khipu 2023)\n",
        "\n",
        "## Reviewers\n",
        "* Javier Antoran\n",
        "* James Allingham\n",
        "* Ruan van der Merwe\n",
        "* Sebastian Bodenstein\n",
        "* Laurence Midgley\n",
        "* Joao Guilherme\n",
        "* Elan van Biljon\n",
        "\n",
        "\n",
        "# Introduction\n",
        "\n",
        "We begin by introducing JAX. JAX is a library that allows for very efficient speedups in the aforementioned operations by transparently choosing the best available hardware (CPU, GPU, TPU, etc.) and provinding with a number of acceleration and automation mechanisms such as automatic differentiation (`grad`), parallelization (`pmap`), vectorization (`vmap`) and just-in-time compilation (`jit`).\n",
        "\n",
        "We will then show you how to use the [Optax](https://github.com/deepmind/optax) library, an efficient optimization library based on JAX.\n",
        "\n",
        "We'll then show you how to use  [Haiku](https://github.com/deepmind/dm-haiku),  for building and training neural networks.\n",
        "\n",
        "\n",
        "## Topics\n",
        "\n",
        "* Accelerated computation with JAX\n",
        "* Optimization with Optax\n",
        "* Model training with Haiku\n",
        "* Training a Convolutional Neural Network with Haiku\n",
        "\n",
        "## Prerequisites\n",
        "\n",
        "This practical assumes that you are familiar with:\n",
        "* Python and NumPy\n",
        "* Basic object oriented programming\n",
        "* Basic optimization (gradient descent, etc.)\n",
        "* Basic model learning (linear models)\n",
        "\n",
        "## Hardware requirements (**important**)\n",
        "\n",
        "For this practical, you will need to use a GPU to speed up training. To do this, go to the \"Runtime\" menu in Colab, select \"Change runtime type\" and then in the popup menu, choose \"GPU\" in the \"Hardware accelerator\" box.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6EqhIg1odqg0"
      },
      "source": [
        "## Installation and Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4boGA9rYdt9l",
        "outputId": "ff583f7a-a569-45fc-f458-f6868fd4e6f4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Only CPU accelerator is connected.\n"
          ]
        }
      ],
      "source": [
        "## Install and import anything required. Capture hides the output from the cell.\n",
        "# @title Install and import required packages. (Run Cell)\n",
        "\n",
        "import os\n",
        "\n",
        "# https://stackoverflow.com/questions/68340858/in-google-colab-is-there-a-programing-way-to-check-which-runtime-like-gpu-or-tpu\n",
        "if \"COLAB_TPU_ADDR\" in os.environ and os.environ[\"COLAB_TPU_ADDR\"]:\n",
        "    print(\"A TPU is connected.\")\n",
        "    import jax.tools.colab_tpu\n",
        "    jax.tools.colab_tpu.setup_tpu()\n",
        "\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from jax import grad, jit, vmap, pmap\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YQe1CfDyrkdL"
      },
      "outputs": [],
      "source": [
        "# @title Helper Functions. (Run Cell)\n",
        "import copy\n",
        "from typing import Dict\n",
        "\n",
        "\n",
        "def plot_performance(data: Dict, title: str):\n",
        "    runs = list(data.keys())\n",
        "    time = list(data.values())\n",
        "\n",
        "    # creating the bar plot\n",
        "    plt.bar(runs, time, width=0.35)\n",
        "\n",
        "    plt.xlabel(\"Implementation\")\n",
        "    plt.ylabel(\"Average time taken (in s)\")\n",
        "    plt.title(title)\n",
        "    plt.show()\n",
        "\n",
        "    best_perf_key = min(data, key=data.get)\n",
        "    all_runs_key = copy.copy(runs)\n",
        "\n",
        "    # all_runs_key_except_best\n",
        "    all_runs_key.remove(best_perf_key)\n",
        "\n",
        "    for k in all_runs_key:\n",
        "        print(\n",
        "            f\"{best_perf_key} was {round((data[k]/data[best_perf_key]),2)} times faster than {k} !!!\"\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yFzjRHUsUQqq",
        "outputId": "8b9e1a1f-364a-4500-e8a3-241a42db4fe4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num devices: 1\n",
            " Devices: [StreamExecutorGpuDevice(id=0, process_index=0, slice_index=0)]\n"
          ]
        }
      ],
      "source": [
        "# @title Check the device you are using (Run Cell)\n",
        "print(f\"Num devices: {jax.device_count()}\")\n",
        "print(f\" Devices: {jax.devices()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "$$\n",
        "\\newcommand{\\because}[1]{&& \\triangleright \\textrm{#1}}\n",
        "$$"
      ],
      "metadata": {
        "id": "blMNBku0dB8h"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CbOEYsWQ6tHv"
      },
      "source": [
        "<hr>\n",
        "\n",
        "# PART 1 - JAX\n",
        "\n",
        "<hr>\n",
        "\n",
        "## Similarities between JAX and NumPy \n",
        "\n",
        "The main similarity between JAX and NumPy is that they share a similar interface and often, JAX and NumPy arrays can be used interchangeably.The code below shows this. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sgRLq58OTz1t",
        "outputId": "e9f6355f-179f-4659-a780-f43ce70038f8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "#@title Using NumPy\n",
        "\n",
        "# 100 linearly spaced numbers from -np.pi to np.pi\n",
        "x = np.linspace(-np.pi, np.pi, 100)\n",
        "\n",
        "# the function, which is y = sin(x) here\n",
        "y = np.sin(x)\n",
        "\n",
        "# plot the functions\n",
        "plt.plot(x, y, \"b\", label=\"y=sin(x)\")\n",
        "\n",
        "plt.legend(loc=\"upper left\")\n",
        "\n",
        "# show the plot\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kRQf2mNRTlt3",
        "outputId": "639f1abe-d822-4763-8a78-dac13528d66e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "#@title using JAX\n",
        "# 100 linearly spaced numbers from -jnp.pi to jnp.pi\n",
        "x = jnp.linspace(-jnp.pi, jnp.pi, 100)\n",
        "\n",
        "# the function, which is y = sin(x) here\n",
        "y = jnp.sin(x)\n",
        "\n",
        "# plot the functions\n",
        "plt.plot(x, y, \"b\", label=\"y=sin(x)\")\n",
        "\n",
        "plt.legend(loc=\"upper left\")\n",
        "\n",
        "# show the plot\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wuNscwHeV_dn"
      },
      "source": [
        "**Exercise 1.1 - Code Task:** Can you plot the cosine function using `jnp`?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5svZFPUCQNsG"
      },
      "outputs": [],
      "source": [
        "#@title Excercise 1.1\n",
        "# construct x, and y = cos(x) using JAX\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m4AVrGzy6JWR",
        "outputId": "dc145f84-8a88-4b8f-801b-82ee5f8d87b5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "# @title Answer to code task (Try not to peek until you've given it a good try!')\n",
        "# 100 linearly spaced numbers\n",
        "x = jnp.linspace(-jnp.pi, jnp.pi, 100)\n",
        "\n",
        "y = jnp.cos(x)\n",
        "\n",
        "# plot the functions\n",
        "plt.plot(x, y, \"b\", label=\"y=cos(x)\")\n",
        "\n",
        "plt.legend(loc=\"upper left\")\n",
        "\n",
        "# show the plot\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lg4__l4A7yqc"
      },
      "source": [
        "## Differences between JAX and NumPy\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dPbOnhE4ZSTi"
      },
      "source": [
        "### JAX arrays are immutable, NumPy arrays are not\n",
        "\n",
        "JAX and NumPy arrays are often interchangeable, **but** Jax arrays are **immutable** (they can't be modified after they are created). Allowing mutations makes transforms difficult and violates conditions for [pure functions](https://en.wikipedia.org/wiki/Pure_function).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### NumPy\n",
        "\n",
        "Let's see this in practice by changing the number at the beginning of an array. "
      ],
      "metadata": {
        "id": "Vdfb1wtd-GkF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7r-Los6YZR-f",
        "outputId": "cc353ae3-89dc-4a30-bc53-6c7c4a04017a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[10  1  2  3  4  5  6  7  8  9]\n"
          ]
        }
      ],
      "source": [
        "# NumPy: mutable arrays\n",
        "x = np.arange(10)\n",
        "x[0] = 10\n",
        "print(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### JAX"
      ],
      "metadata": {
        "id": "8Y23OWjE_BDA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OxjkKpqAZxWo",
        "outputId": "2a468e9b-a762-46e5-866d-8c39220e850f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Exception '<class 'jaxlib.xla_extension.DeviceArray'>' object does not support item assignment. JAX arrays are immutable. Instead of ``x[idx] = y``, use ``x = x.at[idx].set(y)`` or another .at[] method: https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.ndarray.at.html\n"
          ]
        }
      ],
      "source": [
        "# JAX: immutable arrays\n",
        "# Should raise an error.\n",
        "try:\n",
        "    x = jnp.arange(10)\n",
        "    x[0] = 10\n",
        "except Exception as e:\n",
        "    print(\"Exception {}\".format(e))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VoWT5RBUagW8"
      },
      "source": [
        "So it fails! We can't mutate a JAX array once it has been created. To update JAX arrays, we need to use [helper functions](https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.ndarray.at.html) that return an updated copy of the JAX array. \n",
        "\n",
        "Instead of doing this `x[idx] = y`, we need to do this `x = x.at[idx].set(y)`. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qJYxkh4qagwO",
        "outputId": "3ff7e434-8d50-4aca-9171-b9a67cb70e71",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " new_x: [10  1  2  3  4  5  6  7  8  9] original x: [0 1 2 3 4 5 6 7 8 9]\n"
          ]
        }
      ],
      "source": [
        "x = jnp.arange(10)\n",
        "new_x = x.at[0].set(10)\n",
        "print(f\" new_x: {new_x} original x: {x}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ut0meCGB5qD0"
      },
      "source": [
        "Note here that `new_x` is a copy and that the original `x` is unchanged. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oAH4c_smdGQU"
      },
      "source": [
        "## Random number generation in NumPy and JAX \n",
        "\n",
        "JAX is more explicit in Pseudo Random Number Generation (PRNG) than `NumPy` and other libraries (such as `TensorFlow` or `PyTorch`). [PRNG](https://en.wikipedia.org/wiki/Pseudorandom_number_generator) is the process of algorithmically generating a sequence of numbers, which *approximate* the properties of a sequence of random numbers.  \n",
        "\n",
        "Let's see the differences in how JAX and NumPy generate random numbers.\n",
        "\n",
        "**Extra reading**: \"The Hitchhiker's Guide to the Galaxy\" by Douglas Adams"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q2m376Ethf8m"
      },
      "source": [
        "##### In Numpy, PRNG is based on a global `state`.\n",
        "\n",
        "Let's set the initial seed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-0t3sjxzdgmP"
      },
      "outputs": [],
      "source": [
        "# Set random seed\n",
        "# If you don't know why this is 42, read the Hitchhiker's Guide to the Galaxy by Douglas Adams\n",
        "np.random.seed(42)\n",
        "prng_state = np.random.get_state()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "QKVz5atZMMOV"
      },
      "outputs": [],
      "source": [
        "# @title Helper function to compare prng keys (Run Cell)\n",
        "def is_prng_state_the_same(prng_1, prng_2):\n",
        "    \"\"\"Helper function to compare two prng keys.\"\"\"\n",
        "    # concat all elements in prng tuple\n",
        "    list_prng_data_equal = [(a == b) for a, b in zip(prng_1, prng_2)]\n",
        "    # stack all elements together\n",
        "    list_prng_data_equal = np.hstack(list_prng_data_equal)\n",
        "    # check if all elements are the same\n",
        "    is_prng_equal = all(list_prng_data_equal)\n",
        "    return is_prng_equal"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nloZ9abah3J3"
      },
      "source": [
        "Let's take a few samples from a Gaussian (normal) Distribution and check if PRNG keys/global state change."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aiUcfX7iSenY",
        "outputId": "40793957-29b1-4fba-c01b-c238be43d0dd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sample 1 = 0.4967141530112327 Did prng state change: True\n",
            "sample 2 = -0.13826430117118466 Did prng state change: True\n",
            "sample 3 = 0.6476885381006925 Did prng state change: True\n"
          ]
        }
      ],
      "source": [
        "print(\n",
        "    f\"sample 1 = {np.random.normal()} Did prng state change: {not is_prng_state_the_same(prng_state,np.random.get_state())}\"\n",
        ")\n",
        "prng_state = np.random.get_state()\n",
        "print(\n",
        "    f\"sample 2 = {np.random.normal()} Did prng state change: {not is_prng_state_the_same(prng_state,np.random.get_state())}\"\n",
        ")\n",
        "prng_state = np.random.get_state()\n",
        "print(\n",
        "    f\"sample 3 = {np.random.normal()} Did prng state change: {not is_prng_state_the_same(prng_state,np.random.get_state())}\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nuHkW6V4iLa9"
      },
      "source": [
        "Numpy's global random state is updated every time a random number is generated, so *sample 1 != sample 2 != sample 3*. \n",
        "\n",
        "Having the state automatically updated, makes it difficult to handle randomness in a **reproducible** way across different threads, processes and devices. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lGDU6ckKkzqL"
      },
      "source": [
        "##### In JAX, PRNG is explicit.\n",
        "\n",
        "In JAX, for each random number generation, you need to explicitly pass in a random key/state."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6oKdk5CSmD-f"
      },
      "source": [
        "Passing the same state/key results in the same number being generated. This is generally undesirable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y-6B0hjtlTmd",
        "outputId": "acc40cc1-289d-4b2e-af2d-40d295431dac",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sample 1 = -0.1847117692232132\n",
            "sample 2 = -0.1847117692232132\n",
            "sample 3 = -0.1847117692232132\n"
          ]
        }
      ],
      "source": [
        "from jax import random\n",
        "\n",
        "key = random.PRNGKey(42)\n",
        "print(f\"sample 1 = {random.normal(key)}\")\n",
        "print(f\"sample 2 = {random.normal(key)}\")\n",
        "print(f\"sample 3 = {random.normal(key)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l0KcwEbZqIaQ"
      },
      "source": [
        "To generate different and independent samples, you need to manually **split** the keys. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v-7BhY0MmEhI",
        "outputId": "58e6efed-609c-469b-b7e3-89d337b91950",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sample 1 = -0.1847117692232132\n",
            "sample 2 = 1.369469404220581\n",
            "sample 3 = -0.19947023689746857\n"
          ]
        }
      ],
      "source": [
        "from jax import random\n",
        "\n",
        "key = random.PRNGKey(42)\n",
        "print(f\"sample 1 = {random.normal(key)}\")\n",
        "\n",
        "# We split the key -> new key and subkey\n",
        "new_key, subkey = random.split(key)\n",
        "\n",
        "# We use the subkey immediately and keep the new key for future splits.\n",
        "# It doesn't really matter which key we keep and which one we use immediately.\n",
        "print(f\"sample 2 = {random.normal(subkey)}\")\n",
        "\n",
        "# We split the new key -> new key2 and subkey\n",
        "new_key2, subkey = random.split(new_key)\n",
        "print(f\"sample 3 = {random.normal(subkey)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2VnTDptmuk-i"
      },
      "source": [
        "By using JAX, we can more easily reproduce random number generation in parallel across threads, processes, or even devices by explicitly passing and keeping track of the prng key (without relying on a global state that automatically gets updated). For more details on PRNG in JAX, you can read more [here](https://jax.readthedocs.io/en/latest/jep/263-prng.html). "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TSj972IWxTo2"
      },
      "source": [
        "### Acceleration in JAX üöÄ\n",
        "\n",
        "JAX leverages Autograd and XLA for accelerating numerical computation. The use of Autograd allows for automatic differentiation (`grad`), while XLA allows JAX to run on multiple accelerators/backends and run transforms like `jit` and `pmap`. JAX also allows you to use `vmap` for automatic vectorization.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_bQ9QqT-yKbs"
      },
      "source": [
        "### JAX is backend Agnostic\n",
        "\n",
        "Using JAX, you can run the same code on different backends/AI accelerators (e.g. CPU/GPU/TPU), **with no changes in code** (no more `.to(device)` - from frameworks like PyTorch). This means we can easily run linear algebra operations directly on GPU/TPU."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4PbcFsfAibBu"
      },
      "source": [
        "**Multiplying Matrices**\n",
        "\n",
        "Dot products are a common operation in numerical computing and a central part of modern deep learning. They are defined over [vectors](https://en.wikipedia.org/wiki/Coordinate_vector), which can loosely be thought of as a list of multiple scalers (single values). \n",
        "\n",
        "Formally, given two vectors $\\boldsymbol{x}$,$\\boldsymbol{y}$ $\\in R^n$, their dot product is defined as:\n",
        "\n",
        "<center>$\\boldsymbol{x}^{\\top} \\boldsymbol{y}=\\sum_{i=1}^{n} x_{i} y_{i}$</center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AY1RsVkXaokP"
      },
      "source": [
        "Dot Product in NumPy (will run on cpu)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yj59KkD_HDOs",
        "outputId": "4ff0ee9c-e98e-489c-aee7-52892137d6c3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "62.4 ms ¬± 15.9 ms per loop (mean ¬± std. dev. of 7 runs, 10 loops each)\n"
          ]
        }
      ],
      "source": [
        "size = 1000\n",
        "x = np.random.normal(size=(size, size))\n",
        "y = np.random.normal(size=(size, size))\n",
        "numpy_time = %timeit -o -n 10 a_np = np.dot(y,x.T)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6c_kl-u0KPVY"
      },
      "source": [
        "Dot Product using JAX (will run on current runtime - e.g. GPU)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PHRcHK86KO3w",
        "outputId": "f3badb3f-eaea-48de-9f9e-dcbfd9946f92",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The slowest run took 257.38 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
            "45.3 ms ¬± 108 ms per loop (mean ¬± std. dev. of 7 runs, 10 loops each)\n"
          ]
        }
      ],
      "source": [
        "size = 1000\n",
        "key1, key2 = jax.random.split(jax.random.PRNGKey(42), num=2)\n",
        "x = jax.random.normal(key1, shape=(size, size))\n",
        "y = jax.random.normal(key2, shape=(size, size))\n",
        "jax_time = %timeit -o -n 10 jnp.dot(y, x.T).block_until_ready()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LMTSpEG3TNah"
      },
      "source": [
        "\n",
        "> When timing JAX functions, we use `.block_until_ready()` because JAX uses [asynchronous dispatch](https://jax.readthedocs.io/en/latest/async_dispatch.html#async-dispatch). This means JAX doesn't wait for the operation to complete before returning control to your code. To fairly compute the time taken for JAX operations, we therefore block until the operation is done.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S3vwh6Q724gn"
      },
      "source": [
        "How much faster was the dot product in JAX (Using GPU)?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UkASX9p34A1D",
        "outputId": "909ddeb3-7343-42d8-de47-787ba847f3cc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "jax was 1.38 times faster than numpy !!!\n"
          ]
        }
      ],
      "source": [
        "np_average_time = np.mean(numpy_time.all_runs)\n",
        "jax_average_time = np.mean(jax_time.all_runs)\n",
        "data = {\"numpy\": np_average_time, \"jax\": jax_average_time}\n",
        "\n",
        "plot_performance(data, title=\"Average time taken per framework to run dot product\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "JAX not running much faster? -> Re-run the JAX cell.                       \n",
        "> \"Keep in mind that the first time you run JAX code, it will be slower because it is being compiled. T*his is true even if you don‚Äôt use jit in your own code, because JAX‚Äôs builtin functions are also jit compiled*.\" - [JAX Docs](https://jax.readthedocs.io/en/latest/faq.html#benchmarking-jax-code).\n",
        "\n",
        "If you are running on an accelerator, you should see a considerable performance benefit of using JAX, without making any changes to your code! \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "X6Rv_OQgBOqr"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JM_08mXEBRIK"
      },
      "source": [
        "# JAX Transformations\n",
        "\n",
        "JAX transforms (`jit`, `grad`, `vmap`, `pmap`) first convert python functions into an intermediate language called *jaxpr*. Transforms are then applied to this jaxpr representation.\n",
        "\n",
        "JAX generates jaxpr, in a process known as **tracing**. During tracing, function inputs are wrapped by a tracer object and then JAX records all operations (including regular python code) that occur during the function call. These recorded operations are used to reconstruct the function. \n",
        "\n",
        "Any python side-effects are not recorded during tracing. JAX transforms and compilations are designed to work only with **pure functions**. For more on tracing and jaxpr, you can read [here](https://jax.readthedocs.io/en/latest/jaxpr.html).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QsJE_U-ZzVol"
      },
      "source": [
        "## JIT (Just in Time) compilation\n",
        "\n",
        "Jax dispatches operations to accelerators one at a time. If we have repeated operations, we can use `jit` to compile the function the first time it is called, then subsequent calls will be [cached](https://en.wikipedia.org/wiki/Cache_(computing) (save the compiled version so that it doesn't need to be recompiled everytime we call it). "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uIYsqIp_-Dly"
      },
      "source": [
        "Let's compile [ReLU (Rectified Linear Unit)](https://arxiv.org/abs/1803.08375), a popular activation function in deep learning. \n",
        "\n",
        "ReLU is defined as follows:\n",
        "<center>$f(x)=max(0,x)$</center>\n",
        "\n",
        "It can be visualized as follows:\n",
        "\n",
        "<center>\n",
        "<img src=\"https://machinelearningmastery.com/wp-content/uploads/2018/10/Line-Plot-of-Rectified-Linear-Activation-for-Negative-and-Positive-Inputs.png\" width=\"35%\" />\n",
        "</center>,\n",
        "\n",
        "where $x$ is the input to the function and $y$ is output of ReLU.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vm-bN9sQETLV"
      },
      "source": [
        "$$f(x)=\\max (0, x)=\\left\\{\\begin{array}{l}x_{i} \\text { if } x_{i}>0 \\\\ 0 \\text { if } x_{i}<=0\\end{array}\\right.$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dFiuu3BFAKdY"
      },
      "source": [
        "**Exercise 1.2 - Code Task:** Complete the ReLU implementation below using standard python."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1_qMJJbs-Cbe"
      },
      "outputs": [],
      "source": [
        "# Implement ReLU.\n",
        "def relu(x):\n",
        "  pass\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "zCobLakM1esy",
        "outputId": "d0622d19-48b2-49d9-a7cf-88a269f0f1cf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 342
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "AssertionError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-bdf477a27bf1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m \u001b[0mcheck_relu_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0mplot_relu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-23-bdf477a27bf1>\u001b[0m in \u001b[0;36mcheck_relu_function\u001b[0;34m(relu_function)\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinspace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrelu_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mxi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0;32massert\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;31m# Check if x == 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAssertionError\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# @title Run to test your function.\n",
        "\n",
        "\n",
        "def plot_relu(relu_function):\n",
        "    max_int = 5\n",
        "    # Generete 100 evenly spaced points from -max_int to max_int\n",
        "    x = np.linspace(-max_int, max_int, 1000)\n",
        "    y = np.array([relu_function(xi) for xi in x])\n",
        "    plt.plot(x, y, label=\"ReLU\")\n",
        "    plt.legend(loc=\"upper left\")\n",
        "    plt.xticks(np.arange(min(x), max(x) + 1, 1))\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def check_relu_function(relu_function):\n",
        "    # Generete 100 evenly spaced points from -100 to -1\n",
        "    x = np.linspace(-100, -1, 100)\n",
        "    y = np.array([relu_function(xi) for xi in x])\n",
        "    assert (y == 0).all()\n",
        "\n",
        "    # Check if x == 0\n",
        "    x = 0\n",
        "    y = relu_function(x)\n",
        "    assert y == 0\n",
        "\n",
        "    # Generete 100 evenly spaced points from 0 to 100\n",
        "    x = np.linspace(0, 100, 100)\n",
        "    y = np.array([relu_function(xi) for xi in x])\n",
        "    assert np.allclose(x, y)\n",
        "\n",
        "    print(\"Your ReLU function is correct!\")\n",
        "\n",
        "\n",
        "check_relu_function(relu)\n",
        "plot_relu(relu)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kken6_XvDdOK",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Answer to code task (Try not to peek until you've given it a good try!')\n",
        "def relu(x):\n",
        "    if x > 0:\n",
        "        return x\n",
        "    else:\n",
        "        return 0\n",
        "\n",
        "\n",
        "check_relu_function(relu)\n",
        "plot_relu(relu)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2mgIAyE2Fx3O"
      },
      "source": [
        "Let's try to `jit` this function to speed up compilation and then try to call it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4YDkiNlRF6jn"
      },
      "outputs": [],
      "source": [
        "relu_jit = jax.jit(relu)\n",
        "\n",
        "key = jax.random.PRNGKey(42)\n",
        "# Gen 1000000 random numbers and pass them to relu\n",
        "num_random_numbers = 1000000\n",
        "x = jax.random.normal(key, (num_random_numbers,))\n",
        "\n",
        "# Should raise an error.\n",
        "try:\n",
        "    relu_jit(x)\n",
        "except Exception as e:\n",
        "    print(\"Exception {}\".format(e))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y7q33C4pHOQW"
      },
      "source": [
        "**Why does this fail?**\n",
        "\n",
        "\n",
        "> As mentioned above, JAX transforms first converts python functions into an intermediate language called *jaxpr*. Jaxpr only captures what is executed on the parameters given to it during tracing, so this means during conditional calls, jaxpr only considers the branch taken.\n",
        "> \n",
        "> When jit-compiling a function, we want to compile and cache a version of the function that can handle multiple different argument types (so we don't have to recompile for each function evaluation). For example, when we compile a function on an array `jnp.array([1., 2., 3.], jnp.float32)`, we would likely also want to use the compiled function for `jnp.array([4., 5., 6.], jnp.float32)`. \n",
        "> \n",
        "> To achieve this, JAX traces your code based on abstract values. The default abstraction level is a ShapedArray - array that has a fixed size and dtype, for example, if we trace a function using `ShapedArray((3,), jnp.float32)`,  it can be reused for any concrete array of size 3, and float32 dtype. \n",
        "> \n",
        "> This does come with some challenges. Tracing that relies on concrete values becomes tricky and sometimes results in `ConcretizationTypeError` as in the ReLU function above. Furthermore, when tracing a function with conditional statements (\"if ...\"), JAX doesn't know which branch to take when tracing and so tracing can't occur.\n",
        "\n",
        "**TLDR**: JAX tracing doesn't work well with conditional statements (\"if ...\"). \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uLswU8aMEQ9K"
      },
      "source": [
        "To solve this, we have two options:\n",
        "- Use static arguments to make sure JAX traces on a concrete value level - this is not ideal if you need to retrace a lot. Example - bottom of this [section](https://jax.readthedocs.io/en/latest/notebooks/Common_Gotchas_in_JAX.html#python-control-flow-jit).\n",
        "- Use builtin JAX condition flow primitives such as [`lax.cond`](https://jax.readthedocs.io/en/latest/_autosummary/jax.lax.cond.html) or [`jnp.where`](https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.where.html).  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SX8k4R7daBpP"
      },
      "source": [
        "**Exercise 1.3 - Code Task** : Let's convert our ReLU function above to work with jit.\n",
        "\n",
        "**Useful methods:**  [`jnp.where`](https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.where.html) (or [`jnp.maximum`](https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.maximum.html), if you prefer.) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p-4mXLwqaK-b"
      },
      "outputs": [],
      "source": [
        "# Implement a jittable ReLU\n",
        "def relu(x):\n",
        "  pass\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B5fq_QRoaaG5",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Run to test your function.\n",
        "check_relu_function(relu)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XLtBaplGxlS3",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Answer to code task (Try not to peek until you've given it a good try!')\n",
        "def relu(x):\n",
        "    return jnp.where(x > 0, x, 0)\n",
        "    # Another option - return jnp.maximum(x,0)\n",
        "\n",
        "\n",
        "check_relu_function(relu)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KYogDOCLiLXN",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Now let's see the performance benefit of using jit! (Run me)\n",
        "\n",
        "# jit our function\n",
        "relu_jit = jax.jit(relu)\n",
        "\n",
        "# generate random input\n",
        "key = jax.random.PRNGKey(42)\n",
        "num_random_numbers = 1000000\n",
        "x = jax.random.normal(key, (num_random_numbers,))\n",
        "\n",
        "# time normal jit function\n",
        "jax_time = %timeit -o -n 10 relu(x).block_until_ready()\n",
        "\n",
        "# Warm up/Compile - first run for jitted function\n",
        "relu_jit(x).block_until_ready()\n",
        "\n",
        "# time jitted function\n",
        "jax_jit_time = %timeit -o -n 10 relu_jit(x).block_until_ready()\n",
        "\n",
        "# Let's plot the performance difference\n",
        "jax_avg_time = np.mean(jax_time.all_runs)\n",
        "jax_jit_avg_time = np.mean(jax_jit_time.all_runs)\n",
        "data = {\"JAX (no jit)\": jax_avg_time, \"JAX (with jit)\": jax_jit_avg_time}\n",
        "\n",
        "plot_performance(data, title=\"Average time taken for ReLU function\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dxq-z-xzs40s"
      },
      "source": [
        "## Automatic gradient computation\n",
        "\n",
        "The `grad` transformation is used to automatically compute the gradient of a function in JAX. It can be applied to Python and NumPy functions, which means you can differentiate through loops, branches, recursion, and closures.  \n",
        "\n",
        "`grad` takes in a function `f` and returns a function. If `f` is a mathematical function $f(x)$, then `grad(f(x))` corresponds to $f'(x)=\\frac{df}{dx}$. Then `grad(f)(x_0)` yields $f'(x_0)$.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C49R8EOs-GHe"
      },
      "source": [
        "Let's take a simple function $f(x)=6x^4-9x+4$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lUMepl6J-dQP"
      },
      "outputs": [],
      "source": [
        "f = lambda x: 6 * x**4 - 9 * x + 4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ayvrkpiBiu4"
      },
      "source": [
        "We can compute the gradient of this function - $f'(x)$ and evaluate it at $x=3$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YNm9hS2S-vJk"
      },
      "outputs": [],
      "source": [
        "dfdx = grad(f)\n",
        "dfdx_3 = dfdx(3.0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UcRUywsnF3LZ"
      },
      "source": [
        "**Exercise 1.4 - Math Task**: Can you calculate $f'(2)$ by hand?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PybYK6NEFWrD"
      },
      "outputs": [],
      "source": [
        "answer = None  # @param {type:\"integer\"}\n",
        "\n",
        "dfdx_2 = dfdx(2.0)\n",
        "\n",
        "assert (\n",
        "    answer == dfdx_2\n",
        "), \"Incorrect answer, hint https://en.wikipedia.org/wiki/Power_rule#Statement_of_the_power_rule\"\n",
        "\n",
        "print(\"Nice, you got the correct answer!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Answer to math task (Try not to run until you've given it a good try!') \n",
        "%%latex                                                  \n",
        "\\begin{aligned}\n",
        "f(x) & = 6x^4-9x+4 \\\\\n",
        "f'(x) & = 24x^3 -9  && \\triangleright \\textrm{Power Rule.}  \\\\ \n",
        "f'(2) &  = 24(2)^3 -9 = 183 && \\triangleright \\textrm{Substituting x=2} \\\\\n",
        "\\end{aligned}"
      ],
      "metadata": {
        "id": "CAwlhxIlRPp9",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wcB5ZjojH67Q"
      },
      "source": [
        "We can also chain `grad` to calculate higher order deratives. \n",
        "\n",
        "We can calculate $f'''(x)$ as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "013SFq7BE54W"
      },
      "outputs": [],
      "source": [
        "d3dx = grad(grad(grad(f)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7_r9VQGoIsa6"
      },
      "source": [
        "**Exercise 1.5 - Math Task**: How about $f'''(2)$ by hand?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "WZUArv4TInPg"
      },
      "outputs": [],
      "source": [
        "answer = None  # @param {type:\"integer\"}\n",
        "\n",
        "d3dx_2 = d3dx(2.0)\n",
        "\n",
        "assert answer == d3dx_2, \"Incorrect answer, hint ...\"\n",
        "\n",
        "print(\"Nice, you got the correct answer!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Answer to math task (Try not to run until you've given it a good try!') \n",
        "%%latex \n",
        "\n",
        "\\begin{aligned}\n",
        "f(x) & = 6x^4-9x+4 \\\\\n",
        "f'(x) & = 24x^3 -9  && \\triangleright \\textrm{Power Rule.}  \\\\\n",
        "f''(x) & = 72x^2  && \\triangleright \\textrm{Power Rule.}  \\\\\n",
        "f'''(x) & = 144x && \\triangleright \\textrm{Power Rule.} \\\\\n",
        "f'''(2) & = 144(2)=288 && \\triangleright \\textrm{Substituting x=2} \\\\ \n",
        "\\end{aligned}"
      ],
      "metadata": {
        "id": "TCC7SkH8MMVk",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c3QgJNU9XYyz"
      },
      "source": [
        "Another useful method is `value_and_grad`, where we can get the value ($f(x)$) and gradient ($f'(x)$). "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x3zeSv6gXuyd"
      },
      "outputs": [],
      "source": [
        "from jax import value_and_grad\n",
        "\n",
        "f_x, dy_dx = value_and_grad(f)(2.0)\n",
        "print(f\"f(x): {f_x} f‚Ä≤(x): {dy_dx} \")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> For partial derivatives, you need to use the [`argnums`](https://jax.readthedocs.io/en/latest/_autosummary/jax.grad.html) param to specify which variables you want to differentiate with respect to. \n",
        "\n"
      ],
      "metadata": {
        "id": "_vUr-B6gSxnu"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MktOLPnwvnH3"
      },
      "source": [
        "**Exercise 1.6 - Group Task:** Chat with neighbour/think about how JAX's automatic differentiation compares to other libraries such as Pytorch or Tensorflow. "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Another useful application related to `grad` is when you want your `grad` function to return auxiliary (extra) data, that you don't want differentiated. You can use the `has_aux` parameter to do this (example in \"Auxiliary data\" section in [here](https://github.com/google/jax/blob/main/docs/jax-101/01-jax-basics.ipynb))."
      ],
      "metadata": {
        "id": "rvXlE7z02M2D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pure Functions\n",
        "\n",
        "JAX transformation and compilation are designed to work reliably on **pure functions** (see [Wikipedia page on pure functions](https://en.wikipedia.org/wiki/Pure_function)). These functions have the following properties:\n",
        "1. All **input** data is passed through the **function's parameters**. \n",
        "2. All **results** are output through the **function's return**. \n",
        "3. The function always returns the same **result** if invoked with the **same inputs**. What if your function involves randomness? Pass in the random seed!\n",
        "4. **No [side-effects](https://en.wikipedia.org/wiki/Side_effect_(computer_science))** - no mutation of non-local variables or input/output streams.  \n",
        " \n",
        "\n",
        "Let's see what could happen if we don't stick to using pure functions."
      ],
      "metadata": {
        "id": "fT56qxXzTVKZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Side Effects"
      ],
      "metadata": {
        "id": "Mad7l7s0CtT1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's call print within a function."
      ],
      "metadata": {
        "id": "xkQWTE2Xe955"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def impure_print_side_effect(x):\n",
        "    print(\"Print me!\")  # This is a side-effect\n",
        "    return x\n",
        "\n",
        "\n",
        "# The side-effects appear during the first run\n",
        "print(\"First call: \", jax.jit(impure_print_side_effect)(4.0))"
      ],
      "metadata": {
        "id": "S9aeUdUoBmCg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As expected, the print statement is called.\n",
        "\n",
        "Let's call this function again. "
      ],
      "metadata": {
        "id": "nu4rnyS7ox_L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Subsequent runs with parameters of same type and shape may not show the side-effect\n",
        "# This is because JAX now invokes a cached compilation of the function\n",
        "print(\"Second call: \", jax.jit(impure_print_side_effect)(5.0))"
      ],
      "metadata": {
        "id": "-wnkIqAxfDeJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ah, no print statement! Since JAX cached the compilation of the function, `print()` calls will only happen during tracing and not every time the function is called. "
      ],
      "metadata": {
        "id": "64rNvVnwo-eB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# JAX re-runs the Python function when the type or shape of the argument changes\n",
        "print(\n",
        "    \"Third call, different type: \", jax.jit(impure_print_side_effect)(jnp.array([5.0]))\n",
        ")"
      ],
      "metadata": {
        "id": "Mp_CkOL-o86t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this case, we called the function with a different shaped object and so it triggered the re-tracing of the function and print was called again. "
      ],
      "metadata": {
        "id": "XFogrIf5fbLU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To print values in compiled functions, use [host callbacks](https://jax.readthedocs.io/en/latest/jax.experimental.host_callback.html?highlight=print#jax.experimental.host_callback.id_print)([example](https://github.com/google/jax/issues/196#issuecomment-1191155679)) or if your jax version>=0.3.16, you can use [`jax.debug.print`](https://jax.readthedocs.io/en/latest/debugging/print_breakpoint.html). \n"
      ],
      "metadata": {
        "id": "pqV6_25GCxHL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Global variables"
      ],
      "metadata": {
        "id": "EqL1-TGaC8Ir"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using global variables can also lead to some undesired consequences!"
      ],
      "metadata": {
        "id": "t8dzJog8tMe_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "g = 0.0\n",
        "\n",
        "\n",
        "def impure_uses_globals(x):\n",
        "    return x + g\n",
        "\n",
        "\n",
        "# JAX captures the value of the global during the first run\n",
        "print(\"First call: \", jax.jit(impure_uses_globals)(4.0))"
      ],
      "metadata": {
        "id": "vwAkKrDiCXO6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This prints 4, using the original value of `g`.\n",
        "\n",
        "Let's update `g` and call our function again."
      ],
      "metadata": {
        "id": "pWNE8B5btcfc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "g = 10.0  # Update the global\n",
        "\n",
        "# Subsequent runs may silently use the cached value of the globals\n",
        "print(\"Second call: \", jax.jit(impure_uses_globals)(4.0))"
      ],
      "metadata": {
        "id": "mLMpdQZwtUEL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Even though we updated our global variable, this still prints 4, using the original value of `g`. This is because the value of `g` was cached."
      ],
      "metadata": {
        "id": "o3-ygEx0tpBX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# JAX re-runs the Python function when the type or shape of the argument changes\n",
        "# This will end up reading the latest value of the global\n",
        "print(\"Third call, different type: \", jax.jit(impure_uses_globals)(jnp.array([4.0])))"
      ],
      "metadata": {
        "id": "LDecWNyktWDN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Similar to the side-effects example, re-tracing gets triggered when the shape of our input has changed. In this case, our function now uses the updated value of `g`."
      ],
      "metadata": {
        "id": "3mIZaXOqt5ix"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since the global variables are cached, it is still okay to use global **constants** inside jax functions."
      ],
      "metadata": {
        "id": "aLis2BV04BQK"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RCUB9YkCnCFb"
      },
      "source": [
        "## Function vectorization with vmap\n",
        "\n",
        "vmap (Vectorizing map) automatically vectorizes your python functions. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e858lqfYKd4d"
      },
      "source": [
        "Let's define a simple function that calculates the min and max of an input."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-6qalyXgDsKB"
      },
      "outputs": [],
      "source": [
        "def min_max(x):\n",
        "    return jnp.array([jnp.min(x), jnp.max(x)])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "muSIsUkgKlxh"
      },
      "source": [
        "We can apply this function to the vector - `[0, 1, 2, 3, 4]` and get the min and max values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F5wIeGieKsWG"
      },
      "outputs": [],
      "source": [
        "x = jnp.arange(5)\n",
        "min_max(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_PkC7NnPLNXq"
      },
      "source": [
        "What about if we want to apply this to a batch/list of vectors (i.e. calculate the min and max independently across multiple batches)? "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hRngFfwCMHLd"
      },
      "source": [
        "Let's create our batch - 3 vectors of size 5."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EKuh459OD6jx"
      },
      "outputs": [],
      "source": [
        "batch_size = 3\n",
        "batched_x = np.arange(15).reshape((batch_size, -1))\n",
        "print(batched_x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hApYpVEvNS1y"
      },
      "source": [
        "**Exercise 1.7 - Question**: What do you think would be the result if we passed batch_x into `min_max`?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gu6C3J0kMrtj"
      },
      "outputs": [],
      "source": [
        "batch_min_max_output = [[0,4],[5,9],[10,14]]  # @param [\"[[0,4],[5,9],[10,14]]\", \"[[0,10],[1,11],[2,12],[3,13],[4,14]]\", \"[0,14]\"] {type:\"raw\"}\n",
        "\n",
        "assert (batch_min_max_output == np.array(min_max(batched_x))).all(), \"Incorrect answer.\"\n",
        "\n",
        "print(\"Nice, you got the correct answer!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6K0weiHOOb8L"
      },
      "source": [
        "So the above is not what we want. The `min` and `max` is applied across the entire batch, when we want the min and max per vector/mini-batch. \n",
        "\n",
        "We can also manually batch this by `jnp.stack` and a for loop, as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q8RdAqr8N-Fd"
      },
      "outputs": [],
      "source": [
        "@jit\n",
        "def manual_batch_min_max_loop(batched_x):\n",
        "    min_max_result_list = []\n",
        "    for x in batched_x:\n",
        "        min_max_result_list.append(min_max(x))\n",
        "    return jnp.stack(min_max_result_list)\n",
        "\n",
        "\n",
        "print(manual_batch_min_max_loop(batched_x))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jmu3VVtMR0GV"
      },
      "source": [
        "Or, just manually updating the `axis` in `jnp.min` and `jnp.max`. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lzxmORv-RcUg"
      },
      "outputs": [],
      "source": [
        "@jit\n",
        "def manual_batch_min_max_axis(batched_x):\n",
        "    return jnp.array([jnp.min(batched_x, axis=1), jnp.max(batched_x, axis=1)]).T\n",
        "\n",
        "\n",
        "print(manual_batch_min_max_axis(batched_x))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CetKYASUSE4Q"
      },
      "source": [
        "These approaches both work, but we need to change our function to work with batches. We can't just run the same code across a batch of data.\n",
        "\n",
        "There is where `vmap` becomes useful! Using `vmap` we can write a function once, as if it is working on a single element, and then use `vmap` to automatically vectorize it! "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s2F8WUNQROkQ"
      },
      "outputs": [],
      "source": [
        "# define our vmap function using our original single vector function\n",
        "@jit\n",
        "def min_max_vmap(batched_x):\n",
        "    return vmap(min_max)(batched_x)\n",
        "\n",
        "\n",
        "# Run it on a single vecor\n",
        "## We add extra dimention in a single vector, shape changes from (5,) to (1,5), which makes the vmapping possible\n",
        "x_with_leading_dim = jax.numpy.expand_dims(x, axis=0)\n",
        "print(f\"Single vector: {min_max_vmap(x_with_leading_dim)}\")\n",
        "\n",
        "# Run it on batch of vectors\n",
        "print(f\"Batch/list of vector:{min_max_vmap(batched_x)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-3bome92VRL6"
      },
      "source": [
        "So this is really convenient, but what about performance? "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O1Nb4uniUUor"
      },
      "outputs": [],
      "source": [
        "batched_x = np.arange(50000).reshape((500, 100))\n",
        "\n",
        "# Trace the functions with first call\n",
        "manual_batch_min_max_loop(batched_x).block_until_ready()\n",
        "manual_batch_min_max_axis(batched_x).block_until_ready()\n",
        "min_max_vmap(batched_x).block_until_ready()\n",
        "\n",
        "min_max_forloop_time = %timeit -o -n 10 manual_batch_min_max_loop(batched_x).block_until_ready()\n",
        "min_max_axis_time = %timeit -o -n 10 manual_batch_min_max_axis(batched_x).block_until_ready()\n",
        "min_max_vmap_time = %timeit -o -n 10 min_max_vmap(batched_x).block_until_ready()\n",
        "\n",
        "print(\n",
        "    f\"Avg Times (lower is better) - Naive Implementation: {np.round(np.mean(min_max_forloop_time.all_runs),5)} Manually Vectorized: {np.round(np.mean(min_max_axis_time.all_runs),5)} Vmapped Function: {np.round(np.mean(min_max_vmap_time.all_runs),5)} \"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mYL758zCYsrR"
      },
      "source": [
        "So `vmap` should be similar in performance to manually vectorized code (if everything is implemented well), and much better than naively vectorized code (i.e. for loops). "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vAO9dOdrtiqI"
      },
      "source": [
        "## Paralelization with pmap\n",
        "\n",
        "üí°**For this subsection, please ensure that colab is using a `TPU` runtime. If no `TPU` runtimes are available, select `Harware Accelerator` - `None` for a cpu runtime.** \n",
        "\n",
        "With `pmap` we can convert a function written for a single device to a function that can run in parallel across many devices. \n",
        "\n",
        "**Difference between `vmap` and `pmap`**:\n",
        "\n",
        "So both `pmap` and `vmap` transform a function to work over an array, but they differ in implementation. `vmap` adds an extra batch dimension to all the operations in a function, while `pmap` replicates the function and executes each replica on its own XLA device in parallel."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gUYA277soR-0"
      },
      "outputs": [],
      "source": [
        "# @title Check the device you are using (Run Cell)\n",
        "print(f\"Num devices: {jax.device_count()}\")\n",
        "print(f\" Devices: {jax.devices()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6qhlBnLs6AYL"
      },
      "source": [
        "Let's try and `pmap` a batch of dot products.\n",
        "\n",
        "Here is an illustration of how we would typically do this sequentially: \n",
        "\n",
        "[Source](https://www.assemblyai.com/blog/why-you-should-or-shouldnt-be-using-jax-in-2022/)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fz1i2AwA5_7J"
      },
      "outputs": [],
      "source": [
        "# @title Illustration of Sequential Dot Product (Run me)\n",
        "from IPython.display import HTML\n",
        "\n",
        "HTML(\n",
        "    '<iframe width=\"560\" height=\"315\" src=\"https://www.assemblyai.com/blog/content/media/2022/02/not_parallel-2.mp4\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>'\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MTmWNFZ08f8n"
      },
      "source": [
        "Here is the code implementation of this:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GqTuMldJ9Uv5"
      },
      "outputs": [],
      "source": [
        "# Let's generate a batch of size 8, each with a matrix of size (500, 600)\n",
        "\n",
        "# Let create 8 keys, 1 for each batch\n",
        "keys = jax.random.split(jax.random.PRNGKey(0), 8)\n",
        "\n",
        "# Let create our batches\n",
        "mats = jnp.stack([jax.random.normal(key, (500, 600)) for key in keys])\n",
        "\n",
        "\n",
        "def dot_product_sequential():\n",
        "    @jit\n",
        "    def avg_dot_prod(mats):\n",
        "        result = []\n",
        "        # Loop through batch and compute dp\n",
        "        for mat in mats:\n",
        "            # dot product between the a mat and mat.T (transposed version)\n",
        "            result.append(jnp.dot(mat, mat.T))\n",
        "        return jnp.stack(result)\n",
        "\n",
        "    avg_dot_prod(mats).block_until_ready()\n",
        "\n",
        "\n",
        "run_sequential = %timeit -o -n 5 dot_product_sequential()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fBEtecJX-0AW"
      },
      "source": [
        "Here is an illustration of how we would do this in parallel \n",
        "\n",
        "[Source](https://www.assemblyai.com/blog/why-you-should-or-shouldnt-be-using-jax-in-2022/)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uswxurmn-5oC",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Illustration of Parallel Dot Product (Run me)\n",
        "from IPython.display import HTML\n",
        "\n",
        "HTML(\n",
        "    '<iframe width=\"560\" height=\"315\" src=\"https://www.assemblyai.com/blog/content/media/2022/02/parallelized.mp4\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>'\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sGsq8iTA_N9U"
      },
      "source": [
        "Here is code implementation of batched dot products:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ygFWDfQIoeC"
      },
      "source": [
        "First, we will create `8` random matrices (one for each available tpu devices - colab tpu's have 8 available [devices](https://cloud.google.com/tpu/docs/system-architecture-tpu-vm) or the 8 cpu cores as we configured)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MZLMx06_K_qR"
      },
      "outputs": [],
      "source": [
        "# Let create 8 keys, 1 for each batch\n",
        "keys = jax.random.split(jax.random.PRNGKey(0), 8)\n",
        "\n",
        "# Each replicated pmapped function get a different key\n",
        "mats = pmap(lambda key: jax.random.normal(key, (500, 600)))(keys)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6BkMsaOtLISj"
      },
      "source": [
        "The leading dimension here needs to equal the dimension of available devices (since we are sending a batch to each device)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gWrdv_2wLG4T"
      },
      "outputs": [],
      "source": [
        "print(mats.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HnqblcUsLaKZ"
      },
      "source": [
        "Using `pmap` to generate the batches ensures these batches are of type `ShardedDeviceArray`. This is similar to an ndarray, except each batch/shared is stored in the memory of multiple devices, so they can be used in subsequent `pmap` operations without moving data around between devices (GPU/TPU) and hosts (cpu). "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JAeaBCvcLQWg"
      },
      "outputs": [],
      "source": [
        "print(type(mats))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PVz0gOWG9pkr"
      },
      "outputs": [],
      "source": [
        "def dot_product_parallel():\n",
        "\n",
        "    # Run a local matmul on each device in parallel (no data transfer)\n",
        "    result = pmap(lambda x: jnp.dot(x, x.T))(\n",
        "        mats\n",
        "    ).block_until_ready()  # result.shape is (8, 5000, 5000)\n",
        "\n",
        "\n",
        "run_parallel = %timeit -o -n  5 dot_product_parallel()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "64gfyF3ENQzU"
      },
      "source": [
        "It is simple as that. Our dot product now runs in parallel across available devices (cpu, gpus or tpus). As we have more cores/devices, this code will automatically scale! "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5qcQXSbANP_M",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Let's plot the performance difference (Run Cell)\n",
        "\n",
        "jax_parallel_time = np.mean(run_parallel.all_runs)\n",
        "jax_seq_time = np.mean(run_sequential.all_runs)\n",
        "\n",
        "\n",
        "data = {\"JAX (seq)\": jax_seq_time, \"JAX (parallel - pmap)\": jax_parallel_time}\n",
        "\n",
        "plot_performance(data, title=\"Average time taken for Seq vs Parallel Dot Product\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-0j8iJRFUz6v"
      },
      "source": [
        "For some problems, the speed can be directly proportional to the number of devices -- $Nx$ speed up for $N$ devices! \n",
        "\n",
        "We showed an example of using `pmap` for *pure* parallelism, where there is no communication between devices. JAX also has various operations for communication across distributed devices ( more on this [here](https://jax.readthedocs.io/en/latest/jax-101/06-parallelism.html#communication-between-devices).)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sAtms17jtCOU"
      },
      "source": [
        "<hr>\n",
        "\n",
        "# PART 2 - Optimization using Optax\n",
        "\n",
        "<hr>\n",
        "\n",
        "[Optax](https://github.com/deepmind/optax), leverages from the capabilities of JAX, in particular automatic differentiation, to run optimization methods such as vanilla gradient descent, or more advanced ones such as ADAM, on any loss function we may have. \n",
        "\n",
        "Below we show how to use Optax to train a simple linear model of the form \n",
        "\n",
        "$$y=w\\tr x + b,\\,w \\in \\reals^m,\\;y,b \\in \\reals.$$\n",
        "\n",
        "Below is a little function to simulate noisy samples drawn from a linear model, and a classic mean squared loss function."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy import random\n",
        "\n",
        "def sim_linear_model(w, b, num_samples, noise_std=0.05):\n",
        "  \"\"\"\n",
        "  given a linear coefficientw vector w, a bias b and a non-negative integer n\n",
        "  generate n samples according to y = wx + b + v\n",
        "  where v is a random Gaussian variable with mean 0\n",
        "  and standard deviation 0.1\n",
        "  \"\"\"\n",
        "  sample_dim = len(w)\n",
        "  X = random.uniform(size=(sample_dim,num_samples))\n",
        "  noise = noise_std*random.normal(size=num_samples)\n",
        "  y = w @ X + b + noise\n",
        "  return X,y\n",
        "\n",
        "\n",
        "def loss(X, y, w, b):\n",
        "  \"\"\"\n",
        "  JAX-based implementation of mean squared loss\n",
        "  \"\"\"\n",
        "  errors = jnp.square(y - w.T @ X - b)\n",
        "  return jnp.mean(errors)\n",
        "\n",
        "#\n",
        "# ground truth\n",
        "#\n",
        "w = [0.4]\n",
        "b = -0.2\n",
        "#\n",
        "# draw samples\n",
        "#\n",
        "X,y = sim_linear_model(w, b, 100)\n"
      ],
      "metadata": {
        "id": "AKuLize6bkzY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Optax workflow\n",
        "\n",
        "Since JAX is pure function-based, Optax functions need to be pure functions too. Thus, in order to update the model's parameters, two functions are called in sequence:\n",
        "\n",
        "1. `optimizer.update(theta,state)` if the model parameters are $\\theta$, this function returns the update that needs to be applied to theta in order to obtain a new iterate: $\\Delta \\theta$. The `state` stores past information gathered by the optimizer which might influence the computation of $\\Delta \\theta$.\n",
        "1. `optax.apply(theta,delta_theta)` simply adds `delta_theta` to `theta`\n",
        "\n",
        "One important aspect of Optax is that it expects the model parameters to be provided as name-value pairs in a dictionary. In the linear model below we have two parameters $w$ and $b$ so that $\\theta=(w,b)$ and $\\Delta\\theta=(\\Delta w,\\Delta b)$.\n"
      ],
      "metadata": {
        "id": "cY6EQK582Phk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install -U optax\n",
        "\n",
        "import optax\n",
        "import jax.numpy.linalg as jla\n",
        "\n",
        "# \n",
        "# JAX autograd:  the returned value gloss is a function \n",
        "# that computes the gradient of loss(X,y,w,b) with respect to w and b (args 2 and 3)\n",
        "# \n",
        "gloss = jax.grad(loss, argnums=(2,3))\n",
        "\n",
        "\n",
        "def train_model_optax(X, y, fun, grad, stepsize=1e-2,tolerance=1e-4,maxiter=1000):\n",
        "  \"\"\"\n",
        "  optimization using Optax\n",
        "  :param X: reference inputs\n",
        "  :param y: reference output\n",
        "  :param fun: cost function \n",
        "  :param grad: gradient of the cost function\n",
        "  \"\"\"\n",
        "  #\n",
        "  # create optimizer\n",
        "  # this guy defines how to update the parameters in each iteration\n",
        "  #\n",
        "  optimizer = optax.sgd(stepsize) # TRY other optimizers and stepsizes!! optax.adam\n",
        "  #\n",
        "  # initial parameters\n",
        "  #\n",
        "  # Optax expects a dictionary parameters\n",
        "  # in this case we have only one: w\n",
        "  #\n",
        "  theta_t = {\"w\": jnp.zeros(X.shape[0]), \"b\":jnp.ones(1)}\n",
        "  #\n",
        "  # all Optax optimizers have an inner state that needs to \n",
        "  # be stored and updated between calls\n",
        "  #\n",
        "  state_t = optimizer.init(theta_t)\n",
        "  \n",
        "  params     = list()\n",
        "  costs      = list()\n",
        "  grad_norms = list()\n",
        "  #\n",
        "  # main optimization loop\n",
        "  #\n",
        "  for t in range(maxiter):\n",
        "    #\n",
        "    # values at current iteration\n",
        "    #\n",
        "    w_t = theta_t[\"w\"]\n",
        "    b_t = theta_t[\"b\"]\n",
        "    g_t = grad(X,y,w_t,b_t)\n",
        "    f_t = fun(X,y,w_t,b_t)\n",
        "    n_t = jla.norm(g_t)\n",
        "    #\n",
        "    # check for stopping condition\n",
        "    #\n",
        "    if n_t < tolerance:\n",
        "      break\n",
        "    #\n",
        "    # store current solution\n",
        "    #\n",
        "    params.append(theta_t)\n",
        "    costs.append(f_t)\n",
        "    grad_norms.append(n_t)\n",
        "    #\n",
        "    # update solution (here comes Optax)\n",
        "    #\n",
        "    # Optax expects a dictionary of gradients, one per parameter\n",
        "    # corresponding to the dictionary of parameters above\n",
        "    #\n",
        "    g_t = {\"w\":g_t[0],\"b\":g_t[1]} \n",
        "    d_t, state_t = optimizer.update(g_t, state_t)\n",
        "    theta_t = optax.apply_updates(theta_t, d_t)\n",
        "\n",
        "  return params,costs,grad_norms\n",
        "\n",
        "\n",
        "    "
      ],
      "metadata": {
        "id": "a1u1ZpbAqRTZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Running our optax-powered optimizer\n",
        "\n",
        "Run the cell below to see our latest method in action.\n",
        "\n",
        "<hr>\n",
        "\n",
        "**ExcerciseL** try again, now changing the optimizer and/or the stepsize (see [Common Optimizers](https://optax.readthedocs.io/en/latest/api.html) ). One popular one is ADAM (`optax.adam`)\n"
      ],
      "metadata": {
        "id": "j4Sxc76A49jO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_trajectory(params,costs,norms):\n",
        "  \"\"\"\n",
        "  helper function for plotting our thing\n",
        "  \"\"\"\n",
        "  niter = len(costs)\n",
        "\n",
        "  plt.figure(figsize=(15,5))\n",
        "\n",
        "  plt.subplot(1,3,1)\n",
        "  plt.semilogy(costs)\n",
        "  plt.xlabel('iteration')\n",
        "  plt.ylabel('loss')\n",
        "  plt.title('evolution of loss across iterations')\n",
        "  plt.grid(True)\n",
        "\n",
        "#\n",
        "# call the algorithm\n",
        "#\n",
        "params,costs,norms = train_model_optax(X, y, loss, gloss,stepsize=0.1)\n",
        "\n",
        "plot_trajectory(params,costs,norms)"
      ],
      "metadata": {
        "id": "YBN1Ov1dta8Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "exuVety_bFhQ"
      },
      "source": [
        "<hr>\n",
        "\n",
        "# PART 3 - Training Models with Haiku\n",
        "\n",
        "<hr>\n",
        "\n",
        "So far we've seen how to use JAX and Optax to implement an optimization loop.\n",
        "Thanks to these two libraries, we can exploit our computing resources to the maximum, forget about computing gradients by hand, and to define the descent direction altogether.\n",
        "\n",
        "What if we now want to build a deep neural model? If we want to do it with JAX, we need JAX-enabled blocks (dense, convolutional, relus, maxpool, etc.) and a way to construct and maintain the state of the resulting mammoth.\n",
        "\n",
        "Here's where [Haiku](https://github.com/deepmind/dm-haiku) comes in, by providing a JAX-based alternative to those familiar with  the traditional Pytorch or Tensorflow libraries.\n",
        "\n",
        "## Haiku is object oriented\n",
        "\n",
        "Haiku is an *object-oriented* library, meaning that our models, their parameters, etc., are represented as _instances_ of _object classes_. If you are not familiar with object oriented programming, you should learn this concept first. There are many tutorials on the subject ([example](https://realpython.com/python3-object-oriented-programming/)).\n",
        "\n",
        "## JAX is functional oriented\n",
        "\n",
        "However, there is a twist, as JAX follows the  _functional programming_ paradigm! \n",
        "\n",
        "Haiku modules are similar to standard python objects (they have references to their own parameters and functions). However, since JAX operates on *pure functions*, Haiku modules **cannot be directly instantiated**, but rather they need to be **wrapped into pure function transformations.**\n",
        "\n",
        "So, the overall rationale is to:\n",
        "\n",
        "* write an object for your model\n",
        "* wrap the object so that it is seen by JAX as a set of pure functions\n",
        "\n",
        "Don't worry. It sounds more complicated than it is."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "#@title Run this cell to install Haiku!\n",
        "!pip install -U dm-haiku\n"
      ],
      "metadata": {
        "id": "lMubruAF4ZDc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9wvTzTi-YJTp"
      },
      "source": [
        "## Wrapping a model in Haiku\n",
        "\n",
        "We'll now create a linear model in the way of Haiku. This linear model contains another parameter `b`, a constant (bias) term that is added to the linear combination, so that $y=wx+b$.\n",
        "\n",
        "The core object in Haiku is a `Module`. A module contains parameters and a function `__call__` that combines these parameters and user input to produce an output. This is what we could call a `block` within a Neural Network.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H_-3r49B-Orc"
      },
      "outputs": [],
      "source": [
        "import haiku as hk\n",
        "\n",
        "class MyLinearModel(hk.Module): # notice: model inherits from haiku.Module\n",
        "  \"\"\"\n",
        "  Haiku-based linear model of the form y=w*x+b\n",
        "\n",
        "  A Haiku module is expected to implement __call__ in order to produce\n",
        "  its output.\n",
        "  \"\"\"\n",
        "  \n",
        "  def __init__(self, output_dim, name=None):\n",
        "    \"\"\"\n",
        "    :param output_dim: dimension of the model output (y)\n",
        "    \"\"\"\n",
        "    super().__init__(name=name)\n",
        "    self.output_dim = output_dim\n",
        "\n",
        "\n",
        "  def __call__(self, x):\n",
        "    \"\"\"\n",
        "    special function, gets called when we use the () operator on an instance\n",
        "    \"\"\"\n",
        "    j, k = x.shape[-1], self.output_dim\n",
        "    #\n",
        "    # note: the three lines below are actually called *once* and serve\n",
        "    # to declare and initialize the parameters of the model.\n",
        "    # \n",
        "    w_init = hk.initializers.TruncatedNormal(1.0 / np.sqrt(j))\n",
        "    w = hk.get_parameter(\"w\", shape=[j, k], dtype=x.dtype, init=w_init)\n",
        "    b = hk.get_parameter(\"b\", shape=[k], dtype=x.dtype, init=jnp.ones)\n",
        "    #\n",
        "    # this is the actual code that's executed when the module is called\n",
        "    #\n",
        "    return jnp.dot(x, w) + b"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-XGOeJCH-10P"
      },
      "source": [
        "### Wrapping the Module\n",
        "\n",
        "In order for a module to be used by JAX, it needs to be treated not as an object, but as a set of pure functions plus a _state_, which has to be explicitly fed to the functions.\n",
        "\n",
        "The wrapping is done by the Haiku `transform` function. This wrapper takes a **function** of one or more Haiku Modules (not just any class) and \"eviscerates\" the modules it to bring out the parameters. \n",
        "\n",
        "In order for Haiku to know which parameters each  module has, the parameters need to be explicitly created and initialized using `hk.get_parameter`. In the module above, this is done in the `__call__` functions. \n",
        "\n",
        "(Admittedly, this is rather counter-intuitive, as it would seem that the parameters are created and initialized each time the `__call__` function is called. Also, they are not in the `__init__` method, which is the natural place for initializing things in an object. )\n",
        "\n",
        "So, the wrapping goes in two stages:\n",
        "\n",
        "1. we wrap the model (module) into a function\n",
        "1. we transform the function to obtain two other functions:\n",
        "   * an `init` function, that serves to initialize the parameters\n",
        "   * an `apply` function, which calls the function with the parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d1yI7j2h_Esd"
      },
      "outputs": [],
      "source": [
        "#\n",
        "# first step: wrap the module in a function\n",
        "#\n",
        "def model_fn(x):\n",
        "  \"\"\"\n",
        "  wrapper function for our model\n",
        "  \"\"\"\n",
        "  module = MyLinearModel(output_dim=1)\n",
        "  return module(x)\n",
        "\n",
        "#\n",
        "# second step: transform the function into a pair (init,apply)\n",
        "# \n",
        "wrapped_model = hk.transform(model_fn)\n",
        "print(wrapped_model)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Different flavours of transformation\n",
        "\n",
        "As you may know from our previous tutorial on JAX, pseudo-random numbers play a central role in JAX and, because of the pure function concept, their state needs to be passed explicitly anywhere they're used.\n",
        "\n",
        "With this in mind, the functions produced by `hk.transform` include the PRNG state explicitly as their first argument in all calls, as can be seen in the  signature of the wrapper functions returned by `hk.transform`:\n",
        "\n",
        "* `init(prng,data)`\n",
        "* `apply(prng,params,data)`\n",
        "\n",
        "If we don't want or don't need to use a PRNG, we can ask for a different set of wrapper functions which do not include the PRNG as an argument. In our case, our `init` function does use PRNG since the weights are initialized randomly through `hk.initializers.TruncatedNormal`, but  our module's output is deterministic ($y=wx+b$). Accordingly, we'd like our signatures to be:\n",
        "\n",
        "* `init(prng)`\n",
        "* `apply(params,input)`\n",
        "\n",
        "This is achieved by calling `hk.without_apply_rng` to our transformed model:"
      ],
      "metadata": {
        "id": "i0OC1iQVS71H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "# transform again to obtain another pair \n",
        "#\n",
        "wrapped_model = hk.without_apply_rng(wrapped_model)\n",
        "#\n",
        "# inspect the output and notice how the `apply` function has changed \n",
        "# compared to the previous value\n",
        "#\n",
        "print(wrapped_model)"
      ],
      "metadata": {
        "id": "Bv-q5rw7S7d_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lao8wS3tBjc3"
      },
      "source": [
        "### Test the wrapper model\n",
        "\n",
        "Let's see how the wrapped model works.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nt0srU3rQlhL"
      },
      "outputs": [],
      "source": [
        "#\n",
        "# input dimention we are considering\n",
        "#\n",
        "input_dim = 3\n",
        "#\n",
        "# evaluate our model somewhere\n",
        "#\n",
        "some_x= jnp.arange(input_dim, dtype=jnp.float32)\n",
        "print(some_x)\n",
        "#\n",
        "# initialize PRNG, a must in JAX\n",
        "#\n",
        "rng_key = jax.random.PRNGKey(42)\n",
        "#\n",
        "# initialization DOES use the PRNG, and (possibly) the data too\n",
        "#\n",
        "params = wrapped_model.init(rng=rng_key, x=some_x)\n",
        "print(params)\n",
        "#\n",
        "# try it out\n",
        "#\n",
        "print(wrapped_model.apply(params,some_x))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7IaqVuRPg3ER"
      },
      "source": [
        "# Training with Haiku and Optax\n",
        "\n",
        "\n",
        "Here we show a full training loop, using Haiku and Optax. For convenience, we introduce structures like `TrainingState` and functions like `init`,`update` and `loss_fn`. Please read through to get comfortable with how you can effectively train JAX models."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Load packages\n",
        "%%capture\n",
        "# need to install this for plotting.\n",
        "!pip install optax\n",
        "!pip install dm-haiku\n",
        "#\n",
        "# packages\n",
        "#\n",
        "from typing import Any, MutableMapping, NamedTuple, Tuple\n",
        "import time\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "import haiku as hk\n",
        "import optax\n"
      ],
      "metadata": {
        "id": "LY0t6C4OKzSK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Define and wrap our model\n",
        "\n",
        "class MyLinearModel(hk.Module): # notice: model inherits from haiku.Module\n",
        "  \"\"\"\n",
        "  Haiku-based linear model of the form y=w*x+b\n",
        "\n",
        "  A Haiku module is expected to implement __call__ in order to produce\n",
        "  its output.\n",
        "  \"\"\"\n",
        "  \n",
        "  def __init__(self, output_dim, name=None):\n",
        "    \"\"\"\n",
        "    :param output_dim: dimension of the model output (y)\n",
        "    \"\"\"\n",
        "    super().__init__(name=name)\n",
        "    self.output_dim = output_dim\n",
        "\n",
        "\n",
        "  def __call__(self, x):\n",
        "    \"\"\"\n",
        "    special function, gets called when we use the () operator on an instance\n",
        "    \"\"\"\n",
        "    j, k = x.shape[-1], self.output_dim\n",
        "    #\n",
        "    # note: the three lines below are actually called *once* and serve\n",
        "    # to declare and initialize the parameters of the model.\n",
        "    # \n",
        "    w_init = hk.initializers.TruncatedNormal(1.0 / np.sqrt(j))\n",
        "    w = hk.get_parameter(\"w\", shape=[j, k], dtype=x.dtype, init=w_init)\n",
        "    b = hk.get_parameter(\"b\", shape=[k], dtype=x.dtype, init=jnp.ones)\n",
        "    #\n",
        "    # this is the actual code that's executed when the module is called\n",
        "    #\n",
        "    return jnp.dot(x, w) + b\n",
        "\n",
        "#\n",
        "# wrap the model into a function\n",
        "#\n",
        "def model_fn(x):\n",
        "  \"\"\"\n",
        "  same wrapper function for Haiku modules, repeated for clarity\n",
        "  \"\"\"\n",
        "  module = MyLinearModel(output_dim=1)\n",
        "  return module(x).ravel()\n",
        "#\n",
        "# transform using Haiku as a pair init,apply\n",
        "#\n",
        "model = hk.without_apply_rng(hk.transform(model_fn))\n"
      ],
      "metadata": {
        "id": "0epd5-xgZPcW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from jax import random as jrand\n",
        "from jax.numpy import linalg as jla\n",
        "\n",
        "def loss_fun_haiku(params: hk.Params, X, y):\n",
        "  \"\"\"\n",
        "  our loss function\n",
        "  \"\"\"\n",
        "  y_pred = model.apply(params, X)\n",
        "  return jnp.mean((y_pred - y) ** 2)\n",
        "\n",
        "\n",
        "def train_model_haiku(X, y, model, loss_fun, stepsize=1e-3,tolerance=1e-4,maxiter=1000,seed=42):\n",
        "  \"\"\"\n",
        "  optimization using Optax\n",
        "  :param X: reference inputs\n",
        "  :param y: reference output\n",
        "  :param fun: cost function \n",
        "  :param grad: gradient of the cost function\n",
        "  \"\"\"\n",
        "  #\n",
        "  # create optimizer\n",
        "  # this guy defines how to update the parameters in each iteration\n",
        "  #\n",
        "  optimizer = optax.sgd(stepsize) # TRY other optimizers and stepsizes!! optax.adam\n",
        "  #\n",
        "  # initialization\n",
        "  #\n",
        "  rng = jrand.PRNGKey(seed)\n",
        "  rng, init_rng = jrand.split(rng)\n",
        "  theta_t       = model.init(init_rng, X)\n",
        "  state_t       = optimizer.init(theta_t)\n",
        "  \n",
        "  params     = list()\n",
        "  losses     = list()\n",
        "  grad_norms = list()\n",
        "  #\n",
        "  # main optimization loop\n",
        "  #\n",
        "  for t in range(maxiter):\n",
        "    #\n",
        "    # values at current iteration\n",
        "    #\n",
        "    loss_t, gtheta_t  = jax.value_and_grad(loss_fun)(theta_t, X, y)\n",
        "    n_t = jnp.sqrt(jnp.sum(jnp.square(gtheta_t[\"my_linear_model\"][\"w\"])) + jnp.sum(jnp.square(gtheta_t[\"my_linear_model\"][\"b\"])))\n",
        "    #\n",
        "    # check for stopping condition\n",
        "    #\n",
        "    if t > 0 and t % 100: \n",
        "      print(n_t)\n",
        "      \n",
        "    if n_t < tolerance:\n",
        "      break\n",
        "    #\n",
        "    # store current solution\n",
        "    #\n",
        "    params.append(theta_t)\n",
        "    losses.append(loss_t)\n",
        "    grad_norms.append(n_t)\n",
        "    #\n",
        "    # update solution (here comes Optax)\n",
        "    #\n",
        "    dtheta_t, state_t = optimizer.update(gtheta_t, state_t)\n",
        "    theta_t           = optax.apply_updates(theta_t, dtheta_t)\n",
        "\n",
        "  return params,losses,grad_norms\n",
        "\n",
        "\n",
        "params,losses,norms = train_model_haiku(X, y, model, loss_fun_haiku, stepsize=1e-2)\n",
        "\n",
        "plot_trajectory(params,losses,norms)\n",
        "    \n"
      ],
      "metadata": {
        "id": "29k-YKgkaspy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<hr>\n",
        "\n",
        "# PART 4 - Convolutional Neural Networks\n",
        "\n",
        "<hr>\n",
        "\n",
        "We have seen how to create and train a simple model using Haiku. Now we will use the built in models and blocks of Haiku to create and train a deep convolutional network. In the process, we will learn about convolutional networks, one of the core blocks in modern neural models.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "2-ESLy53gDLp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Module Prerequisites (run if you skipped previous modules)\n",
        "!pip install optax # repeated install, if you want to run just PART 3\n",
        "!pip install dm-haiku \n",
        "\n",
        "import jax.numpy as jnp\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "import os # for file i/o\n",
        "\n",
        "#\n",
        "# basic libraries\n",
        "#\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "#\n",
        "# JAX, Optax and Haiku\n",
        "#\n",
        "import jax\n",
        "from jax import numpy as jnp\n",
        "from jax import random as jrand\n",
        "from jax.numpy import linalg as jla\n",
        "from jax import grad, jit, vmap, pmap\n",
        "\n",
        "import optax\n",
        "\n",
        "import haiku as hk\n",
        "\n",
        "import tensorflow as tf # for the datasets\n",
        "\n",
        "\n",
        "if \"COLAB_TPU_ADDR\" in os.environ and os.environ[\"COLAB_TPU_ADDR\"]:\n",
        "    print(\"A TPU is connected.\")\n",
        "    import jax.tools.colab_tpu\n",
        "\n",
        "    jax.tools.colab_tpu.setup_tpu()\n",
        "\n"
      ],
      "metadata": {
        "id": "ywc4u5sm4XVl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u5_zoqk-YK0D"
      },
      "source": [
        "## Preliminaries - color image representation\n",
        "\n",
        "As you may know, there are different types of digital images. The simplest ones are grayscale images, where a pixel is a single scalar value (e.g., 254) where the value 0 is black and the value 255 is white (if pixels are floating point values, the convention is that 0.0 is black and 1.0 is white). \n",
        "\n",
        "In general, however, an image can have several _channels_. Typical color images have three channels: red, green and blue (RGB - generally in that order). In such cases, an RGB pixel's value is a _vector_ of three values `[r,g,b]`.\n",
        "\n",
        "More generally, an _hyperspectral image_ (e.g., with infrared bands) can have any number of channels.\n",
        "\n",
        "The convention used in Python image processing/computer vision/machine learning libraries is to represent images as _three-dimensional tensors_. There are two conventions:  _channels first_ and _channels_last_. As the name implies, a _channels first_ image is a $c{\\times}h{\\times}w{\\times}$ array where $h$ and $w$ are the height and width of the image, and $c$ is the number of channels. So for example, a typical 640x480 RGB image will have dimensions 3x640x480.\n",
        "\n",
        "The more common convention is the _channels last_ one. In this case, a 640x480 image will have dimension 640x480x3, where the _last_ index indicates the channel.\n",
        "\n",
        "\n",
        "## Color images as network inputs\n",
        "\n",
        "As input data, even small images are huge. To see this, consider a simple, shallow network with a single fully connected layer of 500 neurons whose inputs are 100x100 RGB images: **how many parameters would this network have?**. Do this calculation  before continuing!\n",
        "\n",
        "## Convolutional layers\n",
        "\n",
        "**CONTINUE: this text below can be greatly improved**\n",
        "\n",
        "ConvNets address this model parameter issue by exploiting structure in the inputs to the network (in particular, by making the assumption that the input is a 3-D *volume*, which applies to images for example, where the 3 dimensions consist of the three RGB channels). The two key differences between a ConvNet and a Feed-forward network are:\n",
        "\n",
        "* ConvNets have neurons that are arranged in 3 dimensions: width, height, depth. Note that *depth* here means channels, i.e. the depth of the input volume, not the depth of a deep neural network!\n",
        "* The neurons in each layer are only connected to a small region of the layer before it.\n",
        "\n",
        "**QUESTION**: Unfortunately there is no such thing as a free lunch. What trade-off do you think a ConvNet makes for the reduction in memory required by fewer parameters?\n",
        "\n",
        "Generally, a ConvNet architecture is made up of different types of layers, the most common being convolutional layers, pooling layers and fully-connected layers that we encountered in the last practical."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SFvYLebFZzGG"
      },
      "source": [
        "## Optional reading: The rise of deep convolutional architectures"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kEtb0aaeZ6os"
      },
      "source": [
        "ConvNet architectures were key to the tremendous success of deep learning in machine vision. In particular, the first deep learning model to win the ImageNet competition in 2012 was called AlexNet (after Alex Krizhevsky, one of its inventors). It had 5 convolutional layers followed by 3 fully-connected layers. Later winners included GoogLeNet and ResNet. If you're curious, have a look at [this link](https://medium.com/towards-data-science/neural-network-architectures-156e5bad51ba) for a great summary of different ConvNet architectures."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vOp67cfpaTlC"
      },
      "source": [
        "## Convolutional layer architecture"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6JbXSVKulDoD"
      },
      "source": [
        "A 2-dimensional convolutional layer maps an input *volume* (meaning, a 3-D input tensor, e.g. [width, height, channels]) to an output *volume* through a set of learnable filters, which make up the parameters of the layer. Every filter is small spatially (along width and height), but extends through the full depth of the input volume. (Eg: A filter in the first layer of a ConvNet might have size [5, 5, 3]). During the forward pass, we convolve (\"slide\") each filter across the width and height of the input volume and compute element-wise dot products between the entries of the filter and the input at any position. As we slide the filter over the width and height of the input volume we will produce a 2-dimensional activation map that gives the responses of that filter at every spatial position. Each convolutional layer will have such a set of filters, and each of them will produce a separate 2-dimensional activation map. We then stack these activation maps along the depth-dimension to produce the output volume.\n",
        "\n",
        "By using these filters which map to a small sub-volume of the input, we can to a large extent, control the parameter explosion that we would get with a (fully-connected) feed-forward network. This **parameter sharing** actually also tends to improve the performance of the model on inputs like natural images because it provides the model with some limited **translation invariance**. Translation invariance means that if the image (or a feature in the image) is translated (moved), the model will not be significantly affected. Think about why this is the case!\n",
        "\n",
        "The following animation illustrates these ideas, make sure you understand them!\n",
        "\n",
        "![Convolution Animation](https://i.stack.imgur.com/FjvuN.gif)\n",
        "\n",
        "If the parameter sharing aspect of CNNs is still not clear, consider the following diagram which compares a simplified 1-D convolutional layer with a fully-connected layer. The diagram shows how a 1-dimensional input  $\\mathbf{x}$ is mapped to a 1-dimensional output  $\\mathbf{y}$ using both a fully-connected layer and a convolution layer, both without bias parameters. The colours of the edges represent the value of the weight parameters in the layers. For the fully-connected layer, the number of weights is the product of the input and output sizes, in this case, $6 \\times 4 = 24$. On the other hand, the number of weights in the convolutional layer depends only on the filter size of the convolution, in this case, $3$, and is independent of the input and output sizes.\n",
        "\n",
        "![Weight Sharing](https://i.imgur.com/gcmmZz4.png)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eLiuT6TcmXw8"
      },
      "source": [
        "The hyper-parameters of a convolutional layer are as follows:\n",
        "* **Filters** defines the number of filters in the layer\n",
        "* **Kernel Size** defines the width and height of the filters (also called \"kernels\") in the layer. Note that kernels always have the same depth as the inputs to the layer.\n",
        "* **Stride** defines the number of pixels by which we move the filter when \"sliding\" it along the input volume. Typically this value would be 1, but values of 2 and 3 are also sometimes used.\n",
        "* **Padding** refers to the addition of 0-value pixels to the edges of the input volume along the width and height dimensions. In Tensorflow you can set this to \"VALID\", which essentially does no padding or \"SAME\" which pads the input such that the output width and height are the same as the input.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oW5FkOM2EOr3"
      },
      "source": [
        "## Optional reading: Building complex filters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F__JkeqMEbzv"
      },
      "source": [
        "One of the reasons that CNNs have been so successful is their ability to build up complex filters by composing more simple filters. For example, imagine a 5 layer CNN that has been trained to detect faces. The first 4 layers are convolutional and the last layer is fully-connected and outputs the final prediction (is there a face or not). We might find that the filters in each convolution layer pick out the following features:\n",
        "\n",
        "1. lines (horizontal, vertical, diagonal), and colour gradients,\n",
        "2. corners, circles and other simple shapes, and simple textures,\n",
        "3. noses, mouths, and eyes,\n",
        "4. whole faces.\n",
        "\n",
        "The neural net has learned to pick out complex objects like facial features and even whole faces! The reason for this is that each successive layer can combine the filters from the previous layer to detect more and more sophisticated features. The following diagram (adapted from [this paper](http://web.eecs.umich.edu/~honglak/icml09-ConvolutionalDeepBeliefNetworks.pdf)) shows some really cool examples of this kind of behaviour. The lower level features (shown above) detect noses, eyes, and mouths, in the case of faces, and wheels, doors, and windows, for cars. The higher level features are then able to detect whole faces and cars.\n",
        "\n",
        "![Imgur](https://i.imgur.com/653uIty.jpg)\n",
        "\n",
        "The diagrams on page 7 of [this classic paper](https://cs.nyu.edu/~fergus/papers/zeilerECCV2014.pdf) show more examples of this phenomena and are definitely worth checking out!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LhgDU-fVx2Jv"
      },
      "source": [
        "## (Max) Pooling\n",
        "A pooling layer reduces the spatial size of the representation. There are different reasons why we may want to do this. One is to reduce the number of parameters in the network. Imagine a convnet for the MNIST dataset. If the feature tensor produced by the final conv/pool/relu layer was say, of size 20x20 and had 100 feature channels, the final dense layer would have 20x20x100x10=400k parameters. However, if we down-sampled that layer to a 4x4 spatial size, we would have only 20k parameters. A big difference!\n",
        "\n",
        "Another reason is that we want later features (deeper in the network) to have larger *receptive fields* (input regions that they look at), in order to represent bigger objects and object parts for instance. In particular, pooling stride gives later features much larger receptive fields so that they can effectively combine smaller features together.\n",
        "\n",
        "A pooling layer has no trainable parameters. It applies some 2-D aggregation operation (usually a max(), but others like average() may also be used) to regions of the input volume. This is done independently for each depth dimension of the input. For example, a 2x2 max pooling operation with a stride of 2, downsamples every depth slice of the input by 2 along both the width and height.\n",
        "\n",
        "The hyper-parameters of a pooling layer are as follows:\n",
        "* `window_shape` how many values are aggregated together.\n",
        "* `stride` defines the number of pixels by which we move the pooling filter when sliding it along the input. Typically this value would be equal to the pool size.\n",
        "* `padding` refers to the addition of 0-value pixels to the edges of the input volume along the width and height dimensions. In Tensorflow you can set this to \"VALID\", which essentially does no padding or \"SAME\" which pads the input such that the output width and height are the same as the input.\n",
        "\n",
        "#### Question\n",
        "Do 2x2 max-pooling by hand, with a stride of 2, and \"VALID\" padding, on the following 2D input. What is the size of the output?\n",
        "\n",
        "\\begin{bmatrix}\n",
        "  9 & 5 & 4 & 5 & 6 & 4 \\\\\n",
        "  6 & 6 & 3 & 5 & 8 & 2 \\\\\n",
        "  4 & 6 & 9 & 1 & 3 & 6 \\\\\n",
        "  9 & 7 & 1 & 5 & 8 & 1 \\\\\n",
        "  4 & 9 & 9 & 5 & 7 & 3 \\\\\n",
        "  7 & 3 & 6 & 4 & 9 & 1 \n",
        "\\end{bmatrix}\n",
        "\n",
        "\n",
        "Reveal the cell below by double-clicking and running it, to check your answer when you're done!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e1oIm3Nlb9zY"
      },
      "outputs": [],
      "source": [
        "#@title Answer { display-mode: \"form\" }\n",
        "#import numpy as np\n",
        "#import\n",
        "X = np.array([[9, 5, 4, 5, 6, 4],\n",
        "              [6, 6, 3, 5, 8, 2],\n",
        "              [4, 6, 9, 1, 3, 6],\n",
        "              [9, 7, 1, 5, 8, 1],\n",
        "              [4, 9, 9, 5, 7, 3],\n",
        "              [7, 3, 6, 4, 9, 1]])\n",
        "\n",
        "def max_pool_fun(x):\n",
        "  mp = hk.MaxPool(window_shape=(2, 2), strides=2, padding='VALID')\n",
        "  return mp(x)\n",
        "\n",
        "max_pool = hk.without_apply_rng(hk.transform(max_pool_fun))\n",
        "params = max_pool.init(None,X)\n",
        "print('pooled:\\n',max_pool.apply(params,X))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XD8JDduMNh7V"
      },
      "source": [
        "## Optional reading: Receptive fields\n",
        "\n",
        "Earlier we mentioned that one reason to do pooling is to increase the sizes of the receptive fields of our features. Let's take a closer look at what we meant by this. \n",
        "\n",
        "The diagram below shows the effective receptive field of one output \"neuron\" in each layer of a few simple networks. What the diagram tells us is how many of the input values have an effect on each output value.\n",
        "\n",
        "We can see that in the first two examples, with single convolutional layers, the receptive field is simply equal to the kernel size. \n",
        "\n",
        "However, the next two examples are a little more interesting. Here we have drastically increased the receptive field size, without a large increase in the number of parameters, by stacking convolution and pooling layers. The interesting thing here is that by using a pooling layer we increased our receptive field size by a much smaller cost (in the number of parameters) than if we'd simply increased the kernel sizes of our convolution layers.\n",
        "\n",
        "You can read more about receptive fields [here](https://medium.com/mlreview/a-guide-to-receptive-field-arithmetic-for-convolutional-neural-networks-e0f514068807).\n",
        "\n",
        "\n",
        "![Receptive Fields](https://i.imgur.com/TjxEsG4.png)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e4Vsrgyudd2E"
      },
      "source": [
        "## The CIFAR10 Dataset\n",
        "Now that we understand convolutional, max-pooling and feed-forward layers, we can combine these as building blocks to build a ConvNet classifier for images. For this practical, we will use the colour image dataset CIFAR10 (pronounced \"seefar ten\") which consists of 50,000 training images and 10,000 test images. As we did in Practical 1, we take 10,000 images from the training set to form a validation set and visualise some example images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "flWYFg3ydvMU"
      },
      "outputs": [],
      "source": [
        "cifar = tf.keras.datasets.cifar10\n",
        "(all_train_images, all_train_labels), (all_test_images, all_test_labels) = cifar.load_data()\n",
        "all_test_images = all_test_images.astype(float)*(1/255)\n",
        "all_train_images = all_train_images.astype(float)*(1/255)\n",
        "\n",
        "cifar_labels = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QSzdYWpZd8RE"
      },
      "outputs": [],
      "source": [
        "# Take the last 10000 images from the training set to form a validation set\n",
        "ntrain = 10000\n",
        "ntest = 1000\n",
        "\n",
        "all_train_labels = all_train_labels.squeeze()\n",
        "train_images = all_train_images[:ntrain, :, :, :]\n",
        "train_labels = all_train_labels[:ntrain]\n",
        "\n",
        "all_test_labels = all_test_labels.squeeze()\n",
        "test_images = all_test_images[-ntest:, :, :, :]\n",
        "test_labels = all_test_labels[-ntest:]\n",
        "#\n",
        "# convert to JAX\n",
        "#\n",
        "X_train, X_test, Y_train, Y_test = jnp.array(train_images, dtype=jnp.float32),\\\n",
        "                                   jnp.array(test_images,  dtype=jnp.float32),\\\n",
        "                                   jnp.array(train_labels, dtype=jnp.float32),\\\n",
        "                                   jnp.array(test_labels,  dtype=jnp.float32)\n",
        "\n",
        "classes =  jnp.unique(Y_train)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x8jf1myGQP1O"
      },
      "source": [
        "What are the shapes and data-types of train_images and train_labels?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wzLutGn-P7mg"
      },
      "outputs": [],
      "source": [
        "print('all_images.shape = {}, data-type = {}'.format(all_train_images.shape, all_train_images.dtype))\n",
        "print('all_labels.shape = {}, data-type = {}'.format(all_train_labels.shape, all_train_labels.dtype))\n",
        "print('train_images.shape = {}, data-type = {}'.format(X_train.shape, X_train.dtype))\n",
        "print('train_labels.shape = {}, data-type = {}'.format(Y_train.shape, Y_train.dtype))\n",
        "print('train_images.shape = {}, data-type = {}'.format(X_test.shape, X_test.dtype))\n",
        "print('train_labels.shape = {}, data-type = {}'.format(Y_test.shape, Y_test.dtype))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ynwzGIAneBbb"
      },
      "source": [
        "### Visualise examples from the dataset\n",
        "Run the cell below multiple times to see various images. (They might look a bit blurry because we've blown up the small images.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8nMTxCOjd9WW"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10,10))\n",
        "for i in range(25):\n",
        "  plt.subplot(5,5,i+1)\n",
        "  plt.xticks([])\n",
        "  plt.yticks([])\n",
        "  plt.grid('off')\n",
        "\n",
        "  img_index = np.random.randint(0, 10000)\n",
        "  plt.imshow(train_images[img_index])\n",
        "  plt.xlabel(cifar_labels[train_labels[img_index]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wN-XUzp-fpyp"
      },
      "source": [
        "## A ConvNet Classifier\n",
        "Finally, we build a simple convolutional architecture to classify the CIFAR images. We will build a mini version of the AlexNet architecture, which consists of 5 convolutional layers with max-pooling, followed by 3 fully-connected layers at the end. In order to investigate the effect each of these two layers has on the number of parameters, we'll build the model in two stages. \n",
        "\n",
        "First, the convolutional layers + max-pooling:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q9zloewLws0b"
      },
      "outputs": [],
      "source": [
        "#@title Create Network, Haiku way\n",
        "\n",
        "#\n",
        "# Haiku module\n",
        "#\n",
        "class ConvNetModule(hk.Module):\n",
        "\n",
        "  def __init__(self):      \n",
        "    \"\"\"\n",
        "    add some layers to the net\n",
        "    \"\"\"\n",
        "    super().__init__(name=\"ConvNetModule\")\n",
        "    initializer  = hk.initializers.VarianceScaling(1.0,\"fan_avg\",\"truncated_normal\") # Glorot\n",
        "    self.conv1   = hk.Conv2D(output_channels=48, kernel_shape=(3,3), padding=\"SAME\",w_init=initializer)    \n",
        "    self.pool1   = hk.MaxPool(window_shape=(3,3),strides=(1,1),padding=\"SAME\")\n",
        "\n",
        "    self.conv2   = hk.Conv2D(output_channels=128, kernel_shape=(3,3), padding=\"SAME\",w_init=initializer)\n",
        "    self.pool2   = hk.MaxPool(window_shape=(3,3),strides=(1,1),padding=\"SAME\")\n",
        "    \n",
        "    self.conv3_1 = hk.Conv2D(output_channels=192, kernel_shape=(3,3), padding=\"SAME\",w_init=initializer)    \n",
        "    self.conv3_2 = hk.Conv2D(output_channels=192, kernel_shape=(3,3), padding=\"SAME\",w_init=initializer)    \n",
        "    self.conv3_3 = hk.Conv2D(output_channels=128, kernel_shape=(3,3), padding=\"SAME\",w_init=initializer)    \n",
        "    self.pool3 = hk.MaxPool(window_shape=(3,3),strides=(1,1),padding=\"SAME\")\n",
        "\n",
        "    self.flatten = hk.Flatten()\n",
        "    self.linear1 = hk.Linear(output_size=1024,w_init=initializer) # was 1024\n",
        "\n",
        "    self.linear2 = hk.Linear(output_size=1024,w_init=initializer) # was 1024, reduced due to OOM\n",
        "    self.linear3 = hk.Linear(output_size=len(classes),w_init=initializer)\n",
        "\n",
        "\n",
        "  def __call__(self, x_batch):\n",
        "    \"\"\"\n",
        "    apply the network to given inputs\n",
        "    \"\"\"\n",
        "\n",
        "    x = self.conv1(x_batch)\n",
        "    x = jax.nn.relu(x)\n",
        "    x = self.pool1(x)\n",
        "\n",
        "    x = self.conv2(x)\n",
        "    x = jax.nn.relu(x)\n",
        "    x = self.pool2(x)\n",
        "\n",
        "    x = self.conv3_1(x)\n",
        "    x = self.conv3_2(x)\n",
        "    x = self.conv3_3(x)\n",
        "    x = self.pool3(x)\n",
        "\n",
        "    x = self.flatten(x)\n",
        "    x = self.linear1(x)\n",
        "    \n",
        "    coin = hk.next_rng_key()\n",
        "    x = hk.dropout(coin,0.5,x)\n",
        "    x = jax.nn.relu(x)\n",
        "\n",
        "    x = self.linear2(x)\n",
        "    x = jax.nn.relu(x)\n",
        "\n",
        "    x = self.linear3(x)\n",
        "    x = jax.nn.softmax(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "#\n",
        "# wrap it into a function\n",
        "#\n",
        "def cnn_fun(x):\n",
        "    cnn = ConvNetModule()\n",
        "    return cnn(x)\n",
        "\n",
        "#\n",
        "# transform into functional pair: init, apply\n",
        "#\n",
        "conv_net = hk.transform(cnn_fun) \n",
        "\n",
        "#\n",
        "# define the network loss\n",
        "#\n",
        "def cnn_loss(_params, _key, _X, _y_true):\n",
        "  _y_pred     = conv_net.apply(_params, _key, _X)\n",
        "  _y_true_hot = jax.nn.one_hot(_y_true, num_classes=len(classes))\n",
        "  _log_y_pred = jnp.log(_y_pred)\n",
        "  return -jnp.mean(_y_true_hot *_log_y_pred) # logit cost \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5uzeJaMB7DPa"
      },
      "source": [
        "### Optional reading: initialization schemes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AUBKcjfJ6gRs"
      },
      "source": [
        "You might have wondered what values we are using for the initial values of the weights and biases in our model. The short answer is that we typically use random initialization. In this case, we have just been using the default Keras initializers for each layer, which are usually sufficient.\n",
        "\n",
        "The longer answer is that just using completely random numbers does not always work best in practice and that there are a number of common initialization schemes (which are available in most deep learning frameworks such as TensorFlow and Keras).\n",
        "\n",
        "Let's consider a few examples:\n",
        "\n",
        " * When using the ReLU activation it is common to initialize the biases with small positive numbers because this encourages the ReLU activations to start off in the _on_ state, which helps to counteract the _dying ReLU problem_.\n",
        "\n",
        " * The deeper neural networks become the more likely it is that gradients will either shrink to the point that they vanish, or grow to the point that they overflow (the _vanishing_ and _exploding_ gradients problems). To help combat this we can initialize our weights to have a (model-specific) appropriate scale. One method for doing this is called [Xavier or Glorot](http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf) initialization.\n",
        "\n",
        " * The _Xavier_ initialization scheme was designed with the _traditional_ activations Sigmoid and TanH in mind and does not work as well for ReLU activations. An alternative is [He or Kaiming](https://arxiv.org/pdf/1502.01852.pdf) initialization which is a modification of Xavier initialization for ReLU activations.\n",
        "\n",
        " [This blog](http://andyljones.tumblr.com/post/110998971763/an-explanation-of-xavier-initialization) goes into more detail on _He_ and _Xavier_ initialization. [The Keras documentation](https://keras.io/initializers/) lists a number of common schemes. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XFrH0OVORco2"
      },
      "source": [
        "### Training and Validating the model\n",
        "In the last practical we wrote out the dataset pipeline, loss function and training-loop to give you a good appreciation for how it works. This time, we use the training loop built-in to Keras. For simple, standard datasets like CIFAR, doing it this way will work fine, but it's important to know what goes on under the hood because you may need to write some or all of the steps out manually when working with more complex datasets! "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "def train_cnn_haiku(_train_data, _model, _loss_fun, \n",
        "                    _stepsize=1e-4,_epochs=25, _batch_size=100, \n",
        "                    _patience=1, _seed=42):\n",
        "  \"\"\"\n",
        "  optimization using Optax\n",
        "  :param _train_data: tuple (X_train,y_train,X_test,y_test)\n",
        "  :param _model: target model to be optimized\n",
        "  :param _loss_fun: loss function\n",
        "  :param _stepsize: how fast should we update parameters\n",
        "  :param _tolerance: if performance does not improve at least this ratio, stop\n",
        "  :param _batch_size: how many samples to evaluate at each batch\n",
        "  :param _seed: random seed. If you don't know why this is 42, read the Hitchhiker's Guide to the Galaxy by Douglas Adams\n",
        "  \"\"\"\n",
        "  _X_train, _y_train, _X_val, _y_val = _train_data\n",
        "  #\n",
        "  # create optimizer\n",
        "  # this guy defines how to update the parameters in each iteration\n",
        "  # try others! https://optax.readthedocs.io/en/latest/api.html\n",
        "  #\n",
        "  _optimizer = optax.adam(_stepsize)\n",
        "  #\n",
        "  # initialization\n",
        "  # note: handling random numbers in jax is delicate\n",
        "  # you have to carry the next key yourself\n",
        "  # see https://jax.readthedocs.io/en/latest/jax-101/05-random-numbers.html\n",
        "  #\n",
        "  _key = jrand.PRNGKey(_seed)\n",
        "  _key, _subkey = jrand.split(_key) \n",
        "  #\n",
        "  # initial model parameters\n",
        "  #\n",
        "  _theta_t       = _model.init(_subkey, _X_train[:5,:])\n",
        "  #\n",
        "  # initial optimizer state\n",
        "  #\n",
        "  _state_t       = _optimizer.init(_theta_t)\n",
        "  \n",
        "  _nsamples = _X_train.shape[0]\n",
        "  _indexes = jnp.arange(_nsamples)\n",
        "  _nbatch = int(np.ceil(_nsamples / _batch_size))\n",
        "  _batches = list(_batch_size*i for i in range(_nbatch))\n",
        "  _batches.append(None) # tail\n",
        "\n",
        "  print('total number of samples:', _nsamples)\n",
        "  print('number of batches      :', _nbatch)\n",
        "  print('batch size             :', _batch_size)\n",
        "\n",
        "  _loss_history  = list()\n",
        "  _no_improvement = 0\n",
        "  #\n",
        "  # epochs\n",
        "  #\n",
        "  for t in range(_epochs):\n",
        "    #\n",
        "    # perform a number of batches\n",
        "    #\n",
        "    t0 = time.time()\n",
        "    _train_losses = list()\n",
        "    _val_losses   = list()\n",
        "    for i in range(_nbatch):\n",
        "      print('.',end='',flush=True)\n",
        "      _X_batch = _X_train[_batches[i]:_batches[i+1],:]\n",
        "      _y_batch = _y_train[_batches[i]:_batches[i+1]]\n",
        "      #del batch_idx\n",
        "      #\n",
        "      # current iterate\n",
        "      #\n",
        "      _key,_subkey = jrand.split(_key)\n",
        "      _train_loss_t, _grad_t  = jax.value_and_grad(_loss_fun)(_theta_t, _key, _X_batch, _y_batch)\n",
        "      _val_loss_t             = _loss_fun(_theta_t, _key, _X_val, _y_val)\n",
        "      _train_losses.append(_train_loss_t)\n",
        "      _val_losses.append(_val_loss_t)\n",
        "      del _X_batch,_y_batch\n",
        "      #\n",
        "      # optimization may go bananas for large stepsize\n",
        "      #\n",
        "      if np.isnan(_train_loss_t):\n",
        "        print(f\"training became unstable! Try using a step size smaller than {_stepsize}.\")\n",
        "        return None\n",
        "      #\n",
        "      # update iterate\n",
        "      #\n",
        "      _update_t, _state_t = _optimizer.update(_grad_t, _state_t) \n",
        "      del _grad_t\n",
        "      _theta_t = optax.apply_updates(_theta_t, _update_t)\n",
        "      del _update_t\n",
        "    #\n",
        "    # average loss throughout epoch\n",
        "    #\n",
        "    if t > 0:\n",
        "      _prev_val_loss = _mean_val_loss\n",
        "    _mean_train_loss = np.mean(np.array(_train_losses))\n",
        "    _std_train_loss  = np.std( np.array(_train_losses))\n",
        "    _mean_val_loss   = np.mean(np.array(_val_losses))\n",
        "    _std_val_loss    = np.std( np.array(_val_losses))\n",
        "    print(f'\\nepoch {t:3} | train loss {_mean_train_loss:6.4f}+/-{_std_train_loss:6.4f}',end=' | ')\n",
        "    print(f'val loss {_mean_val_loss:6.4f}+/-{_std_val_loss:6.4f}',end=' | ')\n",
        "    dt = time.time() - t0\n",
        "    print(f'batch time {dt:3.0f}')\n",
        "    _loss_history.append((_mean_train_loss,_std_train_loss,_mean_val_loss,_std_val_loss))\n",
        "    #\n",
        "    # evaluate improvement (or lack thereof)\n",
        "    #\n",
        "    # we don't count improvement if it falls below the width of the confidence \n",
        "    # interval of the validation loss in \"patience\" steps\n",
        "    #\n",
        "    if t > 0 and (_prev_val_loss-_mean_val_loss) < _std_val_loss/_patience:\n",
        "      _no_improvement += 1\n",
        "      if _no_improvement >= _patience:\n",
        "        print(\"no further improvement.\")\n",
        "        break\n",
        "    else: # reset patience\n",
        "      _no_improvement = 0\n",
        "    #\n",
        "    # update solution (here comes Optax)\n",
        "    #\n",
        "\n",
        "  return _theta_t,_loss_history\n",
        "\n",
        "#\n",
        "# run optimization\n",
        "#\n",
        "train_data = X_train, Y_train, X_test, Y_test\n",
        "theta,loss_history = train_cnn_haiku(train_data, conv_net, cnn_loss)\n",
        "\n",
        "    \n"
      ],
      "metadata": {
        "id": "Q83gS5PJUEDA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UlrSyMQpoV9f"
      },
      "source": [
        "## Try out our trained model\n",
        "\n",
        "We now use our trained model to classify a sample of 25 images from the test set. We pass these 25 images to the  `conv_net.apply` function, which returns a 25x10 matrix whose  $(i, j)$ entry contains the probability that the image $i$ belongs to class $j$. We obtain the most-likely prediction using the ```np.argmax``` function which returns the index of the maximum entry along the columns. Finally, we plot the result with the prediction and prediction probability labelled underneath the image and true label on the side. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BjzP384wm9OW"
      },
      "outputs": [],
      "source": [
        "#@title Apply the model\n",
        "nsample    = 25\n",
        "rng        = jax.random.PRNGKey(42) # dummy; no random stuff hereafter\n",
        "Y_pred_hot = conv_net.apply(theta, rng, X_test[:nsample,:]) # apply our learned model\n",
        "Y_pred     = np.argmax(Y_pred_hot, axis=1) # prediction: index with highest probability\n",
        "Y_prob     = np.max(Y_pred_hot, axis=1) # retrieve the probabilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ol-f9SacnySQ"
      },
      "outputs": [],
      "source": [
        "#@title Show the results\n",
        "plt.figure(figsize=(12,12))\n",
        "for i in range(nsample):\n",
        "  img = X_test[i,:]\n",
        "  pred_class = Y_pred[i]\n",
        "  pred_label = cifar_labels[pred_class]\n",
        "  pred_prob  = Y_prob[i]\n",
        "  true_class = int(Y_test[i])\n",
        "  true_label = cifar_labels[true_class]\n",
        "  plt.subplot(5,5,i+1)\n",
        "  plt.xticks([])\n",
        "  plt.yticks([])\n",
        "  plt.grid('off')\n",
        "\n",
        "  plt.imshow(img)\n",
        "  # draw green or red dots to see if we hit or miss\n",
        "  plt.scatter(2,2,s=140,color=\"white\")\n",
        "  if true_class == pred_class:\n",
        "    plt.scatter(2,2,s=100,color=\"green\")\n",
        "  else:\n",
        "    plt.scatter(2,2,s=100,color=\"red\")\n",
        "  plt.xlabel('{} ({:0.2f})'.format(pred_label, pred_prob))\n",
        "  plt.ylabel('{}'.format(true_label))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z0618tOoMv0Y"
      },
      "source": [
        "### Question\n",
        "What do you think of the model's predictions? Looking at the model's confidence (the probability assigned to the predicted class), look for examples of the following cases:\n",
        "1. The model was correct with high confidence\n",
        "2. The model was correct with low confidence\n",
        "3. The model was incorrect with high confidence\n",
        "4. The model was incorrect with low confidence\n",
        "\n",
        "What do you think the (relative) loss values would be in those cases? \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "py0V6UwC6_kH"
      },
      "source": [
        "### Optional extra reading: Uncertainty in deep learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lpGl8VzY6B3c"
      },
      "source": [
        "Deep neural networks are not considered to be very good at estimating the uncertainty in their predictions. However, knowing your model's uncertainty can be very important for many applications. For example, consider a deep learning tool for diagnosing diseases, in this case, a false negative could have massive impacts on a person's life! We would really like to know how confident our model is in its prediction. This is a budding field of research, for example, see [this blog](https://www.cs.ox.ac.uk/people/yarin.gal/website/blog_3d801aa532c1ce.html) for a nice introduction."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OJzCooQO66D3"
      },
      "source": [
        "### Optional extra reading: CNN architectures"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n9Sjwpsm5_TD"
      },
      "source": [
        "Deciding on the architecture for a CNN, i.e. the combination of convolution, pooling, dense, and other layers, can be tricky and often can seem arbitrary. On top of that, one also has to make decisions such as what kind of pooling, which activation functions, and what size of convolution to use, among other things. For new and old practitioners of deep learning, these choices can be overwhelming. \n",
        "\n",
        "However, by examining existing successful CNN architectures we can learn a lot about what works and what doesn't. (We can even apply these existing architectures to our problems since many deep learning libraries, such as TensorFlow and Keras, have them [built in](https://keras.io/applications/#available-models) and it is even possible to fine-tune pre-trained models to our specific problem using [transfer learning](https://cs231n.github.io/transfer-learning/).)\n",
        "\n",
        "[This article](https://medium.com/@sidereal/cnns-architectures-lenet-alexnet-vgg-googlenet-resnet-and-more-666091488df5) describes many of the most successful CNN architectures in recent years, including [ResNet](https://arxiv.org/abs/1512.03385), [Inception](https://arxiv.org/pdf/1512.00567v3.pdf) and [VGG](https://arxiv.org/pdf/1409.1556.pdf). For a more detailed and technical description of these models and more see [these slides](http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture9.pdf). Reading through these resources should give you insights into why these architectures are successful as well as best practices and current trends for CNNs that will help you design your own architectures.\n",
        "\n",
        "For example, one of the practices you might pick up on is the use of 3x3 convolutions. You'll notices that older architectures such as [AlexNet](https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf) used a range of convolutions from 7x7 down to 3x3. However, newer architectures such as VGG and ResNet use 3x3 convolutions almost exclusively. In short, the reason is that stacking 3x3 convolutions gives you the same receptive field as a larger convolution but with more non-linearity. \n",
        "\n",
        "Here are some other questions you may want to think about while investigating these architectures:\n",
        "\n",
        "* Why do modern architectures use less max-pooling?\n",
        "* What does a 1x1 convolution do?\n",
        "* What is a residual connection?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-3bIU8BErhiJ"
      },
      "source": [
        "## Your Tasks\n",
        "1. [**ALL**] Experiment with the network architecture, try changing the numbers, types and sizes of layers, the sizes of filters, using different padding etc. How do these decisions affect the performance of the model? In particular, try building a *fully convolutinoal* network, with no (max-)pooling layers. \n",
        "2. [**ALL**] Add BATCH NORMALISATION ([Tensorflow documentation](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/layers/BatchNormalization) and [research paper](http://proceedings.mlr.press/v37/ioffe15.pdf)) to improve the model's generalisation.\n",
        "3. [**ADVANCED**] Read about Residual networks ([original paper](https://arxiv.org/pdf/1512.03385.pdf), ) and add **shortcut connections** to the model architecture. Try to build a simple reusable \"residual block\" as a [Keras Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model). \n",
        "4. [**OPTIONAL**]. Visualise the filters of the convolutional layers using Matplotlib. **HINT**: You can retrieve a reference to an individual layer from the sequential Keras model by calling```model.get_layer(name)```, replacing \"name\" with the name of the layer. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RFHbjtjTZz7Q"
      },
      "source": [
        "**IMPORTANT: Please fill out the exit ticket form before you leave the practical: https://forms.gle/LzHwg1BGqdYUdK5q8**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0e-IdtqUknDK"
      },
      "source": [
        "##Additional Resources\n",
        "\n",
        "Here's some more information on ConvNets:\n",
        "\n",
        "* Chris Colah's blog post on [Understanding Convolutions](https://colah.github.io/posts/2014-07-Understanding-Convolutions/)\n",
        "* [How do convolutional neural networks work?](http://brohrer.github.io/how_convolutional_neural_networks_work.html)\n",
        "* The [CS231n course](https://cs231n.github.io/) which is a great resource that covers just about all the basics of CNNs\n",
        "* [Building blocks of interpretability](https://distill.pub/2018/building-blocks/) (some really cool CNN feature visualisations)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o1ndpYE50BpG"
      },
      "source": [
        "# **Feedback**\n",
        "\n",
        "Please provide feedback that we can use to improve our practicals in the future."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OIZvkhfRz9Jz"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g-tOweUjT5Kd"
      },
      "outputs": [],
      "source": [
        "# @title Generate Feedback Form. (Run Cell)\n",
        "from IPython.display import HTML\n",
        "\n",
        "HTML(\n",
        "    \"\"\"\n",
        "<iframe\n",
        "\tsrc=\"https://forms.gle/QxLT24ECM5ezGxCTA\",\n",
        "  width=\"80%\"\n",
        "\theight=\"1200px\" >\n",
        "\tLoading...\n",
        "</iframe>\n",
        "\"\"\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oglV4kHMWnIN"
      },
      "source": [
        "<center><img src=\"https://github.com/khipu-ai/global-resources/raw/main/logo/khipu.png\" /></center>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "7Wuh9qRkE1vs"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
