{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m2s4kN_QPQVe"
      },
      "source": [
        "# Algebra and JAX <a href=\"https://colab.research.google.com/github/khipu-ai/practicals-2023/blob/main/notebooks/algebra_and_jax.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
        "\n",
        "\n",
        "<div align=\"center\">\n",
        "<img src=\"https://raw.githubusercontent.com/google/jax/main/images/jax_logo.png\" width=\"25%\" />\n",
        "</div>\n",
        "\n",
        "## Summary\n",
        "\n",
        "In this tutorial, we will introduce how to perform numeric computations, in particular matrix computations, which are the basis of most Machine Learning algorithms. We'll begin with the basics using the de facto standard NumPy, and then move on to **JAX**. \n",
        "\n",
        "JAX is a new library that allows for very efficient speedups in the aforementioned operations by transparently choosing the best available hardware (CPU, GPU, TPU, etc.) and provinding with a number of acceleration and automation mechanisms such as automatic differentiation (`grad`), parallelization (`pmap`), vectorization (`vmap`) and just-in-time compilation (`jit`).\n",
        "\n",
        "## Topics\n",
        "\n",
        "* Numeric computation and linear algebra with NumPy\n",
        "* Accelerated computation with JAX\n",
        "\n",
        "## Objectives\n",
        "\n",
        "* Learn to perform mathematical operations, in particular matrix operations in Python using  NumPy\n",
        "* Learn the basics of JAX and its similarities and differences with NumPy\n",
        "* Learn how to use JAX transforms - `jit`, `grad`, `vmap`, and `pmap`.\n",
        "\n",
        "## Outline\n",
        "\n",
        "1. [Introduction](#scrollTo=Enx0WUr8tIPf)\n",
        "1. [Similarities between JAX and NumPy](#scrollTo=CbOEYsWQ6tHv)\n",
        "1. [Differences between JAX and NumPy❌](#scrollTo=lg4__l4A7yqc)\n",
        "1. [Acceleration in JAX 🚀](#scrollTo=TSj972IWxTo2)\n",
        "1. [JAX is backend Agnostic](#scrollTo=_bQ9QqT-yKbs)\n",
        "1. [JAX Transformations](#scrollTo=JM_08mXEBRIK)\n",
        "1. [jit and grad](#scrollTo=cOGuGWtLmP7n)\n",
        "1. [Pure Functions](#scrollTo=fT56qxXzTVKZ)\n",
        "1. [vmap and pmap](#scrollTo=tvBzh8wiGuLf)\n",
        "\n",
        "## Licence\n",
        "\n",
        "* © Deep Learning Indaba 2022. Apache License 2.0.\n",
        "* © Khipu 2023. Apache License 2.0.\n",
        "\n",
        "## Version history\n",
        "\n",
        "### 2022\n",
        "\n",
        "* **Authors:** \n",
        "  * Kale-ab Tessera\n",
        "  * Ignacio Ramírez (nacho@fing.edu.uy)\n",
        "* **Reviewers:** \n",
        "  * Javier Antoran\n",
        "  * James Allingham\n",
        "  * Ruan van der Merwe\n",
        "  * Sebastian Bodenstein\n",
        "  * Laurence Midgley\n",
        "  *  Joao Guilherme\n",
        "  *  Elan van Biljon.   \n",
        "\n",
        "# Before you start\n",
        "\n",
        "For this practical, **you will need to use a GPU** to speed up training. \n",
        "To do this, go to the \"Runtime\" menu in Colab, select \"Change runtime type\" and then in the popup menu, choose \"GPU\" in the \"Hardware accelerator\" box.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6EqhIg1odqg0"
      },
      "source": [
        "## Installation and Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4boGA9rYdt9l"
      },
      "outputs": [],
      "source": [
        "## Install and import anything required. Capture hides the output from the cell.\n",
        "# @title Install and import required packages. (Run Cell)\n",
        "\n",
        "import os\n",
        "\n",
        "# https://stackoverflow.com/questions/68340858/in-google-colab-is-there-a-programing-way-to-check-which-runtime-like-gpu-or-tpu\n",
        "if int(os.environ[\"COLAB_GPU\"]) > 0:\n",
        "    print(\"a GPU is connected.\")\n",
        "elif \"COLAB_TPU_ADDR\" in os.environ and os.environ[\"COLAB_TPU_ADDR\"]:\n",
        "    print(\"A TPU is connected.\")\n",
        "    import jax.tools.colab_tpu\n",
        "\n",
        "    jax.tools.colab_tpu.setup_tpu()\n",
        "else:\n",
        "    print(\"Only CPU accelerator is connected.\")\n",
        "    # x8 cpu devices - number of (emulated) host devices\n",
        "    os.environ[\"XLA_FLAGS\"] = \"--xla_force_host_platform_device_count=8\"\n",
        "\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from jax import grad, jit, vmap, pmap\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YQe1CfDyrkdL"
      },
      "outputs": [],
      "source": [
        "# @title Helper Functions. (Run Cell)\n",
        "import copy\n",
        "from typing import Dict\n",
        "\n",
        "\n",
        "def plot_performance(data: Dict, title: str):\n",
        "    runs = list(data.keys())\n",
        "    time = list(data.values())\n",
        "\n",
        "    # creating the bar plot\n",
        "    plt.bar(runs, time, width=0.35)\n",
        "\n",
        "    plt.xlabel(\"Implementation\")\n",
        "    plt.ylabel(\"Average time taken (in s)\")\n",
        "    plt.title(title)\n",
        "    plt.show()\n",
        "\n",
        "    best_perf_key = min(data, key=data.get)\n",
        "    all_runs_key = copy.copy(runs)\n",
        "\n",
        "    # all_runs_key_except_best\n",
        "    all_runs_key.remove(best_perf_key)\n",
        "\n",
        "    for k in all_runs_key:\n",
        "        print(\n",
        "            f\"{best_perf_key} was {round((data[k]/data[best_perf_key]),2)} times faster than {k} !!!\"\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Matrix computations with NumPy\n",
        "\n",
        "Below we introduce the basics of matrix and array manipulation and operation using the _de facto_ Python library for this task: NumPy. Feel free to skip if you are already familiar with all of this.\n",
        "\n",
        "In Python/Numpy, `array` objects generalize the concept of a (numeric) _matrix_ to any number of dimensions: vectors are _one-dimensional arrays_, in the sense that they are indexed by one single index. Matrices, in turn, are _two-dimensional arrays_, since we need two indexes to specify an element. In general, an N-dimensional array requires N indexes to specify any given value within it.\n",
        "\n",
        "Note that _dimension_ in this context is not the algebraic dimension of the object (in the case of a vector, its size), but the number of indexes that are needed to specifg an element. Let's see some examples:\n",
        "\n",
        "* A vector $x$ is specified as $x=(x_1,x_2,\\ldots,x_n)$. If $x_i \\in \\mathbb{R}$, we say that the vector $x \\in \\mathbb{R}^n$.\n",
        "* A $2{\\times}2$ matrix $A$ is given by\n",
        "$$\\left(\\begin{array}{cc}a_{11}&a_{12}\\\\a_{21}&a_{22}\\end{array}\\right).$$\n",
        "As an algebraic object, an $m{\\times}n$ matrix with real entries belongs to the space $\\mathbb{R}^{m{\\times}n}$.\n",
        "* The algebraic generalization of a matrix to objects with $N$ indexes is known as a _tensor_. For example, an $m{\\times}n{\\times}p$ tensor is given by $T=\\{t_{ijk}: 1 \\leq i \\leq m, 1 \\leq j \\leq n, 1 \\leq k \\leq k\\}$\n",
        "\n",
        "Although not part of the Python distribution, `NumPy` is the de-facto standard for matrix and tensor manipulation and operations, along with a large number of utilities including random number generation, basic statistics, linear transformations, and many more.\n",
        "\n",
        "Below we give a quick overview of the core NumPy interface, along with some basic Linear Algebra definitions.\n",
        "\n",
        "## Array creation and manipulation\n",
        "\n",
        "Here we will see how to:\n",
        "* Create arrays\n",
        "* Accessing array values\n",
        "* Modifying array values\n",
        "* Add and remove rows/columns of an array\n",
        "* Reshaping\n"
      ],
      "metadata": {
        "id": "ibp4dAvRk-Qz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "#\n",
        "# 1. CREATION\n",
        "#\n",
        "x = np.array([1,2,3])\n",
        "print('x:',x)\n",
        "\n",
        "#\n",
        "# create a 2D array (a matrix):\n",
        "# the data is specified as a list of rows, all of the same size\n",
        "# each row is itself a list (or a tuple)\n",
        "#\n",
        "A = np.array( [ [3,4,5], [6,7,8], [9, 10, 11]] )\n",
        "print('A:',A)\n",
        "\n",
        "#\n",
        "# we can also do this with a  tuple of tuples\n",
        "#\n",
        "B = np.array( ( (3,4,5), (6,7,8), (9,10,11) ) )\n",
        "print('B:',A)\n",
        "\n",
        "#\n",
        "# 2. SINGLE ELEMENT ACCESS\n",
        "#\n",
        "# in Python, array indexes begin at 0\n",
        "# so, the first element of a three element vector x is x[0], and the last is x[2]\n",
        "#\n",
        "print('first element of x:',x[0])\n",
        "print('last element of x:',x[2])\n",
        "#\n",
        "# Numpy follows the same indexing convention as Python, allowing us to acces elements\n",
        "# from end to start using negative indices. For example, the last element of x\n",
        "# can also be accessed as:\n",
        "#\n",
        "print('last element of x:',x[-1]) # notice it 'starts with' -1\n",
        "print('next-to-last element of x:',x[-2]) # notice it 'starts with' -1\n",
        "#\n",
        "# for matrices, we need to specify two indexes\n",
        "# the top-left element of a matrix has both indexes 0\n",
        "#\n",
        "print('top-left element of A:',A[0,0])\n",
        "print('bottom-right element of A:',A[-1,-1])\n",
        "#\n",
        "# 3. MULTIPLE-ELEMENT ACCESS (SLICING)\n",
        "#\n",
        "# we can cut chunks of vectors and matrices by specifying ranges\n",
        "#\n",
        "# \n",
        "x_sub = x[0:2] # first two elements of x\n",
        "x_sub = x[:2] # exactly equivalent; the 0 may be ommited\n",
        "x_sub = x[1:3] # last two elements of x \n",
        "x_sub = x[1:] # again, last index may be ommited\n",
        "x_sub = x[-2:] # last two elements, using negative indexes!\n",
        "print('subvector of x:',x_sub)\n",
        "x_fakesub = x[:] # this is 'all' the elements of x = x\n",
        "print('fakesub of x:',x_fakesub)\n",
        "\n",
        "#\n",
        "# matrix slicing\n",
        "#\n",
        "Asub = A[1:,1:] # lower-right 2x2 block of A\n",
        "print('Asub')\n",
        "\n",
        "# -----------------------------------------------------------\n",
        "# EXCERSISE: \n",
        "# extract the 2x2 submatrix of A using negative indexes!\n",
        "#  \n",
        "\n",
        "#\n",
        "#\n",
        "#-------------------------------------------------------------\n",
        "\n",
        "#\n",
        "# extracting rows and columns\n",
        "#\n",
        "# we saw that [:] means 'all the indexes'\n",
        "# then, if we specify an index for a row, and ':' for columns,\n",
        "# we get the entire row:\n",
        "Arow2 = A[1,:]\n",
        "# and if we want, for example, the first column:\n",
        "Acol1 = A[:,0]\n",
        "\n",
        "# -----------------------------------------------------------\n",
        "# EXCERSISE: \n",
        "# extract the last row of A using negative indexes\n",
        "#  \n",
        "\n",
        "#\n",
        "#\n",
        "#-------------------------------------------------------------\n"
      ],
      "metadata": {
        "id": "_CdJt7EApRAR",
        "outputId": "20f0ef0f-6c56-4fd6-8bbe-5f3fc02d3b7b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x: [1 2 3]\n",
            "A: [[ 3  4  5]\n",
            " [ 6  7  8]\n",
            " [ 9 10 11]]\n",
            "B: [[ 3  4  5]\n",
            " [ 6  7  8]\n",
            " [ 9 10 11]]\n",
            "first element of x: 1\n",
            "last element of x: 3\n",
            "last element of x: 3\n",
            "next-to-last element of x: 2\n",
            "top-left element of A: 3\n",
            "bottom-right element of A: 11\n",
            "subvector of x: [2 3]\n",
            "fakesub of x: [1 2 3]\n",
            "Asub\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "# 4. MODIFYING VALUES\n",
        "#"
      ],
      "metadata": {
        "id": "WVF9_Gf7tn8I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "# 5. ADDING AND REMOVING ROWS/COLUMNS\n",
        "#\n"
      ],
      "metadata": {
        "id": "QkeQwPhOttMX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "# 6. RESHAPING\n",
        "#"
      ],
      "metadata": {
        "id": "_E_PLC90t38Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yFzjRHUsUQqq"
      },
      "outputs": [],
      "source": [
        "## @title Check the device you are using (Run Cell)\n",
        "print(f\"Num devices: {jax.device_count()}\")\n",
        "print(f\" Devices: {jax.devices()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "$$\n",
        "\\newcommand{\\because}[1]{&& \\triangleright \\textrm{#1}}\n",
        "$$"
      ],
      "metadata": {
        "id": "blMNBku0dB8h"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Enx0WUr8tIPf"
      },
      "source": [
        "# JAX\n",
        "\n",
        "[JAX](https://jax.readthedocs.io/en/latest/index.html) is a python package for writing composable numerical transformations. It leverages [Autograd](https://github.com/hips/autograd) and [XLA](https://www.tensorflow.org/xla) (Accelerated Linear Algebra), to achieve high-performance numerical computing, which is particularly relevant in machine learning.\n",
        "\n",
        "It provides functionality such as automatic differentiation (`grad`), parallelization (`pmap`), vectorization (`vmap`), just-in-time compilation (`jit`), and more. These transforms operate on [pure functions](https://jax.readthedocs.io/en/latest/notebooks/Common_Gotchas_in_JAX.html#pure-functions), so JAX encourages a **functional programming** paradigm. Furthermore, the use of XLA  allows one to target different kinds of accelerators (CPU, GPU and TPU), without code changes.  \n",
        "\n",
        "JAX is different from frameworks such as PyTorch or Tensorflow (TF). It is more low-level and minimalistic. JAX simply offers a set of primitives (simple operations) like `jit` and `vmap`, and relies on other libraries for other things e.g. using the data loader from PyTorch or TF. Due to JAX's simplicity, it is commonly used with higher-level neural network libraries such as [Haiku](https://github.com/deepmind/dm-haiku) or [Flax](https://github.com/google/flax). (Imagine writing complicated architectures using a NumPy-like interface alone! 😮 )   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CbOEYsWQ6tHv"
      },
      "source": [
        "# JAX vs NumPy \n",
        "\n",
        "## Similarities  🤝\n",
        "\n",
        "The main similarity between JAX and NumPy is that they share a similar interface and often, JAX and NumPy arrays can be used interchangeably. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "McStJC-l3qsG"
      },
      "source": [
        "## Similiar Interface"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KbYfoaujT2F7"
      },
      "source": [
        "Let's plot the sine functions using NumPy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sgRLq58OTz1t"
      },
      "outputs": [],
      "source": [
        "# 100 linearly spaced numbers from -np.pi to np.pi\n",
        "x = np.linspace(-np.pi, np.pi, 100)\n",
        "\n",
        "# the function, which is y = sin(x) here\n",
        "y = np.sin(x)\n",
        "\n",
        "# plot the functions\n",
        "plt.plot(x, y, \"b\", label=\"y=sin(x)\")\n",
        "\n",
        "plt.legend(loc=\"upper left\")\n",
        "\n",
        "# show the plot\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XCEnlC-PU3ps"
      },
      "source": [
        "Now using jax. We already imported `jax.numpy` as `jnp` in the first cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kRQf2mNRTlt3"
      },
      "outputs": [],
      "source": [
        "# 100 linearly spaced numbers from -jnp.pi to jnp.pi\n",
        "x = jnp.linspace(-jnp.pi, jnp.pi, 100)\n",
        "\n",
        "# the function, which is y = sin(x) here\n",
        "y = jnp.sin(x)\n",
        "\n",
        "# plot the functions\n",
        "plt.plot(x, y, \"b\", label=\"y=sin(x)\")\n",
        "\n",
        "plt.legend(loc=\"upper left\")\n",
        "\n",
        "# show the plot\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wuNscwHeV_dn"
      },
      "source": [
        "**Exercise 1.1 - Code Task:** Can you plot the cosine function using `jnp`?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5svZFPUCQNsG"
      },
      "outputs": [],
      "source": [
        "# Plot Cosine using jnp. (UPDATE ME)\n",
        "\n",
        "# 100 linearly spaced numbers\n",
        "# UPDATE ME\n",
        "x = ...\n",
        "\n",
        "# UPDATE ME\n",
        "y = ...\n",
        "\n",
        "\n",
        "# plot the functions\n",
        "plt.plot(x, y, \"b\", label=\"y=cos(x)\")\n",
        "\n",
        "plt.legend(loc=\"upper left\")\n",
        "\n",
        "# show the plot\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m4AVrGzy6JWR",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Answer to code task (Try not to peek until you've given it a good try!')\n",
        "# 100 linearly spaced numbers\n",
        "x = jnp.linspace(-jnp.pi, jnp.pi, 100)\n",
        "\n",
        "y = jnp.cos(x)\n",
        "\n",
        "# plot the functions\n",
        "plt.plot(x, y, \"b\", label=\"y=cos(x)\")\n",
        "\n",
        "plt.legend(loc=\"upper left\")\n",
        "\n",
        "# show the plot\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lg4__l4A7yqc"
      },
      "source": [
        "## Differences\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dPbOnhE4ZSTi"
      },
      "source": [
        "## JAX arrays are immutable, NumPy arrays are not\n",
        "\n",
        "JAX and NumPy arrays are often interchangeable, **but** Jax arrays are **immutable** (they can't be modified after they are created). Allowing mutations makes transforms difficult and violates conditions for [pure functions](https://en.wikipedia.org/wiki/Pure_function).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### NumPy\n",
        "\n",
        "Let's see this in practice by changing the number at the beginning of an array. "
      ],
      "metadata": {
        "id": "Vdfb1wtd-GkF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7r-Los6YZR-f"
      },
      "outputs": [],
      "source": [
        "# NumPy: mutable arrays\n",
        "x = np.arange(10)\n",
        "x[0] = 10\n",
        "print(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### JAX"
      ],
      "metadata": {
        "id": "8Y23OWjE_BDA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OxjkKpqAZxWo"
      },
      "outputs": [],
      "source": [
        "# JAX: immutable arrays\n",
        "# Should raise an error.\n",
        "try:\n",
        "    x = jnp.arange(10)\n",
        "    x[0] = 10\n",
        "except Exception as e:\n",
        "    print(\"Exception {}\".format(e))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VoWT5RBUagW8"
      },
      "source": [
        "So it fails! We can't mutate a JAX array once it has been created. To update JAX arrays, we need to use [helper functions](https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.ndarray.at.html) that return an updated copy of the JAX array. \n",
        "\n",
        "Instead of doing this `x[idx] = y`, we need to do this `x = x.at[idx].set(y)`. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qJYxkh4qagwO"
      },
      "outputs": [],
      "source": [
        "x = jnp.arange(10)\n",
        "new_x = x.at[0].set(10)\n",
        "print(f\" new_x: {new_x} original x: {x}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ut0meCGB5qD0"
      },
      "source": [
        "Note here that `new_x` is a copy and that the original `x` is unchanged. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oAH4c_smdGQU"
      },
      "source": [
        "## Random number generation in NumPy and JAX \n",
        "\n",
        "JAX is more explicit in Pseudo Random Number Generation (PRNG) than `NumPy` and other libraries (such as `TensorFlow` or `PyTorch`). [PRNG](https://en.wikipedia.org/wiki/Pseudorandom_number_generator) is the process of algorithmically generating a sequence of numbers, which *approximate* the properties of a sequence of random numbers.  \n",
        "\n",
        "Let's see the differences in how JAX and NumPy generate random numbers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q2m376Ethf8m"
      },
      "source": [
        "##### In Numpy, PRNG is based on a global `state`.\n",
        "\n",
        "Let's set the initial seed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-0t3sjxzdgmP"
      },
      "outputs": [],
      "source": [
        "# Set random seed\n",
        "np.random.seed(42)\n",
        "prng_state = np.random.get_state()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "QKVz5atZMMOV"
      },
      "outputs": [],
      "source": [
        "# @title Helper function to compare prng keys (Run Cell)\n",
        "def is_prng_state_the_same(prng_1, prng_2):\n",
        "    \"\"\"Helper function to compare two prng keys.\"\"\"\n",
        "    # concat all elements in prng tuple\n",
        "    list_prng_data_equal = [(a == b) for a, b in zip(prng_1, prng_2)]\n",
        "    # stack all elements together\n",
        "    list_prng_data_equal = np.hstack(list_prng_data_equal)\n",
        "    # check if all elements are the same\n",
        "    is_prng_equal = all(list_prng_data_equal)\n",
        "    return is_prng_equal"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nloZ9abah3J3"
      },
      "source": [
        "Let's take a few samples from a Gaussian (normal) Distribution and check if PRNG keys/global state change."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aiUcfX7iSenY"
      },
      "outputs": [],
      "source": [
        "print(\n",
        "    f\"sample 1 = {np.random.normal()} Did prng state change: {not is_prng_state_the_same(prng_state,np.random.get_state())}\"\n",
        ")\n",
        "prng_state = np.random.get_state()\n",
        "print(\n",
        "    f\"sample 2 = {np.random.normal()} Did prng state change: {not is_prng_state_the_same(prng_state,np.random.get_state())}\"\n",
        ")\n",
        "prng_state = np.random.get_state()\n",
        "print(\n",
        "    f\"sample 3 = {np.random.normal()} Did prng state change: {not is_prng_state_the_same(prng_state,np.random.get_state())}\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nuHkW6V4iLa9"
      },
      "source": [
        "Numpy's global random state is updated every time a random number is generated, so *sample 1 != sample 2 != sample 3*. \n",
        "\n",
        "Having the state automatically updated, makes it difficult to handle randomness in a **reproducible** way across different threads, processes and devices. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lGDU6ckKkzqL"
      },
      "source": [
        "##### In JAX, PRNG is explicit.\n",
        "\n",
        "In JAX, for each random number generation, you need to explicitly pass in a random key/state."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6oKdk5CSmD-f"
      },
      "source": [
        "Passing the same state/key results in the same number being generated. This is generally undesirable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y-6B0hjtlTmd"
      },
      "outputs": [],
      "source": [
        "from jax import random\n",
        "\n",
        "key = random.PRNGKey(42)\n",
        "print(f\"sample 1 = {random.normal(key)}\")\n",
        "print(f\"sample 2 = {random.normal(key)}\")\n",
        "print(f\"sample 3 = {random.normal(key)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l0KcwEbZqIaQ"
      },
      "source": [
        "To generate different and independent samples, you need to manually **split** the keys. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v-7BhY0MmEhI"
      },
      "outputs": [],
      "source": [
        "from jax import random\n",
        "\n",
        "key = random.PRNGKey(42)\n",
        "print(f\"sample 1 = {random.normal(key)}\")\n",
        "\n",
        "# We split the key -> new key and subkey\n",
        "new_key, subkey = random.split(key)\n",
        "\n",
        "# We use the subkey immediately and keep the new key for future splits.\n",
        "# It doesn't really matter which key we keep and which one we use immediately.\n",
        "print(f\"sample 2 = {random.normal(subkey)}\")\n",
        "\n",
        "# We split the new key -> new key2 and subkey\n",
        "new_key2, subkey = random.split(new_key)\n",
        "print(f\"sample 3 = {random.normal(subkey)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2VnTDptmuk-i"
      },
      "source": [
        "By using JAX, we can more easily reproduce random number generation in parallel across threads, processes, or even devices by explicitly passing and keeping track of the prng key (without relying on a global state that automatically gets updated). For more details on PRNG in JAX, you can read more [here](https://jax.readthedocs.io/en/latest/jep/263-prng.html). "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TSj972IWxTo2"
      },
      "source": [
        "# Acceleration in JAX 🚀\n",
        "\n",
        "JAX leverages Autograd and XLA for accelerating numerical computation. The use of Autograd allows for automatic differentiation (`grad`), while XLA allows JAX to run on multiple accelerators/backends and run transforms like `jit` and `pmap`. JAX also allows you to use `vmap` for automatic vectorization.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_bQ9QqT-yKbs"
      },
      "source": [
        "## JAX is backend Agnostic\n",
        "\n",
        "Using JAX, you can run the same code on different backends/AI accelerators (e.g. CPU/GPU/TPU), **with no changes in code** (no more `.to(device)` - from frameworks like PyTorch). This means we can easily run linear algebra operations directly on GPU/TPU."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4PbcFsfAibBu"
      },
      "source": [
        "**Multiplying Matrices**\n",
        "\n",
        "Dot products are a common operation in numerical computing and a central part of modern deep learning. They are defined over [vectors](https://en.wikipedia.org/wiki/Coordinate_vector), which can loosely be thought of as a list of multiple scalers (single values). \n",
        "\n",
        "Formally, given two vectors $\\boldsymbol{x}$,$\\boldsymbol{y}$ $\\in R^n$, their dot product is defined as:\n",
        "\n",
        "<center>$\\boldsymbol{x}^{\\top} \\boldsymbol{y}=\\sum_{i=1}^{n} x_{i} y_{i}$</center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AY1RsVkXaokP"
      },
      "source": [
        "Dot Product in NumPy (will run on cpu)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yj59KkD_HDOs"
      },
      "outputs": [],
      "source": [
        "size = 1000\n",
        "x = np.random.normal(size=(size, size))\n",
        "y = np.random.normal(size=(size, size))\n",
        "numpy_time = %timeit -o -n 10 a_np = np.dot(y,x.T)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6c_kl-u0KPVY"
      },
      "source": [
        "Dot Product using JAX (will run on current runtime - e.g. GPU)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PHRcHK86KO3w"
      },
      "outputs": [],
      "source": [
        "size = 1000\n",
        "key1, key2 = jax.random.split(jax.random.PRNGKey(42), num=2)\n",
        "x = jax.random.normal(key1, shape=(size, size))\n",
        "y = jax.random.normal(key2, shape=(size, size))\n",
        "jax_time = %timeit -o -n 10 jnp.dot(y, x.T).block_until_ready()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LMTSpEG3TNah"
      },
      "source": [
        "\n",
        "> When timing JAX functions, we use `.block_until_ready()` because JAX uses [asynchronous dispatch](https://jax.readthedocs.io/en/latest/async_dispatch.html#async-dispatch). This means JAX doesn't wait for the operation to complete before returning control to your code. To fairly compute the time taken for JAX operations, we therefore block until the operation is done.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S3vwh6Q724gn"
      },
      "source": [
        "How much faster was the dot product in JAX (Using GPU)?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UkASX9p34A1D"
      },
      "outputs": [],
      "source": [
        "np_average_time = np.mean(numpy_time.all_runs)\n",
        "jax_average_time = np.mean(jax_time.all_runs)\n",
        "data = {\"numpy\": np_average_time, \"jax\": jax_average_time}\n",
        "\n",
        "plot_performance(data, title=\"Average time taken per framework to run dot product\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "JAX not running much faster? -> Re-run the JAX cell.                       \n",
        "> \"Keep in mind that the first time you run JAX code, it will be slower because it is being compiled. T*his is true even if you don’t use jit in your own code, because JAX’s builtin functions are also jit compiled*.\" - [JAX Docs](https://jax.readthedocs.io/en/latest/faq.html#benchmarking-jax-code).\n",
        "\n",
        "If you are running on an accelerator, you should see a considerable performance benefit of using JAX, without making any changes to your code! \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "X6Rv_OQgBOqr"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JM_08mXEBRIK"
      },
      "source": [
        "# JAX Transformations\n",
        "\n",
        "JAX transforms (`jit`, `grad`, `vmap`, `pmap`) first convert python functions into an intermediate language called *jaxpr*. Transforms are then applied to this jaxpr representation.\n",
        "\n",
        "JAX generates jaxpr, in a process known as **tracing**. During tracing, function inputs are wrapped by a tracer object and then JAX records all operations (including regular python code) that occur during the function call. These recorded operations are used to reconstruct the function. \n",
        "\n",
        "Any python side-effects are not recorded during tracing. JAX transforms and compilations are designed to work only with **pure functions**. For more on tracing and jaxpr, you can read [here](https://jax.readthedocs.io/en/latest/jaxpr.html).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QsJE_U-ZzVol"
      },
      "source": [
        "## JIT (Just in Time) compilation\n",
        "\n",
        "Jax dispatches operations to accelerators one at a time. If we have repeated operations, we can use `jit` to compile the function the first time it is called, then subsequent calls will be [cached](https://en.wikipedia.org/wiki/Cache_(computing) (save the compiled version so that it doesn't need to be recompiled everytime we call it). "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uIYsqIp_-Dly"
      },
      "source": [
        "Let's compile [ReLU (Rectified Linear Unit)](https://arxiv.org/abs/1803.08375), a popular activation function in deep learning. \n",
        "\n",
        "ReLU is defined as follows:\n",
        "<center>$f(x)=max(0,x)$</center>\n",
        "\n",
        "It can be visualized as follows:\n",
        "\n",
        "<center>\n",
        "<img src=\"https://machinelearningmastery.com/wp-content/uploads/2018/10/Line-Plot-of-Rectified-Linear-Activation-for-Negative-and-Positive-Inputs.png\" width=\"35%\" />\n",
        "</center>,\n",
        "\n",
        "where $x$ is the input to the function and $y$ is output of ReLU.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vm-bN9sQETLV"
      },
      "source": [
        "$$f(x)=\\max (0, x)=\\left\\{\\begin{array}{l}x_{i} \\text { if } x_{i}>0 \\\\ 0 \\text { if } x_{i}<=0\\end{array}\\right.$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dFiuu3BFAKdY"
      },
      "source": [
        "**Exercise 1.2 - Code Task:** Complete the ReLU implementation below using standard python."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1_qMJJbs-Cbe"
      },
      "outputs": [],
      "source": [
        "# Implement ReLU.\n",
        "def relu(x):\n",
        "    if x > 0:\n",
        "        return\n",
        "        # TODO Implement me!\n",
        "    else:\n",
        "        return\n",
        "        # TODO Implement me!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "zCobLakM1esy"
      },
      "outputs": [],
      "source": [
        "# @title Run to test your function.\n",
        "\n",
        "\n",
        "def plot_relu(relu_function):\n",
        "    max_int = 5\n",
        "    # Generete 100 evenly spaced points from -max_int to max_int\n",
        "    x = np.linspace(-max_int, max_int, 1000)\n",
        "    y = np.array([relu_function(xi) for xi in x])\n",
        "    plt.plot(x, y, label=\"ReLU\")\n",
        "    plt.legend(loc=\"upper left\")\n",
        "    plt.xticks(np.arange(min(x), max(x) + 1, 1))\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def check_relu_function(relu_function):\n",
        "    # Generete 100 evenly spaced points from -100 to -1\n",
        "    x = np.linspace(-100, -1, 100)\n",
        "    y = np.array([relu_function(xi) for xi in x])\n",
        "    assert (y == 0).all()\n",
        "\n",
        "    # Check if x == 0\n",
        "    x = 0\n",
        "    y = relu_function(x)\n",
        "    assert y == 0\n",
        "\n",
        "    # Generete 100 evenly spaced points from 0 to 100\n",
        "    x = np.linspace(0, 100, 100)\n",
        "    y = np.array([relu_function(xi) for xi in x])\n",
        "    assert np.allclose(x, y)\n",
        "\n",
        "    print(\"Your ReLU function is correct!\")\n",
        "\n",
        "\n",
        "check_relu_function(relu)\n",
        "plot_relu(relu)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kken6_XvDdOK",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Answer to code task (Try not to peek until you've given it a good try!')\n",
        "def relu(x):\n",
        "    if x > 0:\n",
        "        return x\n",
        "    else:\n",
        "        return 0\n",
        "\n",
        "\n",
        "check_relu_function(relu)\n",
        "plot_relu(relu)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2mgIAyE2Fx3O"
      },
      "source": [
        "Let's try to `jit` this function to speed up compilation and then try to call it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4YDkiNlRF6jn"
      },
      "outputs": [],
      "source": [
        "relu_jit = jax.jit(relu)\n",
        "\n",
        "key = jax.random.PRNGKey(42)\n",
        "# Gen 1000000 random numbers and pass them to relu\n",
        "num_random_numbers = 1000000\n",
        "x = jax.random.normal(key, (num_random_numbers,))\n",
        "\n",
        "# Should raise an error.\n",
        "try:\n",
        "    relu_jit(x)\n",
        "except Exception as e:\n",
        "    print(\"Exception {}\".format(e))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y7q33C4pHOQW"
      },
      "source": [
        "**Why does this fail?**\n",
        "\n",
        "\n",
        "> As mentioned above, JAX transforms first converts python functions into an intermediate language called *jaxpr*. Jaxpr only captures what is executed on the parameters given to it during tracing, so this means during conditional calls, jaxpr only considers the branch taken.\n",
        "> \n",
        "> When jit-compiling a function, we want to compile and cache a version of the function that can handle multiple different argument types (so we don't have to recompile for each function evaluation). For example, when we compile a function on an array `jnp.array([1., 2., 3.], jnp.float32)`, we would likely also want to use the compiled function for `jnp.array([4., 5., 6.], jnp.float32)`. \n",
        "> \n",
        "> To achieve this, JAX traces your code based on abstract values. The default abstraction level is a ShapedArray - array that has a fixed size and dtype, for example, if we trace a function using `ShapedArray((3,), jnp.float32)`,  it can be reused for any concrete array of size 3, and float32 dtype. \n",
        "> \n",
        "> This does come with some challenges. Tracing that relies on concrete values becomes tricky and sometimes results in `ConcretizationTypeError` as in the ReLU function above. Furthermore, when tracing a function with conditional statements (\"if ...\"), JAX doesn't know which branch to take when tracing and so tracing can't occur.\n",
        "\n",
        "**TLDR**: JAX tracing doesn't work well with conditional statements (\"if ...\"). \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uLswU8aMEQ9K"
      },
      "source": [
        "To solve this, we have two options:\n",
        "- Use static arguments to make sure JAX traces on a concrete value level - this is not ideal if you need to retrace a lot. Example - bottom of this [section](https://jax.readthedocs.io/en/latest/notebooks/Common_Gotchas_in_JAX.html#python-control-flow-jit).\n",
        "- Use builtin JAX condition flow primitives such as [`lax.cond`](https://jax.readthedocs.io/en/latest/_autosummary/jax.lax.cond.html) or [`jnp.where`](https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.where.html).  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SX8k4R7daBpP"
      },
      "source": [
        "**Exercise 1.3 - Code Task** : Let's convert our ReLU function above to work with jit.\n",
        "\n",
        "**Useful methods:**  [`jnp.where`](https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.where.html) (or [`jnp.maximum`](https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.maximum.html), if you prefer.) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p-4mXLwqaK-b"
      },
      "outputs": [],
      "source": [
        "# Implement a jittable ReLU\n",
        "def relu(x):\n",
        "    # TODO Implement ME!\n",
        "    return ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B5fq_QRoaaG5",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Run to test your function.\n",
        "check_relu_function(relu)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XLtBaplGxlS3",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Answer to code task (Try not to peek until you've given it a good try!')\n",
        "def relu(x):\n",
        "    return jnp.where(x > 0, x, 0)\n",
        "    # Another option - return jnp.maximum(x,0)\n",
        "\n",
        "\n",
        "check_relu_function(relu)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KYogDOCLiLXN",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Now let's see the performance benefit of using jit! (Run me)\n",
        "\n",
        "# jit our function\n",
        "relu_jit = jax.jit(relu)\n",
        "\n",
        "# generate random input\n",
        "key = jax.random.PRNGKey(42)\n",
        "num_random_numbers = 1000000\n",
        "x = jax.random.normal(key, (num_random_numbers,))\n",
        "\n",
        "# time normal jit function\n",
        "jax_time = %timeit -o -n 10 relu(x).block_until_ready()\n",
        "\n",
        "# Warm up/Compile - first run for jitted function\n",
        "relu_jit(x).block_until_ready()\n",
        "\n",
        "# time jitted function\n",
        "jax_jit_time = %timeit -o -n 10 relu_jit(x).block_until_ready()\n",
        "\n",
        "# Let's plot the performance difference\n",
        "jax_avg_time = np.mean(jax_time.all_runs)\n",
        "jax_jit_avg_time = np.mean(jax_jit_time.all_runs)\n",
        "data = {\"JAX (no jit)\": jax_avg_time, \"JAX (with jit)\": jax_jit_avg_time}\n",
        "\n",
        "plot_performance(data, title=\"Average time taken for ReLU function\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dxq-z-xzs40s"
      },
      "source": [
        "## Automatic gradient computation\n",
        "\n",
        "The `grad` transformation is used to automatically compute the gradient of a function in JAX. It can be applied to Python and NumPy functions, which means you can differentiate through loops, branches, recursion, and closures.  \n",
        "\n",
        "`grad` takes in a function `f` and returns a function. If `f` is a mathematical function $f(x)$, then `grad(f(x))` corresponds to $f'(x)=\\frac{df}{dx}$. Then `grad(f)(x_0)` yields $f'(x_0)$.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C49R8EOs-GHe"
      },
      "source": [
        "Let's take a simple function $f(x)=6x^4-9x+4$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lUMepl6J-dQP"
      },
      "outputs": [],
      "source": [
        "f = lambda x: 6 * x**4 - 9 * x + 4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ayvrkpiBiu4"
      },
      "source": [
        "We can compute the gradient of this function - $f'(x)$ and evaluate it at $x=3$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YNm9hS2S-vJk"
      },
      "outputs": [],
      "source": [
        "dfdx = grad(f)\n",
        "dfdx_3 = dfdx(3.0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UcRUywsnF3LZ"
      },
      "source": [
        "**Exercise 1.4 - Math Task**: Can you calculate $f'(2)$ by hand?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PybYK6NEFWrD",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "answer = 0  # @param {type:\"integer\"}\n",
        "\n",
        "dfdx_2 = dfdx(2.0)\n",
        "\n",
        "assert (\n",
        "    answer == dfdx_2\n",
        "), \"Incorrect answer, hint https://en.wikipedia.org/wiki/Power_rule#Statement_of_the_power_rule\"\n",
        "\n",
        "print(\"Nice, you got the correct answer!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Answer to math task (Try not to run until you've given it a good try!') \n",
        "%%latex                                                  \n",
        "\\begin{aligned}\n",
        "f(x) & = 6x^4-9x+4 \\\\\n",
        "f'(x) & = 24x^3 -9  && \\triangleright \\textrm{Power Rule.}  \\\\ \n",
        "f'(2) &  = 24(2)^3 -9 = 183 && \\triangleright \\textrm{Substituting x=2} \\\\\n",
        "\\end{aligned}"
      ],
      "metadata": {
        "id": "CAwlhxIlRPp9",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wcB5ZjojH67Q"
      },
      "source": [
        "We can also chain `grad` to calculate higher order deratives. \n",
        "\n",
        "We can calculate $f'''(x)$ as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "013SFq7BE54W"
      },
      "outputs": [],
      "source": [
        "d3dx = grad(grad(grad(f)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7_r9VQGoIsa6"
      },
      "source": [
        "**Exercise 1.5 - Math Task**: How about $f'''(2)$ by hand?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "WZUArv4TInPg"
      },
      "outputs": [],
      "source": [
        "answer = 0  # @param {type:\"integer\"}\n",
        "\n",
        "d3dx_2 = d3dx(2.0)\n",
        "\n",
        "assert answer == d3dx_2, \"Incorrect answer, hint ...\"\n",
        "\n",
        "print(\"Nice, you got the correct answer!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Answer to math task (Try not to run until you've given it a good try!') \n",
        "%%latex \n",
        "\n",
        "\\begin{aligned}\n",
        "f(x) & = 6x^4-9x+4 \\\\\n",
        "f'(x) & = 24x^3 -9  && \\triangleright \\textrm{Power Rule.}  \\\\\n",
        "f''(x) & = 72x^2  && \\triangleright \\textrm{Power Rule.}  \\\\\n",
        "f'''(x) & = 144x && \\triangleright \\textrm{Power Rule.} \\\\\n",
        "f'''(2) & = 144(2)=288 && \\triangleright \\textrm{Substituting x=2} \\\\ \n",
        "\\end{aligned}"
      ],
      "metadata": {
        "id": "TCC7SkH8MMVk",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c3QgJNU9XYyz"
      },
      "source": [
        "Another useful method is `value_and_grad`, where we can get the value ($f(x)$) and gradient ($f'(x)$). "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x3zeSv6gXuyd"
      },
      "outputs": [],
      "source": [
        "from jax import value_and_grad\n",
        "\n",
        "f_x, dy_dx = value_and_grad(f)(2.0)\n",
        "print(f\"f(x): {f_x} f′(x): {dy_dx} \")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> For partial derivatives, you need to use the [`argnums`](https://jax.readthedocs.io/en/latest/_autosummary/jax.grad.html) param to specify which variables you want to differentiate with respect to. \n",
        "\n"
      ],
      "metadata": {
        "id": "_vUr-B6gSxnu"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MktOLPnwvnH3"
      },
      "source": [
        "**Exercise 1.6 - Group Task:** Chat with neighbour/think about how JAX's automatic differentiation compares to other libraries such as Pytorch or Tensorflow. "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Another useful application related to `grad` is when you want your `grad` function to return auxiliary (extra) data, that you don't want differentiated. You can use the `has_aux` parameter to do this (example in \"Auxiliary data\" section in [here](https://github.com/google/jax/blob/main/docs/jax-101/01-jax-basics.ipynb))."
      ],
      "metadata": {
        "id": "rvXlE7z02M2D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pure Functions\n",
        "\n",
        "JAX transformation and compilation are designed to work reliably on **pure functions** (see [Wikipedia page on pure functions](https://en.wikipedia.org/wiki/Pure_function)). These functions have the following properties:\n",
        "1. All **input** data is passed through the **function's parameters**. \n",
        "2. All **results** are output through the **function's return**. \n",
        "3. The function always returns the same **result** if invoked with the **same inputs**. What if your function involves randomness? Pass in the random seed!\n",
        "4. **No [side-effects](https://en.wikipedia.org/wiki/Side_effect_(computer_science))** - no mutation of non-local variables or input/output streams.  \n",
        " \n",
        "\n",
        "Let's see what could happen if we don't stick to using pure functions."
      ],
      "metadata": {
        "id": "fT56qxXzTVKZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Side Effects"
      ],
      "metadata": {
        "id": "Mad7l7s0CtT1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's call print within a function."
      ],
      "metadata": {
        "id": "xkQWTE2Xe955"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def impure_print_side_effect(x):\n",
        "    print(\"Print me!\")  # This is a side-effect\n",
        "    return x\n",
        "\n",
        "\n",
        "# The side-effects appear during the first run\n",
        "print(\"First call: \", jax.jit(impure_print_side_effect)(4.0))"
      ],
      "metadata": {
        "id": "S9aeUdUoBmCg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As expected, the print statement is called.\n",
        "\n",
        "Let's call this function again. "
      ],
      "metadata": {
        "id": "nu4rnyS7ox_L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Subsequent runs with parameters of same type and shape may not show the side-effect\n",
        "# This is because JAX now invokes a cached compilation of the function\n",
        "print(\"Second call: \", jax.jit(impure_print_side_effect)(5.0))"
      ],
      "metadata": {
        "id": "-wnkIqAxfDeJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ah, no print statement! Since JAX cached the compilation of the function, `print()` calls will only happen during tracing and not every time the function is called. "
      ],
      "metadata": {
        "id": "64rNvVnwo-eB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# JAX re-runs the Python function when the type or shape of the argument changes\n",
        "print(\n",
        "    \"Third call, different type: \", jax.jit(impure_print_side_effect)(jnp.array([5.0]))\n",
        ")"
      ],
      "metadata": {
        "id": "Mp_CkOL-o86t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this case, we called the function with a different shaped object and so it triggered the re-tracing of the function and print was called again. "
      ],
      "metadata": {
        "id": "XFogrIf5fbLU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To print values in compiled functions, use [host callbacks](https://jax.readthedocs.io/en/latest/jax.experimental.host_callback.html?highlight=print#jax.experimental.host_callback.id_print)([example](https://github.com/google/jax/issues/196#issuecomment-1191155679)) or if your jax version>=0.3.16, you can use [`jax.debug.print`](https://jax.readthedocs.io/en/latest/debugging/print_breakpoint.html). \n"
      ],
      "metadata": {
        "id": "pqV6_25GCxHL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Global variables"
      ],
      "metadata": {
        "id": "EqL1-TGaC8Ir"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using global variables can also lead to some undesired consequences!"
      ],
      "metadata": {
        "id": "t8dzJog8tMe_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "g = 0.0\n",
        "\n",
        "\n",
        "def impure_uses_globals(x):\n",
        "    return x + g\n",
        "\n",
        "\n",
        "# JAX captures the value of the global during the first run\n",
        "print(\"First call: \", jax.jit(impure_uses_globals)(4.0))"
      ],
      "metadata": {
        "id": "vwAkKrDiCXO6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This prints 4, using the original value of `g`.\n",
        "\n",
        "Let's update `g` and call our function again."
      ],
      "metadata": {
        "id": "pWNE8B5btcfc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "g = 10.0  # Update the global\n",
        "\n",
        "# Subsequent runs may silently use the cached value of the globals\n",
        "print(\"Second call: \", jax.jit(impure_uses_globals)(4.0))"
      ],
      "metadata": {
        "id": "mLMpdQZwtUEL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Even though we updated our global variable, this still prints 4, using the original value of `g`. This is because the value of `g` was cached."
      ],
      "metadata": {
        "id": "o3-ygEx0tpBX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# JAX re-runs the Python function when the type or shape of the argument changes\n",
        "# This will end up reading the latest value of the global\n",
        "print(\"Third call, different type: \", jax.jit(impure_uses_globals)(jnp.array([4.0])))"
      ],
      "metadata": {
        "id": "LDecWNyktWDN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Similar to the side-effects example, re-tracing gets triggered when the shape of our input has changed. In this case, our function now uses the updated value of `g`."
      ],
      "metadata": {
        "id": "3mIZaXOqt5ix"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since the global variables are cached, it is still okay to use global **constants** inside jax functions."
      ],
      "metadata": {
        "id": "aLis2BV04BQK"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RCUB9YkCnCFb"
      },
      "source": [
        "## Function vectorization with vmap\n",
        "\n",
        "vmap (Vectorizing map) automatically vectorizes your python functions. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e858lqfYKd4d"
      },
      "source": [
        "Let's define a simple function that calculates the min and max of an input."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-6qalyXgDsKB"
      },
      "outputs": [],
      "source": [
        "def min_max(x):\n",
        "    return jnp.array([jnp.min(x), jnp.max(x)])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "muSIsUkgKlxh"
      },
      "source": [
        "We can apply this function to the vector - `[0, 1, 2, 3, 4]` and get the min and max values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F5wIeGieKsWG"
      },
      "outputs": [],
      "source": [
        "x = jnp.arange(5)\n",
        "min_max(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_PkC7NnPLNXq"
      },
      "source": [
        "What about if we want to apply this to a batch/list of vectors (i.e. calculate the min and max independently across multiple batches)? "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hRngFfwCMHLd"
      },
      "source": [
        "Let's create our batch - 3 vectors of size 5."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EKuh459OD6jx"
      },
      "outputs": [],
      "source": [
        "batch_size = 3\n",
        "batched_x = np.arange(15).reshape((batch_size, -1))\n",
        "print(batched_x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hApYpVEvNS1y"
      },
      "source": [
        "**Exercise 1.7 - Question**: What do you think would be the result if we passed batch_x into `min_max`?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gu6C3J0kMrtj",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "batch_min_max_output = [[0,4],[5,9],[10,14]]  # @param [\"[[0,4],[5,9],[10,14]]\", \"[[0,10],[1,11],[2,12],[3,13],[4,14]]\", \"[0,14]\"] {type:\"raw\"}\n",
        "\n",
        "assert (batch_min_max_output == np.array(min_max(batched_x))).all(), \"Incorrect answer.\"\n",
        "\n",
        "print(\"Nice, you got the correct answer!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6K0weiHOOb8L"
      },
      "source": [
        "So the above is not what we want. The `min` and `max` is applied across the entire batch, when we want the min and max per vector/mini-batch. \n",
        "\n",
        "We can also manually batch this by `jnp.stack` and a for loop, as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q8RdAqr8N-Fd"
      },
      "outputs": [],
      "source": [
        "@jit\n",
        "def manual_batch_min_max_loop(batched_x):\n",
        "    min_max_result_list = []\n",
        "    for x in batched_x:\n",
        "        min_max_result_list.append(min_max(x))\n",
        "    return jnp.stack(min_max_result_list)\n",
        "\n",
        "\n",
        "print(manual_batch_min_max_loop(batched_x))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jmu3VVtMR0GV"
      },
      "source": [
        "Or, just manually updating the `axis` in `jnp.min` and `jnp.max`. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lzxmORv-RcUg"
      },
      "outputs": [],
      "source": [
        "@jit\n",
        "def manual_batch_min_max_axis(batched_x):\n",
        "    return jnp.array([jnp.min(batched_x, axis=1), jnp.max(batched_x, axis=1)]).T\n",
        "\n",
        "\n",
        "print(manual_batch_min_max_axis(batched_x))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CetKYASUSE4Q"
      },
      "source": [
        "These approaches both work, but we need to change our function to work with batches. We can't just run the same code across a batch of data.\n",
        "\n",
        "There is where `vmap` becomes useful! Using `vmap` we can write a function once, as if it is working on a single element, and then use `vmap` to automatically vectorize it! "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s2F8WUNQROkQ"
      },
      "outputs": [],
      "source": [
        "# define our vmap function using our original single vector function\n",
        "@jit\n",
        "def min_max_vmap(batched_x):\n",
        "    return vmap(min_max)(batched_x)\n",
        "\n",
        "\n",
        "# Run it on a single vecor\n",
        "## We add extra dimention in a single vector, shape changes from (5,) to (1,5), which makes the vmapping possible\n",
        "x_with_leading_dim = jax.numpy.expand_dims(x, axis=0)\n",
        "print(f\"Single vector: {min_max_vmap(x_with_leading_dim)}\")\n",
        "\n",
        "# Run it on batch of vectors\n",
        "print(f\"Batch/list of vector:{min_max_vmap(batched_x)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-3bome92VRL6"
      },
      "source": [
        "So this is really convenient, but what about performance? "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O1Nb4uniUUor"
      },
      "outputs": [],
      "source": [
        "batched_x = np.arange(50000).reshape((500, 100))\n",
        "\n",
        "# Trace the functions with first call\n",
        "manual_batch_min_max_loop(batched_x).block_until_ready()\n",
        "manual_batch_min_max_axis(batched_x).block_until_ready()\n",
        "min_max_vmap(batched_x).block_until_ready()\n",
        "\n",
        "min_max_forloop_time = %timeit -o -n 10 manual_batch_min_max_loop(batched_x).block_until_ready()\n",
        "min_max_axis_time = %timeit -o -n 10 manual_batch_min_max_axis(batched_x).block_until_ready()\n",
        "min_max_vmap_time = %timeit -o -n 10 min_max_vmap(batched_x).block_until_ready()\n",
        "\n",
        "print(\n",
        "    f\"Avg Times (lower is better) - Naive Implementation: {np.round(np.mean(min_max_forloop_time.all_runs),5)} Manually Vectorized: {np.round(np.mean(min_max_axis_time.all_runs),5)} Vmapped Function: {np.round(np.mean(min_max_vmap_time.all_runs),5)} \"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mYL758zCYsrR"
      },
      "source": [
        "So `vmap` should be similar in performance to manually vectorized code (if everything is implemented well), and much better than naively vectorized code (i.e. for loops). "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vAO9dOdrtiqI"
      },
      "source": [
        "## Paralelization with pmap\n",
        "\n",
        "💡**For this subsection, please ensure that colab is using a `TPU` runtime. If no `TPU` runtimes are available, select `Harware Accelerator` - `None` for a cpu runtime.** \n",
        "\n",
        "With `pmap` we can convert a function written for a single device to a function that can run in parallel across many devices. \n",
        "\n",
        "**Difference between `vmap` and `pmap`**:\n",
        "\n",
        "So both `pmap` and `vmap` transform a function to work over an array, but they differ in implementation. `vmap` adds an extra batch dimension to all the operations in a function, while `pmap` replicates the function and executes each replica on its own XLA device in parallel."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "gUYA277soR-0"
      },
      "outputs": [],
      "source": [
        "# @title Check the device you are using (Run Cell)\n",
        "print(f\"Num devices: {jax.device_count()}\")\n",
        "print(f\" Devices: {jax.devices()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6qhlBnLs6AYL"
      },
      "source": [
        "Let's try and `pmap` a batch of dot products.\n",
        "\n",
        "Here is an illustration of how we would typically do this sequentially: \n",
        "\n",
        "[Source](https://www.assemblyai.com/blog/why-you-should-or-shouldnt-be-using-jax-in-2022/)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "fz1i2AwA5_7J"
      },
      "outputs": [],
      "source": [
        "# @title Illustration of Sequential Dot Product (Run me)\n",
        "from IPython.display import HTML\n",
        "\n",
        "HTML(\n",
        "    '<iframe width=\"560\" height=\"315\" src=\"https://www.assemblyai.com/blog/content/media/2022/02/not_parallel-2.mp4\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>'\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MTmWNFZ08f8n"
      },
      "source": [
        "Here is the code implementation of this:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GqTuMldJ9Uv5"
      },
      "outputs": [],
      "source": [
        "# Let's generate a batch of size 8, each with a matrix of size (500, 600)\n",
        "\n",
        "# Let create 8 keys, 1 for each batch\n",
        "keys = jax.random.split(jax.random.PRNGKey(0), 8)\n",
        "\n",
        "# Let create our batches\n",
        "mats = jnp.stack([jax.random.normal(key, (500, 600)) for key in keys])\n",
        "\n",
        "\n",
        "def dot_product_sequential():\n",
        "    @jit\n",
        "    def avg_dot_prod(mats):\n",
        "        result = []\n",
        "        # Loop through batch and compute dp\n",
        "        for mat in mats:\n",
        "            # dot product between the a mat and mat.T (transposed version)\n",
        "            result.append(jnp.dot(mat, mat.T))\n",
        "        return jnp.stack(result)\n",
        "\n",
        "    avg_dot_prod(mats).block_until_ready()\n",
        "\n",
        "\n",
        "run_sequential = %timeit -o -n 5 dot_product_sequential()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fBEtecJX-0AW"
      },
      "source": [
        "Here is an illustration of how we would do this in parallel \n",
        "\n",
        "[Source](https://www.assemblyai.com/blog/why-you-should-or-shouldnt-be-using-jax-in-2022/)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uswxurmn-5oC",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Illustration of Parallel Dot Product (Run me)\n",
        "from IPython.display import HTML\n",
        "\n",
        "HTML(\n",
        "    '<iframe width=\"560\" height=\"315\" src=\"https://www.assemblyai.com/blog/content/media/2022/02/parallelized.mp4\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>'\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sGsq8iTA_N9U"
      },
      "source": [
        "Here is code implementation of batched dot products:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ygFWDfQIoeC"
      },
      "source": [
        "First, we will create `8` random matrices (one for each available tpu devices - colab tpu's have 8 available [devices](https://cloud.google.com/tpu/docs/system-architecture-tpu-vm) or the 8 cpu cores as we configured)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MZLMx06_K_qR"
      },
      "outputs": [],
      "source": [
        "# Let create 8 keys, 1 for each batch\n",
        "keys = jax.random.split(jax.random.PRNGKey(0), 8)\n",
        "\n",
        "# Each replicated pmapped function get a different key\n",
        "mats = pmap(lambda key: jax.random.normal(key, (500, 600)))(keys)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6BkMsaOtLISj"
      },
      "source": [
        "The leading dimension here needs to equal the dimension of available devices (since we are sending a batch to each device)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gWrdv_2wLG4T"
      },
      "outputs": [],
      "source": [
        "print(mats.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HnqblcUsLaKZ"
      },
      "source": [
        "Using `pmap` to generate the batches ensures these batches are of type `ShardedDeviceArray`. This is similar to an ndarray, except each batch/shared is stored in the memory of multiple devices, so they can be used in subsequent `pmap` operations without moving data around between devices (GPU/TPU) and hosts (cpu). "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JAeaBCvcLQWg"
      },
      "outputs": [],
      "source": [
        "print(type(mats))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PVz0gOWG9pkr"
      },
      "outputs": [],
      "source": [
        "def dot_product_parallel():\n",
        "\n",
        "    # Run a local matmul on each device in parallel (no data transfer)\n",
        "    result = pmap(lambda x: jnp.dot(x, x.T))(\n",
        "        mats\n",
        "    ).block_until_ready()  # result.shape is (8, 5000, 5000)\n",
        "\n",
        "\n",
        "run_parallel = %timeit -o -n  5 dot_product_parallel()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "64gfyF3ENQzU"
      },
      "source": [
        "It is simple as that. Our dot product now runs in parallel across available devices (cpu, gpus or tpus). As we have more cores/devices, this code will automatically scale! "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5qcQXSbANP_M",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Let's plot the performance difference (Run Cell)\n",
        "\n",
        "jax_parallel_time = np.mean(run_parallel.all_runs)\n",
        "jax_seq_time = np.mean(run_sequential.all_runs)\n",
        "\n",
        "\n",
        "data = {\"JAX (seq)\": jax_seq_time, \"JAX (parallel - pmap)\": jax_parallel_time}\n",
        "\n",
        "plot_performance(data, title=\"Average time taken for Seq vs Parallel Dot Product\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-0j8iJRFUz6v"
      },
      "source": [
        "For some problems, the speed can be directly proportional to the number of devices -- $Nx$ speed up for $N$ devices! \n",
        "\n",
        "We showed an example of using `pmap` for *pure* parallelism, where there is no communication between devices. JAX also has various operations for communication across distributed devices ( more on this [here](https://jax.readthedocs.io/en/latest/jax-101/06-parallelism.html#communication-between-devices).)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fV3YG7QOZD-B"
      },
      "source": [
        "# **Conclusion**\n",
        "**Summary:**\n",
        "- JAX combines Autograd and XLA to perform **accelerated** 🚀 numerical computations. These computations are achieved using transforms such as `jit`,`grad`,`vmap` and `pmap`.\n",
        "- JAX's `grad` function automatically calculates the gradients of your functions for you! \n",
        "- Gradient descent is an effective algorithm to learn linear models, but also more complicated models, where analytical solutions don't exist. \n",
        "- We need to be careful not to over-fit or under-fit on our datasets. \n",
        "- Haiku and Optax make training JAX models more convenient.  \n",
        "\n",
        "\n",
        "**Next Steps:** \n",
        "\n",
        "* If you are interested in going deeper into optimization, check out [Optimization Practical](https://github.com/khipu-ai/practicals-2023/blob/main/practicals/optimization_and_haiku.ipynb).\n",
        "\n",
        "\n",
        "**References:** \n",
        "\n",
        "Part 1 \n",
        "1. Various JAX [docs](https://jax.readthedocs.io/en/latest/) - specifically [quickstart](https://jax.readthedocs.io/en/latest/notebooks/quickstart.html), [common gotchas](https://jax.readthedocs.io/en/latest/notebooks/Common_Gotchas_in_JAX.html), [jitting](\n",
        "https://jax.readthedocs.io/en/latest/jax-101/02-jitting.html#), [random numbers](https://jax.readthedocs.io/en/latest/jax-101/05-random-numbers.html) and [pmap](https://jax.readthedocs.io/en/latest/jax-101/06-parallelism.html?highlight=pmap#). \n",
        "2. http://matpalm.com/blog/ymxb_pod_slice/\n",
        "3. https://roberttlange.github.io/posts/2020/03/blog-post-10/\n",
        "4. [Machine Learning with JAX - From Zero to Hero | Tutorial #1](https://www.youtube.com/watch?v=SstuvS-tVc0). \n",
        "\n",
        "\n",
        "For other Khipu practicals  visit the [Khipu Practicals GitHub Repository](https://github.com/khipu-ai/practicals-2023)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OIZvkhfRz9Jz",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Generate Feedback Form. (Run Cell)\n",
        "from IPython.display import HTML\n",
        "\n",
        "HTML(\n",
        "    \"\"\"\n",
        "<iframe \n",
        "\tsrc=\"https://forms.gle/bvLLPX74LMGrFefo9\",\n",
        "  width=\"80%\" \n",
        "\theight=\"1200px\" >\n",
        "\tLoading...\n",
        "</iframe>\n",
        "\"\"\"\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "XrRoSqlxfi7f"
      ],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}