{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m2s4kN_QPQVe"
      },
      "source": [
        "# Optimization, Optax and Haiku\n",
        "\n",
        "\n",
        "\n",
        "<a href=\"https://colab.research.google.com/github/khipu-ai/practicals-2023/blob/main/notebooks/optimization_and_haiku.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
        "\n",
        "## Copyright\n",
        "\n",
        "© Deep Learning Indaba 2022. Apache License 2.0.\n",
        "© Khipu 2023. Apache License 2.0.\n",
        "\n",
        "## Authors\n",
        "\n",
        "* Kale-ab Tessera (Indaba 2022)\n",
        "* Ignacio Ramírez (Khipu 2023)\n",
        "\n",
        "## Reviewers\n",
        "* Javier Antoran\n",
        "* James Allingham\n",
        "* Ruan van der Merwe\n",
        "* Sebastian Bodenstein\n",
        "* Laurence Midgley\n",
        "* Joao Guilherme\n",
        "* Elan van Biljon\n",
        "\n",
        "\n",
        "# Introduction\n",
        "\n",
        "The three topics introduced in this practical aim at providing you with the tools to train your models. We will begin with the basic concepts of **numerical optimization**, in particular cost function minimization, which is the main method whereby we learn models.\n",
        "\n",
        "We will then show you how to use the [Optax](https://github.com/deepmind/optax) library, an efficient optimization library based on JAX.\n",
        "\n",
        "Finally, we'll visit [Haiku](https://github.com/deepmind/dm-haiku), a library for building and training neural networks.\n",
        "\n",
        "## Prerequisites\n",
        "\n",
        "We will assume that you are familiar with Python, Numpy and JAX. If not, please check out our [Practical](http://no.link.yet).\n",
        "\n",
        "## Requirements\n",
        "\n",
        "For this practical, you will need to use a GPU to speed up training. To do this, go to the \"Runtime\" menu in Colab, select \"Change runtime type\" and then in the popup menu, choose \"GPU\" in the \"Hardware accelerator\" box.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6EqhIg1odqg0"
      },
      "source": [
        "## Installation and Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "4boGA9rYdt9l",
        "cellView": "form",
        "outputId": "9561b34b-0453-4110-c5eb-945ca6b0d610",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a GPU is connected.\n"
          ]
        }
      ],
      "source": [
        "## Install and import anything required. Capture hides the output from the cell.\n",
        "# @title Install and import required packages. (Run Cell)\n",
        "\n",
        "import os\n",
        "\n",
        "# https://stackoverflow.com/questions/68340858/in-google-colab-is-there-a-programing-way-to-check-which-runtime-like-gpu-or-tpu\n",
        "if int(os.environ[\"COLAB_GPU\"]) > 0:\n",
        "    print(\"a GPU is connected.\")\n",
        "elif \"COLAB_TPU_ADDR\" in os.environ and os.environ[\"COLAB_TPU_ADDR\"]:\n",
        "    print(\"A TPU is connected.\")\n",
        "    import jax.tools.colab_tpu\n",
        "\n",
        "    jax.tools.colab_tpu.setup_tpu()\n",
        "else:\n",
        "    print(\"Only CPU accelerator is connected.\")\n",
        "    # x8 cpu devices - number of (emulated) host devices\n",
        "    os.environ[\"XLA_FLAGS\"] = \"--xla_force_host_platform_device_count=8\"\n",
        "\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from jax import grad, jit, vmap, pmap\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "YQe1CfDyrkdL",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Helper Functions. (Run Cell)\n",
        "import copy\n",
        "from typing import Dict\n",
        "\n",
        "\n",
        "def plot_performance(data: Dict, title: str):\n",
        "    runs = list(data.keys())\n",
        "    time = list(data.values())\n",
        "\n",
        "    # creating the bar plot\n",
        "    plt.bar(runs, time, width=0.35)\n",
        "\n",
        "    plt.xlabel(\"Implementation\")\n",
        "    plt.ylabel(\"Average time taken (in s)\")\n",
        "    plt.title(title)\n",
        "    plt.show()\n",
        "\n",
        "    best_perf_key = min(data, key=data.get)\n",
        "    all_runs_key = copy.copy(runs)\n",
        "\n",
        "    # all_runs_key_except_best\n",
        "    all_runs_key.remove(best_perf_key)\n",
        "\n",
        "    for k in all_runs_key:\n",
        "        print(\n",
        "            f\"{best_perf_key} was {round((data[k]/data[best_perf_key]),2)} times faster than {k} !!!\"\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "yFzjRHUsUQqq",
        "cellView": "form",
        "outputId": "62f0ae5c-8dd5-453c-b0ec-205582366eda",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num devices: 1\n",
            " Devices: [GpuDevice(id=0, process_index=0)]\n"
          ]
        }
      ],
      "source": [
        "# @title Check the device you are using (Run Cell)\n",
        "print(f\"Num devices: {jax.device_count()}\")\n",
        "print(f\" Devices: {jax.devices()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "$$\n",
        "\\newcommand{\\because}[1]{&& \\triangleright \\textrm{#1}}\n",
        "$$"
      ],
      "metadata": {
        "id": "blMNBku0dB8h"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aB0503xgmSFh"
      },
      "source": [
        "# Model learning and optimization\n",
        "\n",
        "Machine learning is all about learning _models_. In this context, models are mathematical objects that synthesize our knowledge on data from some domain. This knowledge then allows us, for example, to infer the output or response of a system given a certain input (regression, classification), to synthesize new data samples (unsupervised learning, simmulations, generative models), or to take the optimal action in a certain context (reinforcement learning).\n",
        "\n",
        "In any case, models are defined by a certain functional form which depends on a set of _parameters_ (this includes _non parametric_ models despite the name). Although the general form of a model may be known beforehand (e.g., a certain neural network architecture), its parameters are not, and depend on the data that we want to process.\n",
        "\n",
        "The process of adjusting (learning) the parameters of a model is usually done by defining a  _cost function_ which tells us how good our model is for some data, and then tuning the parameters, automatically, to minimize the cost; this is done by means of _optimization algorithms_.\n",
        "\n",
        "## Linear regression\n",
        "\n",
        "$\\def\\reals{\\mathbb{R}}$\n",
        "$\\def\\tr{^\\mathsf{T}}$\n",
        "$\\def\\data{\\mathcal{D}}$\n",
        "$\\def\\opt{^{*}}$\n",
        "$\\def\\loss{\\ell}$\n",
        "$\\newcommand{\\mat}[1]{\\mathbf{#1}}$\n",
        "$\\newcommand{\\vec}[1]{\\mathbf{#1}}$\n",
        "$\\def\\inv{^{-1}}$\n",
        "A simple example to begin with is linear regression. Given an input vector $x\\in\\reals^n$ and an output $y\\in\\reals$, we assume that the input-output relationship is $y=w\\tr x$ where $w\\in\\reals$ are parameters to be tuned. \n",
        "The following code generates data approximately following the above relation.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy import random\n",
        "\n",
        "rng = random.default_rng(2023)\n",
        "\n",
        "def sim_linear_model(w,n):\n",
        "  \"\"\"\n",
        "  given a linear coefficient w and a non-negative integer n\n",
        "  generate n samples according to y = wx + v\n",
        "  where v is a random Gaussian variable with mean 0\n",
        "  and standard deviation 0.1\n",
        "  \"\"\"\n",
        "  m = len(w)\n",
        "  X = random.normal(size=(n))\n",
        "  v = 0.1*random.normal(size=n)\n",
        "  y = w*X + v\n",
        "  return X,y\n",
        "\n",
        "#\n",
        "# our ground truth, w = 0.4\n",
        "#\n",
        "w = [0.4]\n",
        "#\n",
        "# simmulate samples\n",
        "#\n",
        "x,y = sim_linear_model(w,100)\n",
        "#\n",
        "# plot simmulated data\n",
        "#\n",
        "plt.figure(figsize=(7,7))\n",
        "plt.plot(x,y,'o')\n",
        "plt.title('linear model')\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('y')\n",
        "plt.grid(True)\n",
        "plt.ylim(-2,2)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "LNO9tb69qH5e",
        "outputId": "83d457a3-7847-42af-80a9-e1902538153c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 458
        }
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 504x504 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In general, we don't know $w$; what we have instead is data given as pairs $\\data = \\{(x_i,y_i):i=1,\\ldots,n\\}$ for which we assume that the above functional relationship holds, at least approximately. \n",
        "\n",
        "What we need to do is to find optimal values of $w\\opt$ so that the relationship $y=w\\tr x$ holds as best as possible, that is, we want $y-w\\tr x \\approx 0$ for every data pair. A natural way of doing so is to compute the accumulated squared approximation error $(y-w\\tr x)^2$ for all the samples in the data. With this, we can now define our loss function $\\ell(\\data;w)$:\n",
        "\n",
        "$$\\ell(\\data;w) = \\sum_i (y_i - w\\tr x_i)^2$$\n",
        "\n",
        "For simplicity, we will drop the $\\data$ and write $\\loss(x)$ hereafter. However, remember that this is a function of $\\data$ as well.  In the one-dimensional case above, we can actually plot $\\loss(w)$. Let's try this for a range of values:\n",
        "\n"
      ],
      "metadata": {
        "id": "RQl4cVwpqBsR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def quadratic_loss(x,y,w):\n",
        "  \"\"\" \n",
        "  implements the above loss function for one dimensional inputs\n",
        "  \"\"\"\n",
        "  return np.mean( (w*x-y)**2 ) # we divide by n to keep things small\n",
        "#\n",
        "# 100 parameter values between -3 and 3\n",
        "#\n",
        "ws = np.linspace(-3,3,100)\n",
        "#\n",
        "# very nice Python stuff: comprehensions\n",
        "#\n",
        "ls = [quadratic_loss(x,y,w) for w in ws]\n",
        "\n",
        "\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.plot(ws,ls,'o-')\n",
        "plt.title('quadratic loss function')\n",
        "plt.xlabel('w')\n",
        "plt.ylabel('\\ell(w)')\n",
        "plt.ylim(0,4)\n",
        "plt.xlim(-1,1)\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "aYv8WIk-uA8j",
        "outputId": "1a5d8819-3d19-4a73-cf43-26a82adc7aa9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        }
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "So we see above that the function is a simple parabola. In $n$-dimensional problems, it is actually, always, a paraboloid. \n",
        "\n",
        "## Finding the optimal parameters\n",
        "\n",
        "So how do we find $w\\opt$? Well, as we can see, and as any paraboloid, $\\loss(w)$ it is a convex function of $w$, which means that it there exists a parameters $w\\opt$ so that the loss is minimal with respect to any other parameter values. In this case the function is strongly convex, which means that $w\\opt$ is unique. \n",
        "\n",
        "Basic calculus tells us that, when a minima is attained, the derivative of $\\ell(w)$ is $0$. For this case, we can find the optimal parameters right away by differentiating and solving for $w$:\n",
        "\n",
        "$$\\frac{d\\ell}{dw}=-2 \\sum_i (y_i - w\\tr x_i)x_i\\tr = 0\\;\\Rightarrow\\sum_i y_ix_i\\tr  - \\sum_i w\\tr x_ix_i\\tr = 0$$\n",
        "\n",
        "When we work with Machine Learning, and in optimization in general, it is convenient to express such relationships in a more compact way. This makes it more clear and, fundamentally, allows us to speed up computations. In our case, if we define $\\mat{X}$ to be a matrix whose columns are $x_i$, and $\\vec{y}$ a vector whose elements are $y_i$, we can write the above relationship as:\n",
        "\n",
        "$$\\vec{y}\\mat{X}\\tr - \\vec{w}\\mat{X}\\mat{X}\\tr = 0 \\Rightarrow \\vec{y}\\mat{X}\\tr =\\vec{w}\\mat{X}\\mat{X}\\tr.$$\n",
        "\n",
        "We can now multiply both sides by $(\\mat{X}\\mat{X}\\tr)\\inv$ to obtain $\\vec{w}\\opt$ (provided that the inverse exists):\n",
        "\n",
        "$$\\vec{w} = \\vec{y}\\mat{X}\\tr(\\mat{X}\\mat{X}\\tr)\\inv.$$\n",
        "\n",
        "The above is known as the _least squares_ solution to the quadratic cost regression problem. Let's compute it:\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "fn3pIhset-pD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "w_ls =  np.sum(y*x)/np.dot(x,x)\n",
        "print('true w:',w,'w found by least squares:',w_ls)"
      ],
      "metadata": {
        "id": "9GBEhqKowhk5",
        "outputId": "89818c90-354f-4ee1-d2f1-9a8ffe8a28af",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "true w: [0.4] w found by least squares: 0.38829678571836745\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Comments\n",
        "\n",
        "Notice that the  parameter _estimated_ by least squares is not the exact, true value $0.4$. Why might this be? \n",
        "\n",
        "In any case, it is quite close. \n",
        "\n",
        "## Gradient descent\n",
        "\n",
        "So why bother with optimization algorithms? There are two main reasons:\n",
        "\n",
        "1. The parameters $w\\opt$ cannot be found by hand. **this happens most of the time**\n",
        "1. In some cases, although the a closed form solution exists, it may be very hard to compute. Imagine for example that the dimension of the data samples is $n=1000000$; this might be a small $1000{\\times}1000$ image. In most computers we will have a hard time inverting a $1000000{\\times}1000000$ matrix!\n",
        "\n",
        "When any of those occur, we need to find the solution in a different way. Optimization algorithms do so _iteratively_. \n",
        "\n",
        "There are brute force algorithms which try to find the solution by evaluating the cost function for many values of $w$, but this is impractical unless $n$ is very small, for example $1$. Sometimes, this is all you can do. However, if the cost function to be minimized is _differentiable_, there is a smarter way to look for solutions by following the _gradient_ of the function, but in the opposite direction.\n",
        "\n",
        "The rationale behind this idea is simple: by definition, the gradient of the const function, $\\nabla \\loss(w)$, indicates  the direction along which the function _increases the most_. Thus, if we go the opposite direction, we can expect the function to _decrease the most_. Algorithms based on this idea are known as _descent methods_. Those that go in the exact opposite direction, $-\\nabla \\loss(w)$, are known as _gradient descent methods_.\n",
        "\n",
        "Linear models with quadratic losses are excellent examples to begin exploring descent techniques because we can find the exact solution by hand and thus monitor our progress. We will now step up the problem a bit and work in two dimensions.\n",
        "\n",
        " "
      ],
      "metadata": {
        "id": "un3fSJ_Awf5H"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WmrI-52Zyvp1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dAeEMynv3GaI"
      },
      "outputs": [],
      "source": [
        "def manual_grad(b, w):\n",
        "    grad_b = 0\n",
        "    grad_w = 0\n",
        "    for x, y in zip(x_data_list, y_data_list):\n",
        "        f = w * x + b\n",
        "        grad_b += f - y\n",
        "        grad_w += (f - y) * x\n",
        "    grad_b /= len(x_data_list)\n",
        "    grad_w /= len(x_data_list)\n",
        "    return grad_b, grad_w"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RMt9Qlox28Oa"
      },
      "source": [
        "##### **Gradient Descent** \n",
        "\n",
        "Not that we have the gradients, we can use gradient descent. The general idea is to start with an initial value/guess for the model weights and then repeatedly use the gradients to tweak the parameters $\\color{red}{b}$ and $\\color{red}{w}$ in the right direction.  \n",
        "\n",
        "These updates can be formulated as follows:\n",
        "\n",
        "$$\\color{red}{b} \\leftarrow \\color{red}{b} - \\color{blue}{\\eta} \\frac{\\partial \\mathcal{L}}{\\partial \\color{red}{b}} $$ \n",
        "\n",
        "$$\\color{red}{w} \\leftarrow \\color{red}{w} - \\color{blue}{\\eta} \\frac{\\partial \\mathcal{L}}{\\partial \\color{red}{w}} $$ \n",
        "\n",
        ", where $\\color{blue}{\\eta}$ is the **learning rate** and just tells us how much we are going to scale the gradient before we use it to update our parameters:\n",
        "are we going to try to walk downhill with big steps or small steps?"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise 2.5**\n",
        "1. Run the code snippet below, and note the $(\\color{red}{b}, \\color{red}{w})$ trajectory as we use the gradient to (try to) get to the minimum.\n",
        "2. Adjust the starting values for $\\color{red}{b}$ or $\\color{red}{w}$ or the value of $\\color{blue}{\\eta}$ and see how the resulting trajectory to the minimum changes.\n",
        "3. Can you find a setting for $\\color{blue}{\\eta}$ where things start spiraling out of control and the loss gets bigger and bigger (and not smaller)?\n",
        "4. Can you find a setting for $\\color{blue}{\\eta}$ so that we're still far away from the minimum after `200` parameter update steps?\n",
        "5. Play around with the `max_grad` variable. Do we always need this? What problem does this solve? (Hint: Trying printing the grads values with `max_grad = None`).\n",
        "\n"
      ],
      "metadata": {
        "id": "YsL-Goz8hTOb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6AvZzHQx1AKM"
      },
      "outputs": [],
      "source": [
        "b = 0  # Change me! Try 2, 4\n",
        "w = -0.05  # Change me! Try -1, 2\n",
        "learning_rate = 0.01  # Change me! Try 0.1, 0.5, ...\n",
        "max_grad = 1  # Change me! Try None, 10\n",
        "\n",
        "parameters_step_list = []\n",
        "\n",
        "for _ in range(200):\n",
        "    parameters_step_list.append([b, w])\n",
        "    grad_b, grad_w = manual_grad(b, w)\n",
        "    # Naive gradient value clipping - different from standard gradient clipping - which clips the gradient norm.\n",
        "    if max_grad:\n",
        "        grad_b = jnp.clip(grad_b, a_min=-max_grad, a_max=max_grad)\n",
        "        grad_w = jnp.clip(grad_w, a_min=-max_grad, a_max=max_grad)\n",
        "    b = b - learning_rate * grad_b\n",
        "    w = w - learning_rate * grad_w\n",
        "\n",
        "plot_loss(\n",
        "    parameters_step_list, \"A loss function, and minimizing it with gradient descent\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3shLExakrzIW"
      },
      "source": [
        "##### **Autodiff using JAX: No more manual gradients!**\n",
        "\n",
        "In the above example, we calculated the gradients by hand (`manual_grad`). Thanks to automatic differentiation, we don't have to do this! While you can probably derive and code the gradients of the loss function for our linear model without making a mistake somewhere, getting the gradients right for more complex models can be much more work. Much, much more work! \n",
        "\n",
        "We use JAX to do the automatic differentiation, using the `grad` function as follows:\n",
        "```\n",
        "auto_grad = jax.grad(loss_function, argnums=(0, 1))\n",
        "```\n",
        "\n",
        "and call it in the same way as we called `manual_grad`. `argnums` tells JAX we want the partial derivative of our function with respect to the first 2 parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3WiF4oYi1xGK"
      },
      "outputs": [],
      "source": [
        "x = np.array(x_data_list)\n",
        "y = np.array(y_data_list)\n",
        "\n",
        "\n",
        "def loss_function(b, w):\n",
        "    f = w * x + b\n",
        "    errors = jnp.square(y - f)\n",
        "    # Instead of summing over individual data points in a for-loop, and then\n",
        "    # dividing to get the average, we do it in one go. No more for-loops!\n",
        "    return 1 / 2 * jnp.mean(errors)\n",
        "\n",
        "\n",
        "# This is it! One line of code.\n",
        "auto_grad = jax.grad(loss_function, argnums=(0, 1))\n",
        "\n",
        "# Let's see if it works. Does auto_grad match our manual version?\n",
        "b, w = 2.5, 3.5\n",
        "\n",
        "grad_b_autograd, grad_w_autograd = auto_grad(b, w)\n",
        "print(\"Autograd         grad_b:\", grad_b_autograd, \"  grad_w\", grad_w_autograd)\n",
        "\n",
        "grad_b_manual, grad_w_manual = manual_grad(b, w)\n",
        "print(\"Manual gradients grad_b:\", grad_b_manual, \"  grad_w\", grad_w_manual)\n",
        "\n",
        "# We use isclose, since the rounding is slightly different.\n",
        "assert jnp.isclose(grad_b_autograd, grad_b_manual) and jnp.isclose(\n",
        "    grad_w_autograd, grad_w_manual\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "okaeUVNf347w"
      },
      "source": [
        "Nice! So we can use automatic differentiation and we don't have to manually calculate gradients. "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **Gradient Descent vs Analytical Solution**\n",
        ">\n",
        "> So we used gradient descent to learn the weights for our linear model, but other options exist! For linear regression, there exists an [Analytical Solution](https://staff.fnwi.uva.nl/r.vandenboomgaard/MachineLearning/LectureNotes/Regression/LinearRegression/analytical_solution.html). This means we can calculate our weights directly in one step, without having to iterate using numerical methods like gradient descent.\n",
        ">\n",
        ">*Why use gradient descent then?*\n",
        "- `More General` - Gradient Descent is a more general algorithm, that can be applied to problems where analytical solutions aren't feasible to calculate or don't exit e.g. neural networks.  \n",
        "- `Computational Complexity` - Even when a closed form solution is available, in some cases it may be faster to find the solution using gradient descent. Read more on this [here](https://stats.stackexchange.com/questions/278755/why-use-gradient-descent-for-linear-regression-when-a-closed-form-math-solution).\n"
      ],
      "metadata": {
        "id": "uW5rnjwoVv0m"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rK3RJPvAf4zm"
      },
      "source": [
        "### **Assumptions**\n",
        "\n",
        "All models have assumptions. One assumption that we made is that our model is a *linear* model, i.e. that our best guess is for $y$ is with $f(x) = \\color{red}{w} x + \\color{red}{b}$. Is this assumption always valid for all kinds of data and datasets?\n",
        "\n",
        "> More assumptions for [simple linear regression](https://online.stat.psu.edu/stat500/lesson/9/9.2/9.2.3#paragraph--3265)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ao93xuXGJhLh"
      },
      "source": [
        "## **2.2 From Linear to Polynomial Regression** - <font color='orange'>`Intermediate`</font>\n",
        "\n",
        "So far we've looked at data that could be fitted fairly accurately with a single straight line. Despite its simplicity, linear regression tends to be very useful in practice, especially as a starting point in data analysis! However, there are cases where a linear fit is unsatisfying. \n",
        "\n",
        "Suppose our dataset looked like the following:\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/deep-learning-indaba/indaba-pracs-2022/main/images/Introduction_to_ML_using_JAX/sine-wave.png\" width=\"35%\" />\n",
        "\n",
        "How would we fit a model to this data? One possible option is to increase the complexity of our linear model by attempting to fit a higher-order polynomial,  for example, a 4th-degree [polynomial](https://en.wikipedia.org/wiki/Polynomial):\n",
        "$\\hat{y} = \\color{red}{w_4}x^4 + \\color{red}{w_3}x^3 + \\color{red}{w_2}x^2 + \\color{red}{w_1}x + \\color{red}{w_0}$. \n",
        "\n",
        "Do we have to derive a whole new algorithm? Luckily, not! We can still solve for the least squares parameters $\\color{red}{w_4}, \\color{red}{w_3}, \\color{red}{w_2}, \\color{red}{w_1}, \\color{red}{w_0}$ using the same techniques we used for fitting a line. \n",
        "\n",
        "Given the dataset $\\{(x_1, y_1), (x_2, y_2), ..., (x_n, y_n)\\}$, we construct a *feature* matrix $\\mathbf{\\Phi}$ by expending original features, being careful to include terms corresponding to each power of $x$, as follows:\n",
        "\n",
        "$\\mathbf{\\Phi} =\n",
        "\\begin{pmatrix}\n",
        "x_1^4 & x_1^3 & x_1^2 & x_1 & 1 \\\\\n",
        "x_2^4 & x_2^3 & x_2^2 & x_2 & 1 \\\\\n",
        "\\vdots & \\vdots & \\vdots & \\vdots & \\vdots \\\\\n",
        "x_n^4 & x_n^3 & x_n^2 & x_n & 1\n",
        "\\end{pmatrix}\n",
        "$\n",
        "\n",
        "And just like before, our $\\mathbf{y}$ vector is $(y_1, y_2, ..., y_n)^\\mathsf{T}$\n",
        "\n",
        "Next, we fit a 4th-degree polynomial to our data and find that the fit is visually a lot better and captures the wave-like pattern of the data! \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "XoSIWpUvKtlE"
      },
      "outputs": [],
      "source": [
        "# @title Polynomial Helper Functions (Run Me)\n",
        "def generate_wave_like_dataset(min_x=-1, max_x=1, n=100):\n",
        "    xs = np.linspace(min_x, max_x, n)\n",
        "    ys = np.sin(5 * xs) + np.random.normal(size=len(xs), scale=0.1)\n",
        "    return xs, ys\n",
        "\n",
        "\n",
        "def regression_analytical_solution(X, y):\n",
        "    return ((np.linalg.inv(X.T.dot(X))).dot(X.T)).dot(y)\n",
        "\n",
        "\n",
        "def gradient_descent(X, y, learning_rate=0.01, num_steps=1000, debug=False):\n",
        "    report_every = num_steps // 10\n",
        "\n",
        "    def loss(current_w, X, y):\n",
        "        y_hat = jnp.dot(X, current_w)\n",
        "        loss = jnp.mean((y_hat - y) ** 2)\n",
        "        return loss, y_hat\n",
        "\n",
        "    loss_and_grad = jax.value_and_grad(loss, has_aux=True)\n",
        "    # Initialize the parameters\n",
        "    key = jax.random.PRNGKey(42)\n",
        "    w = jax.random.normal(key=key, shape=(X.shape[1],))\n",
        "\n",
        "    # Run a a few steps of gradient descent\n",
        "    for i in range(num_steps):\n",
        "        (loss, y_hat), grad = loss_and_grad(w, X, ys)\n",
        "\n",
        "        if i % report_every == 0:\n",
        "            if debug:\n",
        "                print(f\"Step {i}: w: {w}, Loss: {loss}, Grad: {grad}\")\n",
        "            else:\n",
        "                print(f\"Step {i}: Loss: {loss}\")\n",
        "\n",
        "        w = w - learning_rate * grad\n",
        "\n",
        "    return w\n",
        "\n",
        "\n",
        "def plot_data(y_hat, xs, ys, title):\n",
        "    plt.figure()\n",
        "    plt.scatter(xs, ys, label=\"Data\")\n",
        "    plt.plot(xs, y_hat, \"r\", label=title)\n",
        "\n",
        "    plt.title(title)\n",
        "    plt.xlabel(\"Input x\")\n",
        "    plt.ylabel(\"Output y\")\n",
        "    plt.legend();"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CcXjMKi0Znr6"
      },
      "source": [
        "### **Under-fitting**\n",
        "Let's see how our linear model does on our new dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QmAWgBEIZh0X"
      },
      "outputs": [],
      "source": [
        "xs, ys = generate_wave_like_dataset(min_x=-1, max_x=1, n=25)\n",
        "X = np.vstack([xs, np.ones(len(xs))]).T\n",
        "w = regression_analytical_solution(X, ys)\n",
        "y_hat = X.dot(w)\n",
        "\n",
        "plot_data(y_hat, xs, ys, \"Linear regression (analytic minimum)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pzlcvE8pZrYj"
      },
      "source": [
        "Our linear model has missed the majority of the points in our dataset. This is also known as **under-fitting**, which is when our model is too simple to capture the relationship between the inputs and outputs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uwwajy30U9fX"
      },
      "source": [
        "### **Over-fitting**\n",
        "\n",
        "Since our linear model was too simple, we can try a more complicated model.\n",
        "\n",
        "**Exercise 2.5 - Code Task**: Spend a couple of minutes selecting different parameters (by moving the sliders), to see the best loss you can get using polynomial regression. \n",
        "\n",
        "1. `degree` - Degree $n$ of a polynomial in this form - $\\hat{y} = \\color{red}{w_n}x^n +\\color{red}{w_{n-1}}x^{n-1}+ ... + \\color{red}{w_2}x^2 + \\color{red}{w_1}x + \\color{red}{w_0}$. \n",
        "2. `num_steps` - The number of steps to running gradient descent for. \n",
        "3. `learning_rate` - The learning rate used when updating the weights in gradient descent. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "eGrB9V66-P9L"
      },
      "outputs": [],
      "source": [
        "# @title Choose parameters. { run: \"auto\" }\n",
        "degree = 3  # @param {type:\"slider\", min:1, max:10, step:1}\n",
        "num_steps = 1500  # @param {type:\"slider\", min:1000, max:5000, step:500}\n",
        "learning_rate = 0.1  # @param [\"0.2\",\"0.1\", \"0.01\"] {type:\"raw\"}\n",
        "\n",
        "\n",
        "# def create_data_matrix(xs,degree=4):\n",
        "#   return np.vstack([[np.power(xs,pow) for pow in np.arange(degree)],np.ones(len(xs))]).T\n",
        "\n",
        "\n",
        "def create_data_matrix(xs, degree=4):\n",
        "    pows = [np.power(xs, pow) for pow in np.arange(1, degree + 1)]\n",
        "    pows.reverse()\n",
        "    return np.vstack([pows, np.ones(len(xs))]).T\n",
        "\n",
        "\n",
        "phi = create_data_matrix(xs, degree=degree)\n",
        "\n",
        "\n",
        "w = gradient_descent(phi, ys, learning_rate=learning_rate, num_steps=num_steps)\n",
        "y_hat = phi.dot(w)\n",
        "\n",
        "plot_data(y_hat, xs, ys, \"Polynomial regression (gradient descent steps)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tGcJv82aFiLc"
      },
      "source": [
        "Let's see how a 10-th degree polynomial fits our data. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EszayH6Q-z6_"
      },
      "outputs": [],
      "source": [
        "degree = 10\n",
        "num_steps = 5000\n",
        "learning_rate = 0.2\n",
        "\n",
        "\n",
        "phi = create_data_matrix(xs, degree=degree)\n",
        "w = gradient_descent(phi, ys, learning_rate=learning_rate, num_steps=num_steps)\n",
        "y_hat = phi.dot(w)\n",
        "\n",
        "plot_data(y_hat, xs, ys, \"Polynomial regression (gradient descent steps)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o8SPF0UILmXW"
      },
      "source": [
        "**What happens if we extend our predictions out a bit?**\n",
        "\n",
        "Our model fits the majority of the data! This sounds great, but let's see how our model handles new data sampled from the same **data generation process**! \n",
        "\n",
        "In the plot below we fill in some extra data points from the true function (in orange) for comparison, but bear in mind that these were not used to fit the regression model. We are **extrapolating** the model into a previously unseen region!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y2d5QywylwTK"
      },
      "outputs": [],
      "source": [
        "# Recover the analytic solution.\n",
        "degree = 10\n",
        "phi = create_data_matrix(xs, degree=degree)\n",
        "w = regression_analytical_solution(phi, ys)\n",
        "\n",
        "# Extend the x's and y's.\n",
        "more_xs, more_ys = generate_wave_like_dataset(min_x=-1.3, max_x=-1, n=20)\n",
        "all_xs = np.concatenate([more_xs, xs])\n",
        "all_ys = np.concatenate([more_ys, ys])\n",
        "\n",
        "# Get the design matrix for the extended data, so that we could make predictions\n",
        "# for it.\n",
        "phi = create_data_matrix(all_xs, degree=degree)\n",
        "\n",
        "# Note that we don't recompute w, we use the previously computed values that\n",
        "# only saw x values in the range [0, 10]\n",
        "y_hat = phi.dot(w)\n",
        "\n",
        "plt.scatter(xs, ys, label=\"Data\")\n",
        "plt.scatter(more_xs, more_ys, label=\"Unseen Data\")\n",
        "plt.plot(all_xs, y_hat, \"r\", label=\"Polynomial Regression\")\n",
        "\n",
        "plt.title(\"A wave-like dataset with the best-fit line\")\n",
        "plt.xlabel(\"Input x\")\n",
        "plt.ylabel(\"Output y\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V3ld4cRlPVGy"
      },
      "source": [
        "We see that while the fit looks good in the blue region that the model was fitted on, the fit seems to diverge significantly in the orange region.\n",
        "The model is able to **interpolate** well (fill in gaps in the region it was fitted), but it **extrapolates** (outside the fitted region) poorly.\n",
        "This is a common concern with models in general, unless you can be sure that you have the correct *inductive biases* (assumptions about the data generating process) built into the model, you should be cautious about extrapolating from it.\n",
        "\n",
        "The fact that our model has very low training loss and high test loss (unseen data) is a sign of over-fitting. Over-fitting is when our models fits our training data, but fails to generalise to previously unseen data from the same data generating process. This is usually the result of the model having sufficient degrees of freedom to fit the noise in the training data.  \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise 2.6 - Group Task** \n",
        "\n",
        "**What shall we do? Pause here!**\n",
        "\n",
        "Before progressing with this practical, take a moment to think about the problem. In machine learning, there are many practical approaches to getting a model that generalizes well. As you can guess, much theory is devoted to the problem too!\n",
        "\n",
        "With what you've seen so far, try to explain to your neighbour\n",
        "\n",
        "1.   every factor that you can think of, that could cause a model to generalize poorly;\n",
        "2.   some ideas that you could think of to improve the model's fit to (unseen) data;\n",
        "3.   any underlying assumptions that you are making about unseen data.\n",
        "\n",
        "Don't proceed until you've had a solid discussion on the topic. If someone is tutoring this practical, they might contribute to the discussion!"
      ],
      "metadata": {
        "id": "2feKuHJplo0U"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sAtms17jtCOU"
      },
      "source": [
        "## **2.3 Training Models Using Haiku and Optax** - <font color='blue'>`Beginner`</font>\n",
        "\n",
        "For our Linear and Polynomial examples, we only used core JAX to keep track of and optimize our weights. This can be tedious, especially when dealing with larger models and when using more complicated optimization methods. \n",
        "\n",
        "Luckily, JAX has higher-level neural network libraries such as [Haiku](https://github.com/deepmind/dm-haiku) or [Flax](https://github.com/google/flax), which make building models more convenient, and libraries like [Optax](https://github.com/deepmind/optax), that make gradient processing and optimization more convenient. \n",
        "\n",
        "In this section, we will briefly go through how to use Haiku and Optax. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "0ySycQo7txoF"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "# @title Install Haiku and Optax. (Run Cell)\n",
        "!pip install -U dm-haiku\n",
        "!pip install -U optax\n",
        "# For plotting.\n",
        "!pip install livelossplot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "exuVety_bFhQ"
      },
      "source": [
        "### Haiku\n",
        "\n",
        "[Haiku](https://github.com/deepmind/dm-haiku) is JAX neural network library intended to be familiar to people used to object-oriented programming models (like PyTorch or Tensorflow), by making managing state simpler. \n",
        "\n",
        "Haiku modules are similar to standard python objects (they have references to their own parameters and functions). However, since JAX operates on *pure functions*, Haiku modules **cannot be directly instantiated**, but rather they need to be **wrapped into pure function transformations.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9wvTzTi-YJTp"
      },
      "source": [
        "Let's create a simple linear module."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H_-3r49B-Orc"
      },
      "outputs": [],
      "source": [
        "import haiku as hk\n",
        "\n",
        "\n",
        "class MyLinearModel(hk.Module):\n",
        "    def __init__(self, output_size, name=None):\n",
        "        super().__init__(name=name)\n",
        "        self.output_size = output_size\n",
        "\n",
        "    def __call__(self, x):\n",
        "        j, k = x.shape[-1], self.output_size\n",
        "        w_init = hk.initializers.TruncatedNormal(1.0 / np.sqrt(j))\n",
        "        w = hk.get_parameter(\"w\", shape=[j, k], dtype=x.dtype, init=w_init)\n",
        "        b = hk.get_parameter(\"b\", shape=[k], dtype=x.dtype, init=jnp.ones)\n",
        "        return jnp.dot(x, w) + b"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3WYb35ffYOSt"
      },
      "source": [
        "And attempt to directly **instantiate** it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LuZy7pj9-b2m"
      },
      "outputs": [],
      "source": [
        "# Should raise an error.\n",
        "try:\n",
        "    MyLinearModel(output_size=1)\n",
        "except Exception as e:\n",
        "    print(\"Exception {}\".format(e))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-XGOeJCH-10P"
      },
      "source": [
        "This fails since we are trying to **directly** instantiate `MyLinearModel`. Instead what we should do is wrap our model in a pure functional transform as follows: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d1yI7j2h_Esd"
      },
      "outputs": [],
      "source": [
        "def model_fn(x):\n",
        "    module = MyLinearModel(output_size=1)\n",
        "    return module(x)\n",
        "\n",
        "\n",
        "model = hk.without_apply_rng(hk.transform(model_fn))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EZ24tXUiaHQa"
      },
      "source": [
        "> We use `hk.without_apply_rng` since our model's *inference* (not initialization) is deterministic and hence has no use for a random key when calling `.apply`. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3aWAc_f0BVFU"
      },
      "outputs": [],
      "source": [
        "model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lao8wS3tBjc3"
      },
      "source": [
        "Our wrapper object has two methods: \n",
        "- `init` - initialize the variables in the model and return these params. \n",
        "- `apply` - run a forward pass through our data. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gTJcV6hjFh6u"
      },
      "source": [
        "If we want to get the initial state of our module, we need to call `init` with an example input."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nt0srU3rQlhL"
      },
      "outputs": [],
      "source": [
        "# input dimention we are considering\n",
        "input_dim = 3\n",
        "\n",
        "example_x = jnp.arange(input_dim, dtype=jnp.float32)\n",
        "rng_key = jax.random.PRNGKey(42)\n",
        "\n",
        "params = model.init(rng=rng_key, x=example_x)\n",
        "print(params)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VCoYMnZkGKOb"
      },
      "source": [
        "We can now call the `apply` method as follows. Note we pass in the `params` variable that holds the current model weights. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XA8n5cEMGVWC"
      },
      "outputs": [],
      "source": [
        "new_x = jnp.arange(input_dim, dtype=jnp.float32)\n",
        "# example forward pass through our model\n",
        "prediction = model.apply(params, new_x)\n",
        "print(prediction)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mmk2jcIHbRlS"
      },
      "source": [
        "So that is it! Those are basics of using Haiku modules!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_3h034w5bWn6"
      },
      "source": [
        "### Optax\n",
        "\n",
        "[Optax](https://github.com/deepmind/optax) is an optimization and gradient processing library in JAX. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CuWggGFEcdoy"
      },
      "source": [
        "In our linear regression section, we manually updated the params of our model (e.g. `w = w - learning_rate * grad_w`). \n",
        "\n",
        "This wasn't too difficult in our simple case, but for more challenging optimizations, especially when chaining optimizations (e.g. clipping gradient norm and then applying an optimizer update), it becomes trickier to effectively and accurately implement these parameter updates. Luckily, Optax comes to the rescue here!    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hvecjyZGelIV"
      },
      "source": [
        "Here is a simple example of how you create and initialize an optimizer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zhqkLvtRe6zf"
      },
      "outputs": [],
      "source": [
        "import optax\n",
        "\n",
        "# create optim\n",
        "learning_rate = 0.1\n",
        "optimizer = optax.adam(learning_rate)\n",
        "\n",
        "# init optim\n",
        "input_dim = 3\n",
        "# init weights to pass to our optim\n",
        "params = {\"w\": jnp.ones((input_dim,))}\n",
        "\n",
        "# Obtain the `opt_state` that contains statistics for the optimizer.\n",
        "opt_state = optimizer.init(params)\n",
        "print(opt_state)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Io4mLeIifxkX"
      },
      "source": [
        "Once we have calculated the gradients, we pass them (`grads`) and the `opt_state` to our optimizer to get `updates` that should be applied to the current parameters and `new_opt_state`, which keeps track of the current state of the optimizer. \n",
        "\n",
        "```\n",
        "updates, new_opt_state = optimizer.update(grads, opt_state)\n",
        "params = optax.apply_updates(params, updates)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "And that is the basics of Optax. "
      ],
      "metadata": {
        "id": "4p1l2rUWpRZ7"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7IaqVuRPg3ER"
      },
      "source": [
        "### Full Training Loop Using Haiku and Optax 🧙\n",
        "\n",
        "Here we show a full training loop, using Haiku and Optax. For convenience, we introduce structures like `TrainingState` and functions like `init`,`update` and `loss_fn`. Please read through to get comfortable with how you can effectively train JAX models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pqZZlOfNuMEn"
      },
      "source": [
        "Here we define some helper functions. "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Any, MutableMapping, NamedTuple, Tuple\n",
        "import time\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "import haiku as hk\n",
        "import optax\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "from livelossplot import PlotLosses\n",
        "\n",
        "# Convenient container for keeping track of training state.\n",
        "class TrainingState(NamedTuple):\n",
        "    \"\"\"Container for the training state.\"\"\"\n",
        "\n",
        "    params: hk.Params\n",
        "    opt_state: optax.OptState\n",
        "    step: jnp.DeviceArray\n",
        "\n",
        "\n",
        "# function for our model (same as above)\n",
        "def model_fn(x):\n",
        "    module = MyLinearModel(output_size=1)\n",
        "    return module(x).ravel()\n",
        "\n",
        "\n",
        "# Load a simple dataset - diabetes (https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_diabetes.html)\n",
        "# and convert to an iterator. Although it would be faster to use pure jnp arrays in this example,\n",
        "# in practice for large datasets we use iterators.\n",
        "# Read here https://www.tensorflow.org/guide/data_performance for best practices.\n",
        "def load_dataset(seed, input_dim=3, train_batch_size=32, shuffule_train_data=True):\n",
        "    # Load the diabetes dataset\n",
        "    diabetes_X, diabetes_y = datasets.load_diabetes(return_X_y=True)\n",
        "\n",
        "    # Use only the first input_dim (3) features\n",
        "    diabetes_X = diabetes_X[:, :input_dim]\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        diabetes_X, diabetes_y, test_size=0.2, train_size=0.8, random_state=seed\n",
        "    )\n",
        "\n",
        "    train_dataset = (\n",
        "        tf.data.Dataset.from_tensor_slices((X_train, y_train)).cache().repeat()\n",
        "    )\n",
        "    test_dataset = tf.data.Dataset.from_tensor_slices((X_test, y_test)).cache().repeat()\n",
        "\n",
        "    if shuffule_train_data:\n",
        "        train_dataset = train_dataset.shuffle(10 * train_batch_size, seed=seed)\n",
        "\n",
        "    train_dataset = train_dataset.batch(train_batch_size)\n",
        "    # Using full test dataset\n",
        "    test_dataset = test_dataset.batch(len(X_test))\n",
        "\n",
        "    train_dataset = iter(tfds.as_numpy(train_dataset))\n",
        "    test_dataset = iter(tfds.as_numpy(test_dataset))\n",
        "    return train_dataset, test_dataset"
      ],
      "metadata": {
        "id": "LY0t6C4OKzSK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Full training and evaluation loop."
      ],
      "metadata": {
        "id": "-rGuA_Y4DHXA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# first we retrive our model\n",
        "model = hk.without_apply_rng(hk.transform(model_fn))\n",
        "\n",
        "# Then we create the optimiser - chain clipping by gradient norm and using Adam\n",
        "learning_rate = 0.01\n",
        "optimizer = optax.chain(\n",
        "    optax.clip_by_global_norm(0.5),\n",
        "    optax.adam(learning_rate=learning_rate),\n",
        ")\n",
        "\n",
        "# define our loss function\n",
        "def loss_fn(params, x, y_true):\n",
        "    y_pred = model.apply(params, x)\n",
        "    loss = (y_pred - y_true) ** 2\n",
        "    return jnp.mean(loss)\n",
        "\n",
        "\n",
        "# Function to initialize our model and optimizer.\n",
        "@jax.jit\n",
        "def init(rng: jnp.ndarray, data) -> TrainingState:\n",
        "    \"\"\"\n",
        "    rng: jax prng seed.\n",
        "    data: Sample of the dataset used to get correct shape.\n",
        "    \"\"\"\n",
        "\n",
        "    rng, init_rng = jax.random.split(rng)\n",
        "    initial_params = model.init(init_rng, data)\n",
        "    initial_opt_state = optimizer.init(initial_params)\n",
        "    return TrainingState(\n",
        "        params=initial_params,\n",
        "        opt_state=initial_opt_state,\n",
        "        step=np.array(0),\n",
        "    )\n",
        "\n",
        "\n",
        "# Function to update our params and keep track of metrics\n",
        "@jax.jit\n",
        "def update(state: TrainingState, data):\n",
        "    X, y = data\n",
        "    loss_value, grads = jax.value_and_grad(loss_fn)(state.params, X, y)\n",
        "    updates, new_opt_state = optimizer.update(grads, state.opt_state)\n",
        "    new_params = optax.apply_updates(state.params, updates)\n",
        "\n",
        "    new_state = TrainingState(\n",
        "        params=new_params,\n",
        "        opt_state=new_opt_state,\n",
        "        step=state.step + 1,\n",
        "    )\n",
        "    metrics = {\"train_loss\": loss_value, \"step\": state.step}\n",
        "    return new_state, metrics\n",
        "\n",
        "\n",
        "# Function to evaluate our models\n",
        "@jax.jit\n",
        "def evaluate(params: hk.Params, test_dataset) -> jnp.ndarray:\n",
        "    # Here we simply use our loss func/mse to eval our models,\n",
        "    # but we can use diff functions for loss and evaluation,\n",
        "    # e.g. in classification we use Cross-entropy classification loss\n",
        "    # , but we use accuracy as an eval metric.\n",
        "    x_test, y_test_true = test_dataset\n",
        "    return loss_fn(params, x_test, y_test_true)\n",
        "\n",
        "\n",
        "# We get our dataset\n",
        "seed = 42\n",
        "train_dataset, test_dataset = load_dataset(seed=seed, input_dim=10)\n",
        "\n",
        "# Initialise model params and optimiser;\n",
        "rng = jax.random.PRNGKey(seed)\n",
        "# We pass an example of the input to get the correct shapes\n",
        "state = init(rng, next(train_dataset)[0])\n",
        "\n",
        "# Time our training\n",
        "prev_time = time.time()\n",
        "max_steps = 10**5\n",
        "eval_every = 5000\n",
        "metrics = {}\n",
        "plotlosses = PlotLosses()\n",
        "\n",
        "# Training & evaluation loop.\n",
        "for step in range(max_steps):\n",
        "    state, metrics = update(state, data=next(train_dataset))\n",
        "\n",
        "    # Periodically evaluate on test set.\n",
        "    if step % eval_every == 0:\n",
        "        steps_per_sec = eval_every / (time.time() - prev_time)\n",
        "        prev_time = time.time()\n",
        "        test_loss = evaluate(state.params, next(test_dataset))\n",
        "        metrics.update({\"steps_per_sec\": steps_per_sec})\n",
        "        metrics.update({\"test_loss\": test_loss})\n",
        "        plotlosses.update(\n",
        "            {\n",
        "                \"train_loss\": jnp.mean(metrics[\"train_loss\"]),\n",
        "            }\n",
        "        )\n",
        "        plotlosses.update(\n",
        "            {\n",
        "                \"test_loss\": test_loss,\n",
        "            }\n",
        "        )\n",
        "        plotlosses.send()"
      ],
      "metadata": {
        "id": "EsD62L4cUM9r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Please try and get comfortable with the above code since we will be using Haiku and Optax in other practicals. If you need assistance, please call a tutor!"
      ],
      "metadata": {
        "id": "03woGcY0pxPb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Let's plot our predictions vs targets.\n",
        "\n",
        "X_test, y_test = next(test_dataset)\n",
        "pred = model.apply(state.params, X_test)\n",
        "\n",
        "plt.figure(figsize=(7, 7))\n",
        "plt.scatter(y_test, pred, c=\"crimson\")\n",
        "\n",
        "p1 = max(max(pred), max(y_test))\n",
        "p2 = min(min(pred), min(y_test))\n",
        "plt.plot([p1, p2], [p1, p2], \"b-\")\n",
        "plt.xlabel(\"Actual Values\", fontsize=15)\n",
        "plt.ylabel(\"Predictions\", fontsize=15)\n",
        "plt.axis(\"equal\")\n",
        "plt.show()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "uU3aRT3p-QVY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So there is some correlation with our predictions and our actual targets. This shows that we are learning a useful model for our data."
      ],
      "metadata": {
        "id": "qgFA9zHOBiLh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You have officially trained a model end-to-end using the latest JAX techniques! 🔥\n",
        "\n",
        "Although, we have only done simple Linear Regression in this tutorial, you have learned optimization techniques like gradient descent, which can apply to a variety of models! "
      ],
      "metadata": {
        "id": "BMTbY9uv-lIk"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fV3YG7QOZD-B"
      },
      "source": [
        "# **Conclusion**\n",
        "**Summary:**\n",
        "- JAX combines Autograd and XLA to perform **accelerated** 🚀 numerical computations. These computations are achieved using transforms such as `jit`,`grad`,`vmap` and `pmap`.\n",
        "- JAX's `grad` function automatically calculates the gradients of your functions for you! \n",
        "- Gradient descent is an effective algorithm to learn linear models, but also more complicated models, where analytical solutions don't exist. \n",
        "- We need to be careful not to over-fit or under-fit on our datasets. \n",
        "- Haiku and Optax make training JAX models more convenient.  \n",
        "\n",
        "\n",
        "**Next Steps:** \n",
        "\n",
        "- If you are interested in going deeper into Linear Regression, we have a Bayesian Linear Regression section in the [Bayesian Deep Learning Prac](https://github.com/deep-learning-indaba/indaba-pracs-2022/blob/main/practicals/Bayesian_Deep_Learning_Prac.ipynb).\n",
        "- You can also adapt the model and dataset from the \"*Full Training Loop Using Haiku and Optax*\" section to train your custom models on custom datasets. \n",
        "\n",
        "\n",
        "**References:** \n",
        "\n",
        "Part 1 \n",
        "1. Various JAX [docs](https://jax.readthedocs.io/en/latest/) - specifically [quickstart](https://jax.readthedocs.io/en/latest/notebooks/quickstart.html), [common gotchas](https://jax.readthedocs.io/en/latest/notebooks/Common_Gotchas_in_JAX.html), [jitting](\n",
        "https://jax.readthedocs.io/en/latest/jax-101/02-jitting.html#), [random numbers](https://jax.readthedocs.io/en/latest/jax-101/05-random-numbers.html) and [pmap](https://jax.readthedocs.io/en/latest/jax-101/06-parallelism.html?highlight=pmap#). \n",
        "2. http://matpalm.com/blog/ymxb_pod_slice/\n",
        "3. https://roberttlange.github.io/posts/2020/03/blog-post-10/\n",
        "4. [Machine Learning with JAX - From Zero to Hero | Tutorial #1](https://www.youtube.com/watch?v=SstuvS-tVc0). \n",
        "\n",
        "Part 2 \n",
        "1. Parts of this section are adapted from [Deepmind's Regression Tutorial](https://github.com/deepmind/educational/blob/master/colabs/summer_schools/intro_to_regression.ipynb). \n",
        "2. https://d2l.ai/chapter_linear-networks/linear-regression.html\n",
        "3. https://www.cs.toronto.edu/~rgrosse/courses/csc411_f18/slides/lec06-slides.pdf\n",
        "4. [Linear Regression Chapter - Mathematics for Machine Learning Book](https://mml-book.github.io/). \n",
        "\n",
        "\n",
        "For other practicals from the Deep Learning Indaba, please visit [here](https://github.com/deep-learning-indaba/indaba-pracs-2022)."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Appendix:** \n",
        "\n"
      ],
      "metadata": {
        "id": "XrRoSqlxfi7f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Derivation of partial derivatives for exercise 2.4.\n",
        "\n",
        "Derive $\\frac{\\partial \\mathcal{L}}{\\partial w}$:\n",
        "\\begin{aligned}\n",
        "\\frac{\\partial \\mathcal{L}}{\\partial w} & = \\frac{ \\partial}{\\partial w} (\\frac{1}{2N} \\sum_{i=1}^N (y_i - (w x_i + b))^2) \\because{Definition of $\\mathcal{L}$} \\\\\n",
        " \\frac{\\partial \\mathcal{L}}{\\partial w} & = \\frac{1}{2N} \\frac{ \\partial }{\\partial w} ( \\sum_{i=1}^N (y_i - (w x_i + b))^2) \\because{Constant multiple rule} \\\\\n",
        "\\frac{\\partial \\mathcal{L}}{\\partial w} & = \\frac{1}{2N} \\sum_{i=1}^N \\frac{ \\partial }{\\partial w} (y_i - (w x_i + b))^2 \\because{Sum Rule - derivative of sum is sum of derivatives.} \\\\ \n",
        "\\frac{\\partial \\mathcal{L}}{\\partial w} & = \\frac{1}{2N} \\sum_{i=1}^N 2 (y_i - (w x_i + b)) \\frac{ \\partial }{\\partial w}(y_i -(w x_i + b))  \\because{Power Rule + Chain Rule.} \\\\ \n",
        "\\frac{\\partial \\mathcal{L}}{\\partial w} & = \\frac{1}{2N} \\sum_{i=1}^N 2 (y_i - (w x_i + b)) (-x_i)  \\because{Compute derative.} \\\\ \n",
        "\\frac{\\partial \\mathcal{L}}{\\partial w} & = \\frac{1(2)}{2N} \\sum_{i=1}^N  (y_i - (w x_i + b)) (-x_i)  \\because{Factor constant out of summation.} \\\\ \n",
        "\\frac{\\partial \\mathcal{L}}{\\partial w} & = \\frac{1}{N} \\sum_{i=1}^N  -y_ix_i + (w x_i + b)x_i  \\because{Multiply brackets and simplify.} \\\\ \n",
        "\\frac{\\partial \\mathcal{L}}{\\partial w} & = \\frac{1}{N} \\sum_{i=1}^N  (-y_i + (w x_i + b))x_i  \\because{Rewrite.} \\\\ \n",
        "\\frac{\\partial \\mathcal{L}}{\\partial w} & = \\frac{1}{N} \\sum_{i=1}^N  ((w x_i + b) -y_i )x_i  \\because{Rewrite.} \\\\ \n",
        "\\frac{\\partial \\mathcal{L}}{\\partial w} & = \\frac{1}{N} \\sum_{i=1}^N  (f(x_i) -y_i )x_i  \\because{Substitute in $f(x_i)$.} \\\\  \n",
        "\\end{aligned}\n",
        "\n",
        "Derive $\\frac{\\partial \\mathcal{L}}{\\partial b}$:\n",
        "\\begin{aligned}\n",
        "\\frac{\\partial \\mathcal{L}}{\\partial b} & = \\frac{ \\partial}{\\partial b} (\\frac{1}{2N} \\sum_{i=1}^N (y_i - (w x_i + b))^2) \\because{Definition of $\\mathcal{L}$} \\\\\n",
        "\\frac{\\partial \\mathcal{L}}{\\partial b} & = \\frac{1}{2N} \\frac{ \\partial }{\\partial b} ( \\sum_{i=1}^N (y_i - (w x_i + b))^2) \\because{Constant multiple rule} \\\\\n",
        "\\frac{\\partial \\mathcal{L}}{\\partial b} & = \\frac{1}{2N} \\sum_{i=1}^N \\frac{ \\partial }{\\partial b} (y_i - (w x_i + b))^2 \\because{Sum Rule - derivative of sum is sum of derivatives.} \\\\ \n",
        "\\frac{\\partial \\mathcal{L}}{\\partial b} & = \\frac{1}{2N} \\sum_{i=1}^N 2 (y_i - (w x_i + b)) \\frac{ \\partial }{\\partial b}(y_i -(w x_i + b))  \\because{Power Rule + Chain Rule.} \\\\ \n",
        "\\frac{\\partial \\mathcal{L}}{\\partial b} & = \\frac{1}{2N} \\sum_{i=1}^N 2 (y_i - (w x_i + b)) (-1)  \\because{Compute derative.} \\\\ \n",
        "\\frac{\\partial \\mathcal{L}}{\\partial b} & = \\frac{1(2)}{2N} \\sum_{i=1}^N (y_i - (w x_i + b)) (-1)  \\because{Factor constant out of summation.} \\\\ \n",
        "\\frac{\\partial \\mathcal{L}}{\\partial b} & = \\frac{1}{N} \\sum_{i=1}^N (-y_i + (w x_i + b))  \\because{Multiply brackets and simplify.} \\\\ \n",
        "\\frac{\\partial \\mathcal{L}}{\\partial b} & = \\frac{1}{N} \\sum_{i=1}^N ((w x_i + b) -y_i )  \\because{Rewrite.} \\\\ \n",
        "\\frac{\\partial \\mathcal{L}}{\\partial b} & = \\frac{1}{N} \\sum_{i=1}^N (f(x_i) -y_i )  \\because{Substitute in $f(x_i)$.} \\\\ \n",
        "\\end{aligned}"
      ],
      "metadata": {
        "id": "9OH9H7ndfuyQ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o1ndpYE50BpG"
      },
      "source": [
        "# **Feedback**\n",
        "\n",
        "Please provide feedback that we can use to improve our practicals in the future."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OIZvkhfRz9Jz",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Generate Feedback Form. (Run Cell)\n",
        "from IPython.display import HTML\n",
        "\n",
        "HTML(\n",
        "    \"\"\"\n",
        "<iframe \n",
        "\tsrc=\"https://forms.gle/bvLLPX74LMGrFefo9\",\n",
        "  width=\"80%\" \n",
        "\theight=\"1200px\" >\n",
        "\tLoading...\n",
        "</iframe>\n",
        "\"\"\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oglV4kHMWnIN"
      },
      "source": [
        "<img src=\"https://baobab.deeplearningindaba.com/static/media/indaba-logo-dark.d5a6196d.png\" width=\"50%\" />"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "XrRoSqlxfi7f"
      ],
      "name": "optimization_and_haiku.ipynb",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}